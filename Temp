
package com.abcdefg.abc.generator;

import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import com.abcdefg.abc.generator.config.ConfigurationLoader;
import com.abcdefg.abc.generator.csv.CSVMetadataGenerator;
import com.abcdefg.abc.generator.report.GlobalGeneratorReport;
import com.abcdefg.abc.generator.tar.TARFileGenerator;
import com.abcdefg.abc.generator.utils.UniqueIDManager;

/**
 * Main service abc for generating large-scale sample data
 * Generates CSV metadata files and corresponding TAR archives
 * 
 * Features:
 * - Multi-threaded generation (configurable thread pool)
 * - Progress monitoring (real-time reporting)
 * - Global statistics tracking
 * - Two TAR strategies (SINGLE_PDF or REUSE_EXISTING)
 * - Configurable via properties file
 */
public class WawaSampleDataGeneratorService {
    
    public static void main(String[] args) {
        System.out.println("===== WawaSampleDataGenerator STARTED =====");
        
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        
        // Initialize global report
        GlobalGeneratorReport globalReport = new GlobalGeneratorReport();
        
        try {
            // Load configuration
            ConfigurationLoader config = ConfigurationLoader.getInstance();
            
            System.out.println("=== CONFIGURATION ===");
            System.out.println("Config File: " + System.getProperty("config.file"));
            
            // Get configuration parameters
            int filesPerCategory = config.getIntProperty("generator.files.per.category", 1000);
            int rowsPerFile = config.getIntProperty("generator.rows.per.file", 10000);
            String[] categories = config.getArrayProperty("generator.categories");
            String outputPath = config.getProperty("generator.output.path", "D:/Rameshwar/abcdefg_GENERATED/Source/");
            int threadPoolSize = config.getIntProperty("generator.thread.pool.size", 10);
            // Progress report interval (reserved for future use)
            // int progressReportInterval = config.getIntProperty("generator.progress.report.interval", 50);
            
            System.out.println("Files per category: " + filesPerCategory);
            System.out.println("Rows per file: " + rowsPerFile);
            System.out.println("Categories: " + String.join(", ", categories));
            System.out.println("Output path: " + outputPath);
            System.out.println("Thread pool size: " + threadPoolSize);
            
            // Calculate total volume
            int totalFiles = filesPerCategory * categories.length;
            long totalDocuments = (long) totalFiles * rowsPerFile;
            System.out.println("\n=== VOLUME ===");
            System.out.println("Total files to generate: " + totalFiles);
            System.out.println("Total documents to generate: " + totalDocuments);
            
            // Initialize thread pool
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService;
            System.out.println("\nThread pool initialized with " + threadPoolSize + " threads");
            
            // Initialize ID manager
            UniqueIDManager idManager = new UniqueIDManager(config);
            
            // Initialize generators
            CSVMetadataGenerator csvGenerator = new CSVMetadataGenerator(config, idManager);
            TARFileGenerator tarGenerator = new TARFileGenerator(config);
            
            // Setup progress monitoring
            ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
            progressMonitor = progressMonitorService;
            
            final ExecutorService executorRef = executor;
            progressMonitorService.scheduleAtFixedRate(() -> {
                try {
                    globalReport.logProgress();
                    
                    if (executorRef instanceof ThreadPoolExecutor) {
                        ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                        globalReport.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount());
                    }
                } catch (Exception e) {
                    System.err.println("Error in progress monitor: " + e.getMessage());
                }
            }, 1, 1, TimeUnit.MINUTES);
            
            System.out.println("Progress monitoring enabled (interval: 1 minute)");
            
            // Create output directory
            Files.createDirectories(Paths.get(outputPath));
            
            // Create category directories
            for (String category : categories) {
                Path categoryPath = Paths.get(outputPath, category);
                Files.createDirectories(categoryPath);
                System.out.println("Created category directory: " + categoryPath);
            }
            
            // Generate file tasks
            System.out.println("\n=== GENERATING FILES ===");
            List<GenerationTask> tasks = createGenerationTasks(categories, filesPerCategory, 
                                                                rowsPerFile, outputPath, config);
            
            // Queue tasks
            for (GenerationTask task : tasks) {
                globalReport.incrementCSVFilesQueued(task.category);
            }
            
            // Execute generation tasks
            for (GenerationTask task : tasks) {
                final GenerationTask taskRef = task;
                
                executor.submit(() -> {
                    try {
                        // Generate TAR file first
                        String tarFileName = taskRef.tarFileName;
                        Path tarFilePath = Paths.get(outputPath, taskRef.category, tarFileName);
                        String docFilePath = tarGenerator.generateTARFile(tarFilePath, 
                                taskRef.tarFolderName, taskRef.category);
                        globalReport.recordTARFileSuccess(tarFileName);
                        
                        // Generate CSV file
                        String csvFileName = taskRef.csvFileName;
                        Path csvFilePath = Paths.get(outputPath, taskRef.category, csvFileName);
                        int rowsGenerated = csvGenerator.generateCSVFile(csvFilePath, 
                                taskRef.category, taskRef.archiveId, taskRef.rowsPerFile, docFilePath);
                        globalReport.recordCSVFileSuccess(csvFileName, rowsGenerated);
                        
                        // Progress reporting (commented out - use scheduled monitor instead)
                        // if (globalReport.totalCSVFilesGenerated.get() % progressReportInterval == 0) {
                        //     System.out.println("Progress: " + globalReport.totalCSVFilesGenerated.get() + 
                        //                      "/" + totalFiles + " files generated");
                        // }
                        
                    } catch (Exception e) {
                        System.err.println("Error processing task: " + taskRef.csvFileName);
                        e.printStackTrabc();
                        globalReport.recordCSVFileFailure(taskRef.csvFileName, e.getMessage());
                        globalReport.recordTARFileFailure(taskRef.tarFileName, e.getMessage());
                    }
                });
            }
            
            // Shutdown executor and wait for completion
            executor.shutdown();
            System.out.println("\nWaiting for all generation tasks to complete...");
            
            executor.awaitTermination(24, TimeUnit.HOURS);
            System.out.println("All generation tasks completed!");
            
            // Shutdown progress monitor
            progressMonitor.shutdown();
            progressMonitor.awaitTermination(10, TimeUnit.SECONDS);
            
            // Print final summary
            System.out.println(globalReport.getSummary());
            
            // Write report to file
            globalReport.writeReportToFile(outputPath);
            
            System.out.println("===== WawaSampleDataGenerator COMPLETED SUCCESSFULLY =====");
            
        } catch (Exception e) {
            System.err.println("Critical error in WawaSampleDataGeneratorService");
            e.printStackTrabc();
        } finally {
            // Cleanup
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                progressMonitor.shutdownNow();
            }
            if (executor != null && !executor.isShutdown()) {
                executor.shutdownNow();
            }
        }
    }
    
    /**
     * Create generation tasks for all files
     */
    private static List<GenerationTask> createGenerationTasks(String[] categories, 
                                                                int filesPerCategory,
                                                                int rowsPerFile,
                                                                String outputPath,
                                                                ConfigurationLoader config) {
        List<GenerationTask> tasks = new ArrayList<>();
        
        // Get starting archive ID
        int startArchiveId = config.getIntProperty("generator.start.archiveid", 1);
        
        // Date pattern
        String dateStart = config.getProperty("generator.filename.date.start", "10_2");
        boolean dateIncrement = config.getBooleanProperty("generator.filename.date.increment", true);
        
        int currentArchiveId = startArchiveId;
        int currentDateNum = Integer.parseInt(dateStart.split("_")[1]);
        int currentMonth = Integer.parseInt(dateStart.split("_")[0]);
        
        for (String category : categories) {
            for (int i = 0; i < filesPerCategory; i++) {
                // Generate date pattern
                String datePattern = currentMonth + "_" + currentDateNum;
                
                // Generate filenames
                String csvFileName = "metadata_" + datePattern + "_" + category + "_" + currentArchiveId + ".csv";
                String tarFileName = "documents_" + datePattern + "_" + category + "_" + currentArchiveId + ".tar";
                String tarFolderName = "documents_" + datePattern + "_" + category + "_" + currentArchiveId;
                
                GenerationTask task = new GenerationTask();
                task.category = category;
                task.csvFileName = csvFileName;
                task.tarFileName = tarFileName;
                task.tarFolderName = tarFolderName;
                task.archiveId = currentArchiveId;
                task.rowsPerFile = rowsPerFile;
                
                tasks.add(task);
                
                // Increment archive ID
                currentArchiveId++;
                
                // Increment date if enabled
                if (dateIncrement) {
                    currentDateNum++;
                    if (currentDateNum > 30) {
                        currentDateNum = 1;
                        currentMonth++;
                        if (currentMonth > 12) {
                            currentMonth = 1;
                        }
                    }
                }
            }
        }
        
        return tasks;
    }
    
    /**
     * Inner class to hold generation task details
     */
    private static class GenerationTask {
        String category;
        String csvFileName;
        String tarFileName;
        String tarFolderName;
        int archiveId;
        int rowsPerFile;
    }
}

====================================================

package com.abcdefg.abc.generator.config;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

/**
 * Configuration loader for WawaSampleDataGenerator
 * Loads properties from external config file
 */
public class ConfigurationLoader {
    
    private static ConfigurationLoader instance;
    private Properties properties;
    
    private ConfigurationLoader() {
        properties = new Properties();
        loadProperties();
    }
    
    public static synchronized ConfigurationLoader getInstance() {
        if (instance == null) {
            instance = new ConfigurationLoader();
        }
        return instance;
    }
    
    private void loadProperties() {
        String configFile = System.getProperty("config.file");
        
        if (configFile != null) {
            try (InputStream input = new FileInputStream(configFile)) {
                properties.load(input);
                System.out.println("Loaded configuration from: " + configFile);
            } catch (IOException e) {
                System.err.println("Error loading config file: " + configFile);
                e.printStackTrabc();
            }
        } else {
            // Try to load from classpath
            try (InputStream input = getClass().getClassLoader().getResourceAsStream("WawaSampleDataGenerator.properties")) {
                if (input != null) {
                    properties.load(input);
                    System.out.println("Loaded configuration from classpath");
                } else {
                    System.err.println("Configuration file not found!");
                }
            } catch (IOException e) {
                System.err.println("Error loading config from classpath");
                e.printStackTrabc();
            }
        }
    }
    
    public String getProperty(String key) {
        return properties.getProperty(key);
    }
    
    public String getProperty(String key, String defaultValue) {
        return properties.getProperty(key, defaultValue);
    }
    
    public int getIntProperty(String key, int defaultValue) {
        String value = properties.getProperty(key);
        if (value != null) {
            try {
                return Integer.parseInt(value);
            } catch (NumberFormatException e) {
                System.err.println("Invalid integer for key: " + key);
            }
        }
        return defaultValue;
    }
    
    public long getLongProperty(String key, long defaultValue) {
        String value = properties.getProperty(key);
        if (value != null) {
            try {
                return Long.parseLong(value);
            } catch (NumberFormatException e) {
                System.err.println("Invalid long for key: " + key);
            }
        }
        return defaultValue;
    }
    
    public boolean getBooleanProperty(String key, boolean defaultValue) {
        String value = properties.getProperty(key);
        if (value != null) {
            return Boolean.parseBoolean(value);
        }
        return defaultValue;
    }
    
    public String[] getArrayProperty(String key) {
        String value = properties.getProperty(key);
        if (value != null && !value.isEmpty()) {
            return value.split(",");
        }
        return new String[0];
    }
}

=========================================

package com.abcdefg.abc.generator.csv;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Random;

import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVPrinter;
import org.apache.commons.csv.QuoteMode;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonObject;
import com.abcdefg.abc.generator.config.ConfigurationLoader;
import com.abcdefg.abc.generator.utils.UniqueIDManager;

/**
 * Generates CSV metadata files with realistic data
 */
public class CSVMetadataGenerator {
    
    private final ConfigurationLoader config;
    private final UniqueIDManager idManager;
    private final Random random;
    private final Gson gson;
    
    // Data pools from configuration
    private final String[] authors;
    private final String[] documentTypes;
    private final String[] sourceSystems;
    private final String[] mimeTypes;
    
    // Sample values from production data (for realistic generation)
    private final String[] sampleBatchIds = {
        "97b6687a-5307-4b1d-9583-1f8f4a6bf889",
        "20231214.005763",
        "5c40a986-df05-4af9-ae25-42d5354d8682",
        "20240715.002818",
        "8476a539-ee7e-4e19-8b05-76da3562d4dc",
        "ff1b7143-d680-4a01-8872-6b98e08e1129",
        "d6ba0e87-dd29-4aca-a109-47113983b353",
        "fcc12152-2816-4d09-80cc-a00595940166",
        "f4fedff3-1a64-43b2-9ccf-86362b2f3548",
        "610000000000019250773"
    };
    
    private final String[] sampleDatacapDocIds = {
        "20250320210841208290",
        "20231214.005763.02",
        "20250312014221676565",
        "20240715.002818.01",
        "b32300f8-5d9e-4a34-83b8-1319a1a2e270",
        "20250403194513016865",
        "20241014175524104782",
        "20240308.004725.01",
        "20240501.007404.02",
        "20240221.007498.01"
    };
    
    private final String[] sampleSourceDocIds = {
        "40790184",
        "52069316",
        "85043557",
        "68272585",
        "88228436",
        "94735231",
        "92875041",
        "52293543",
        "91994082"
    };
    
    private final String[] sampleUsernames = {
        "nhilfers",
        "Jamie Garrett",
        "Amanda Lamperts",
        "Jennifer Fockler",
        "David Devine",
        "Janice Jin",
        "Natasha Richman",
        "alamperts",
        "fjameson",
        "isirkro",
        "ryanpowell",
        "mccsena"
    };
    
    // Timestamp range
    private final int startYear;
    private final int endYear;
    
    // CSV Header
    private static final String[] HEADER = {
        "wdr_id", "orig_rep", "doc_uid", "filesize", "category",
        "metadata_json", "doc_filepath", "mimetype", 
        "file_creation_ts", "file_creation_year", "archiveid"
    };
    
    public CSVMetadataGenerator(ConfigurationLoader config, UniqueIDManager idManager) {
        this.config = config;
        this.idManager = idManager;
        this.random = new Random();
        // Configure Gson to serialize null values (CRITICAL for matching production format)
        this.gson = new GsonBuilder().serializeNulls().create();
        
        // Load data pools from config
        this.authors = config.getArrayProperty("generator.authors");
        this.documentTypes = config.getArrayProperty("generator.document.types");
        this.sourceSystems = config.getArrayProperty("generator.source.systems");
        
        // MIME types for variety (used with REUSE_EXISTING strategy)
        this.mimeTypes = new String[]{
            "application/pdf", "text/html", "application/x-zip-compressed",
            "text/plain", "video/mp4", "audio/x-ms-wma", "image/pjpeg",
            "image/tiff", "application/rtf", "text/csv", "audio/amr"
        };
        
        // Timestamp range
        this.startYear = config.getIntProperty("generator.timestamp.start.year", 2023);
        this.endYear = config.getIntProperty("generator.timestamp.end.year", 2025);
    }
    
    /**
     * Generate a complete CSV file with specified number of rows
     * 
     * @param csvFilePath Path where CSV will be written
     * @param category Category name (e.g., "category_claimcenter")
     * @param archiveId Archive ID for this CSV/TAR pair
     * @param numberOfRows Number of rows to generate
     * @param docFilePath Document file path inside TAR (for SINGLE_PDF strategy)
     * @return Number of rows generated
     */
    public int generateCSVFile(Path csvFilePath, String category, int archiveId, 
                                 int numberOfRows, String docFilePath) throws IOException {
        
        // Configure CSV format to match production (quote all non-numeric fields)
        CSVFormat format = CSVFormat.DEFAULT.builder()
            .setHeader(HEADER)
            .setQuoteMode(QuoteMode.NON_NUMERIC)
            .build();
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(csvFilePath.toFile()));
             CSVPrinter csvPrinter = new CSVPrinter(writer, format)) {
            
            for (int i = 0; i < numberOfRows; i++) {
                generateCSVRow(csvPrinter, category, archiveId, docFilePath);
            }
            
            return numberOfRows;
        }
    }
    
    /**
     * Generate a single CSV row with realistic data
     */
    private void generateCSVRow(CSVPrinter csvPrinter, String category, 
                                  int archiveId, String docFilePath) throws IOException {
        
        // Generate unique IDs
        String wdrId = idManager.getNextWdrId(); // 10-digit padded string
        String claimNum = idManager.getNextClaimNumber(); // 19-digit claim number (grouped: 10-15 docs per claim)
        
        // Determine orig_rep (80% opentext, 20% filenet)
        int openTextPercentage = config.getIntProperty("generator.orig.rep.opentext.percentage", 80);
        String origRep = (random.nextInt(100) < openTextPercentage) ? "opentext" : "filenet";
        
        // Generate doc_uid based on orig_rep
        // - opentext: "OT:1000001"
        // - filenet: "{7F21D554-A295-4772-8D4D-843D90C56FE4}"
        String docUid = idManager.getNextDocUid(origRep);
        
        // Generate filesize (random between 0 and 50MB)
        long filesize = random.nextInt(50 * 1024 * 1024);
        
        // Generate timestamp (spread across 2023-2025)
        String timestamp = generateRandomTimestamp();
        String year = timestamp.substring(0, 4);
        
        // MIME type (application/pdf for SINGLE_PDF, or varied for REUSE_EXISTING)
        String mimeType = config.getProperty("generator.single.pdf.mimetype", "application/pdf");
        
        // Get document name from filepath
        String documentName = extractFileName(docFilePath);
        
        // Generate metadata JSON
        String metadataJson = generateMetadataJson(wdrId, claimNum, documentName, 
                                                     category, origRep, mimeType);
        // Write CSV row
        csvPrinter.printRecord(
            Long.parseLong(wdrId),  // wdr_id (as number - no quotes)
            origRep,                // orig_rep
            docUid,                 // doc_uid
            filesize,               // filesize
            category,               // category
            metadataJson,           // metadata_json
            docFilePath,            // doc_filepath
            mimeType,               // mimetype
            timestamp,              // file_creation_ts
            year,                   // file_creation_year
            archiveId               // archiveid
        );
    }
    
    /**
     * Generate realistic metadata JSON
     */
    private String generateMetadataJson(String fileId, String claimNum, String documentName,
                                         String category, String origRep, String mimeType) {
        JsonObject metadata = new JsonObject();
        JsonObject metadataContent = new JsonObject();
        
        // Core metadata fields - ALL ALWAYS PRESENT (matching production format)
        metadataContent.addProperty("author", getRandomAuthor());
        
        // set_id: Sometimes null, sometimes has complex value (ALWAYS PRESENT)
        if (random.nextInt(100) < 30) { // 30% chance of having a value
            String setIdValue = String.format("RelatedTo>ClaimNumber>%s*DocCreationMethod>Upload*PolicyNumber>%d", 
                                             claimNum, 10000000 + random.nextInt(90000000));
            metadataContent.addProperty("set_id", setIdValue);
        } else {
            metadataContent.addProperty("set_id", (String) null);
        }
        
        metadataContent.addProperty("file_id", Long.parseLong(fileId)); // Convert to long for JSON
        
        // batch_id: Sometimes null, sometimes from sample values (ALWAYS PRESENT)
        if (random.nextInt(100) < 40) { // 40% chance of having a value
            metadataContent.addProperty("batch_id", sampleBatchIds[random.nextInt(sampleBatchIds.length)]);
        } else {
            metadataContent.addProperty("batch_id", (String) null);
        }
        
        metadataContent.addProperty("claim_num", claimNum);
        
        // public_id: Sometimes null, sometimes "cc:{number}" (ALWAYS PRESENT)
        if (random.nextInt(100) < 50) { // 50% chance of having a value
            metadataContent.addProperty("public_id", "cc:" + (50000000 + random.nextInt(20000000)));
        } else {
            metadataContent.addProperty("public_id", (String) null);
        }
        
        metadataContent.addProperty("document_name", documentName);
        metadataContent.addProperty("document_type", getRandomDocumentType());
        metadataContent.addProperty("source_system", getRandomSourceSystem());
        
        // datacap_doc_id: Sometimes null, sometimes from sample values (ALWAYS PRESENT)
        if (random.nextInt(100) < 35) { // 35% chance of having a value
            metadataContent.addProperty("datacap_doc_id", sampleDatacapDocIds[random.nextInt(sampleDatacapDocIds.length)]);
        } else {
            metadataContent.addProperty("datacap_doc_id", (String) null);
        }
        
        // Category-specific fields (ALWAYS PRESENT)
        if (category.contains("dcpfnol") || category.contains("claimcenter")) {
            metadataContent.addProperty("category_type_id", random.nextInt(10) + 1);
        } else {
            metadataContent.addProperty("category_type_id", (Integer) null);
        }
        
        // source_document_id: Sometimes null, sometimes from sample values (ALWAYS PRESENT)
        if (random.nextInt(100) < 25) { // 25% chance of having a value
            metadataContent.addProperty("source_document_id", sampleSourceDocIds[random.nextInt(sampleSourceDocIds.length)]);
        } else {
            metadataContent.addProperty("source_document_id", (String) null);
        }
        
        // claim_center_user_id: Sometimes null, sometimes from sample values (ALWAYS PRESENT)
        if (random.nextInt(100) < 40) { // 40% chance of having a value
            metadataContent.addProperty("claim_center_user_id", sampleUsernames[random.nextInt(sampleUsernames.length)]);
        } else {
            metadataContent.addProperty("claim_center_user_id", (String) null);
        }
        
        metadata.add("metadata", metadataContent);
        
        // Top-level metadata fields (outside of "metadata" object)
        metadata.addProperty("mime_type", mimeType);
        
        // deleted_by and date_deleted: Usually null (documents are not deleted)
        metadata.addProperty("deleted_by", (String) null);
        metadata.addProperty("date_deleted", (String) null);
        
        metadata.addProperty("display_name", documentName);
        return gson.toJson(metadata);
    }
    
    /**
     * Generate random timestamp between startYear and endYear
     */
    private String generateRandomTimestamp() {
        // Generate random year
        int year = startYear + random.nextInt(endYear - startYear + 1);
        
        // Generate random month (1-12)
        int month = 1 + random.nextInt(12);
        
        // Generate random day (1-28 for simplicity)
        int day = 1 + random.nextInt(28);
        
        // Generate random time
        int hour = random.nextInt(24);
        int minute = random.nextInt(60);
        int second = random.nextInt(60);
        int nano = random.nextInt(1000000) * 1000; // Microseconds
        
        LocalDateTime dateTime = LocalDateTime.of(year, month, day, hour, minute, second, nano);
        
        // Format: "2025-04-11 20:10:41.150000+00:00"
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSSSSS");
        return dateTime.format(formatter) + "+00:00";
    }
    
    /**
     * Extract filename from filepath
     */
    private String extractFileName(String filepath) {
        if (filepath == null || filepath.isEmpty()) {
            return "sample_document.pdf";
        }
        
        int lastSlash = filepath.lastIndexOf('/');
        if (lastSlash >= 0 && lastSlash < filepath.length() - 1) {
            return filepath.substring(lastSlash + 1);
        }
        
        return filepath;
    }
    
    /**
     * Get random author from pool
     */
    private String getRandomAuthor() {
        if (authors.length == 0) {
            return "Auto-generated";
        }
        return authors[random.nextInt(authors.length)];
    }
    
    /**
     * Get random document type from pool
     */
    private String getRandomDocumentType() {
        if (documentTypes.length == 0) {
            return "PDF";
        }
        return documentTypes[random.nextInt(documentTypes.length)];
    }
    
    /**
     * Get random source system from pool
     */
    private String getRandomSourceSystem() {
        if (sourceSystems.length == 0) {
            return "DoubleDouble";
        }
        return sourceSystems[random.nextInt(sourceSystems.length)];
    }
}


==========================================

package com.abcdefg.abc.generator.report;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * Global report for tracking sample data generation statistics
 * Thread-safe implementation using atomic counters
 */
public class GlobalGeneratorReport {
    
    // File counters
    private final AtomicInteger totalCSVFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesGenerated = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesFailed = new AtomicInteger(0);
    
    private final AtomicInteger totalTARFilesGenerated = new AtomicInteger(0);
    private final AtomicInteger totalTARFilesFailed = new AtomicInteger(0);
    
    // Document counters
    private final AtomicLong totalDocumentsGenerated = new AtomicLong(0);
    
    // Category counters
    private final AtomicInteger categoryDoubleDoubleFiles = new AtomicInteger(0);
    private final AtomicInteger categoryDcpFnolFiles = new AtomicInteger(0);
    private final AtomicInteger categoryDcpFnolDoubleDoubleFiles = new AtomicInteger(0);
    
    // Performance metrics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalGeneratorReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a CSV file has been queued for generation
     */
    public void incrementCSVFilesQueued(String category) {
        totalCSVFilesQueued.incrementAndGet();
        incrementCategoryCounter(category);
    }
    
    /**
     * Record successful CSV file generation
     */
    public void recordCSVFileSuccess(String fileName, int rowsGenerated) {
        totalCSVFilesGenerated.incrementAndGet();
        totalDocumentsGenerated.addAndGet(rowsGenerated);
        
        System.out.println("CSV generated: " + fileName + " (" + rowsGenerated + " rows)");
    }
    
    /**
     * Record failed CSV file generation
     */
    public void recordCSVFileFailure(String fileName, String errorMessage) {
        totalCSVFilesFailed.incrementAndGet();
        System.err.println("CSV failed: " + fileName + " | Error: " + errorMessage);
    }
    
    /**
     * Record successful TAR file generation
     */
    public void recordTARFileSuccess(String fileName) {
        totalTARFilesGenerated.incrementAndGet();
    }
    
    /**
     * Record failed TAR file generation
     */
    public void recordTARFileFailure(String fileName, String errorMessage) {
        totalTARFilesFailed.incrementAndGet();
        System.err.println("TAR failed: " + fileName + " | Error: " + errorMessage);
    }
    
    /**
     * Increment category-specific counter
     */
    private void incrementCategoryCounter(String category) {
        if (category.contains("claimcenter") && !category.contains("dcpfnol")) {
            categoryDoubleDoubleFiles.incrementAndGet();
        } else if (category.contains("dcpfnol") && !category.contains("claimcenter")) {
            categoryDcpFnolFiles.incrementAndGet();
        } else if (category.contains("dcpfnolclaimcenter")) {
            categoryDcpFnolDoubleDoubleFiles.incrementAndGet();
        }
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        System.out.println("========== PROGRESS REPORT ==========");
        System.out.println("Elapsed Time: " + elapsedMinutes + " min " + elapsedSeconds + " sec");
        System.out.println("CSV Files: Queued=" + totalCSVFilesQueued.get() + 
                          ", Generated=" + totalCSVFilesGenerated.get() + 
                          ", Failed=" + totalCSVFilesFailed.get());
        System.out.println("TAR Files: Generated=" + totalTARFilesGenerated.get() + 
                          ", Failed=" + totalTARFilesFailed.get());
        System.out.println("Documents: Generated=" + totalDocumentsGenerated.get());
        System.out.println("By Category: DoubleDouble=" + categoryDoubleDoubleFiles.get() + 
                          ", DcpFnol=" + categoryDcpFnolFiles.get() + 
                          ", DcpFnolDoubleDouble=" + categoryDcpFnolDoubleDoubleFiles.get());
        
        // Calculate throughput
        if (elapsed.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsGenerated.get() / elapsed.getSeconds();
            long filesPerMinute = totalCSVFilesGenerated.get() / Math.max(1, elapsed.toMinutes());
            System.out.println("Throughput: " + docsPerSecond + " docs/sec, " + 
                              filesPerMinute + " files/min");
        }
        
        // Log memory
        logMemoryStats();
        
        System.out.println("=====================================");
    }
    
    /**
     * Log memory statistics
     */
    private void logMemoryStats() {
        Runtime runtime = Runtime.getRuntime();
        long maxMemory = runtime.maxMemory();
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long usedMemory = totalMemory - freeMemory;
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        System.out.println("Memory: Used=" + (usedMemory / (1024 * 1024)) + " MB, " +
                          "Total=" + (totalMemory / (1024 * 1024)) + " MB, " +
                          "Max=" + (maxMemory / (1024 * 1024)) + " MB, " +
                          "Usage=" + String.format("%.1f%%", usagePercent));
        
        if (usagePercent >= 80) {
            System.out.println("⚠️ HIGH MEMORY USAGE WARNING: " + String.format("%.1f%%", usagePercent));
        }
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        
        sb.append("\n--- File Statistics ---\n");
        sb.append(String.format("Total CSV Files Queued: %d\n", totalCSVFilesQueued.get()));
        sb.append(String.format("Total CSV Files Generated: %d\n", totalCSVFilesGenerated.get()));
        sb.append(String.format("Total CSV Files Failed: %d\n", totalCSVFilesFailed.get()));
        sb.append(String.format("Total TAR Files Generated: %d\n", totalTARFilesGenerated.get()));
        sb.append(String.format("Total TAR Files Failed: %d\n", totalTARFilesFailed.get()));
        
        sb.append("\n--- Document Statistics ---\n");
        sb.append(String.format("Total Documents Generated: %d\n", totalDocumentsGenerated.get()));
        
        sb.append("\n--- Category Statistics ---\n");
        sb.append(String.format("category_claimcenter: %d files\n", categoryDoubleDoubleFiles.get()));
        sb.append(String.format("category_dcpfnol: %d files\n", categoryDcpFnolFiles.get()));
        sb.append(String.format("category_dcpfnolclaimcenter: %d files\n", categoryDcpFnolDoubleDoubleFiles.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsGenerated.get() / totalDuration.getSeconds();
            long filesPerMinute = totalCSVFilesGenerated.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d docs/sec, %d files/min\n", docsPerSecond, filesPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark generation as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String outputPath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = "WawaSampleDataGenerator_Report_" + timestamp + ".csv";
        Path reportPath = Paths.get(outputPath, fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            writer.write("Metric,Value");
            writer.newLine();
            
            // Timing
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Files
            writer.newLine();
            writer.write("--- File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total CSV Files Queued,%d", totalCSVFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Generated,%d", totalCSVFilesGenerated.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Failed,%d", totalCSVFilesFailed.get()));
            writer.newLine();
            writer.write(String.format("Total TAR Files Generated,%d", totalTARFilesGenerated.get()));
            writer.newLine();
            writer.write(String.format("Total TAR Files Failed,%d", totalTARFilesFailed.get()));
            writer.newLine();
            
            // Documents
            writer.newLine();
            writer.write("--- Document Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Documents Generated,%d", totalDocumentsGenerated.get()));
            writer.newLine();
            
            // Categories
            writer.newLine();
            writer.write("--- Category Statistics ---,");
            writer.newLine();
            writer.write(String.format("category_claimcenter,%d", categoryDoubleDoubleFiles.get()));
            writer.newLine();
            writer.write(String.format("category_dcpfnol,%d", categoryDcpFnolFiles.get()));
            writer.newLine();
            writer.write(String.format("category_dcpfnolclaimcenter,%d", categoryDcpFnolDoubleDoubleFiles.get()));
            writer.newLine();
            
            // Performance
            if (totalDuration.getSeconds() > 0) {
                long docsPerSecond = totalDocumentsGenerated.get() / totalDuration.getSeconds();
                long filesPerMinute = totalCSVFilesGenerated.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance ---,");
                writer.newLine();
                writer.write(String.format("Documents Per Second,%d", docsPerSecond));
                writer.newLine();
                writer.write(String.format("Files Per Minute,%d", filesPerMinute));
                writer.newLine();
            }
            
            System.out.println("Report written to: " + reportPath);
        }
    }
}

================================================

package com.abcdefg.abc.generator.tar;

import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.StandardCopyOption;

import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;

import com.itextpdf.text.Document;
import com.itextpdf.text.DocumentException;
import com.itextpdf.text.Paragraph;
import com.itextpdf.text.pdf.PdfWriter;
import com.abcdefg.abc.generator.config.ConfigurationLoader;

/**
 * Generates TAR archive files with document content
 * Supports two strategies:
 * 1. SINGLE_PDF: Create TAR with single small PDF (memory efficient)
 * 2. REUSE_EXISTING: Reuse existing TAR files for variety
 */
public class TARFileGenerator {
    
    private final ConfigurationLoader config;
    private final String strategy;
    private final String samplePath;
    
    // For SINGLE_PDF strategy
    private File samplePdfFile;
    private final int pdfSizeKB;
    
    public TARFileGenerator(ConfigurationLoader config) {
        this.config = config;
        this.strategy = config.getProperty("generator.tar.strategy", "SINGLE_PDF");
        this.samplePath = config.getProperty("generator.tar.sample.path", "");
        this.pdfSizeKB = config.getIntProperty("generator.tar.sample.pdf.size.kb", 2);
        
        System.out.println("TARFileGenerator initialized with strategy: " + strategy);
    }
    
    /**
     * Generate TAR file based on configured strategy
     * 
     * @param tarFilePath Path where TAR file will be created
     * @param tarFolderName Name of the folder inside TAR
     * @param category Category name for organizing files
     * @return Document file path inside TAR (relative path)
     */
    public String generateTARFile(Path tarFilePath, String tarFolderName, String category) 
            throws IOException {
        
        if ("SINGLE_PDF".equalsIgnoreCase(strategy)) {
            return generateSinglePDFTar(tarFilePath, tarFolderName, category);
        } else if ("REUSE_EXISTING".equalsIgnoreCase(strategy)) {
            return reuseExistingTar(tarFilePath, category);
        } else {
            throw new IllegalArgumentException("Unknown TAR strategy: " + strategy);
        }
    }
    
    /**
     * Strategy 1: Create TAR with single small PDF file
     * Memory efficient - all CSV rows point to same file
     */
    private String generateSinglePDFTar(Path tarFilePath, String tarFolderName, String category) 
            throws IOException {
        
        // Ensure sample PDF exists
        if (samplePdfFile == null || !samplePdfFile.exists()) {
            samplePdfFile = createSamplePDF();
        }
        
        // Determine subcategory folder
        String subcategory = extractSubcategory(category);
        
        // Build internal TAR structure
        String docFilePath = "docrepo/" + subcategory + "/sample_document.pdf";
        
        // Create TAR file
        try (FileOutputStream fos = new FileOutputStream(tarFilePath.toFile());
             BufferedOutputStream bos = new BufferedOutputStream(fos);
             TarArchiveOutputStream tarOut = new TarArchiveOutputStream(bos)) {
            
            // Set long file name mode
            tarOut.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU);
            
            // Create directory structure
            addDirectoryToTar(tarOut, tarFolderName + "/");
            addDirectoryToTar(tarOut, tarFolderName + "/docrepo/");
            addDirectoryToTar(tarOut, tarFolderName + "/docrepo/" + subcategory + "/");
            
            // Add the PDF file
            String entryName = tarFolderName + "/" + docFilePath;
            addFileToTar(tarOut, samplePdfFile, entryName);
            
            tarOut.finish();
        }
        
        return docFilePath;
    }
    
    /**
     * Strategy 2: Reuse existing TAR files
     * Copies existing TAR and returns varied file paths
     */
    private String reuseExistingTar(Path tarFilePath, String category) throws IOException {
        // Find existing TAR file from sample path
        Path sourceTarPath = findExistingTar(category);
        
        if (sourceTarPath != null && Files.exists(sourceTarPath)) {
            // Copy existing TAR to new location
            Files.copy(sourceTarPath, tarFilePath, StandardCopyOption.REPLACE_EXISTING);
            
            // Return a typical doc_filepath from existing TARs
            return getTypicalDocFilePath(category);
        } else {
            // Fallback to SINGLE_PDF if no existing TAR found
            System.out.println("Warning: No existing TAR found for " + category + ", falling back to SINGLE_PDF");
            return generateSinglePDFTar(tarFilePath, tarFilePath.getFileName().toString().replabc(".tar", ""), category);
        }
    }
    
    /**
     * Create a small sample PDF file (1-2 KB)
     */
    private File createSamplePDF() throws IOException {
        File tempPdf = File.createTempFile("sample_document_", ".pdf");
        tempPdf.deleteOnExit();
        
        try {
            Document document = new Document();
            PdfWriter.getInstance(document, new FileOutputStream(tempPdf));
            document.open();
            
            // Add simple content
            document.add(new Paragraph("Sample Document"));
            document.add(new Paragraph("This is a generated sample PDF for testing purposes."));
            document.add(new Paragraph("Generated by WawaSampleDataGenerator"));
            document.add(new Paragraph("File ID: " + System.currentTimeMillis()));
            
            document.close();
            
            System.out.println("Created sample PDF: " + tempPdf.getAbsolutePath() + 
                             " (" + (tempPdf.length() / 1024) + " KB)");
            
        } catch (DocumentException e) {
            throw new IOException("Failed to create sample PDF", e);
        }
        
        return tempPdf;
    }
    
    /**
     * Add directory entry to TAR
     */
    private void addDirectoryToTar(TarArchiveOutputStream tarOut, String dirName) throws IOException {
        TarArchiveEntry entry = new TarArchiveEntry(dirName);
        entry.setMode(TarArchiveEntry.DEFAULT_DIR_MODE);
        tarOut.putArchiveEntry(entry);
        tarOut.closeArchiveEntry();
    }
    
    /**
     * Add file to TAR
     */
    private void addFileToTar(TarArchiveOutputStream tarOut, File file, String entryName) 
            throws IOException {
        
        TarArchiveEntry entry = new TarArchiveEntry(file, entryName);
        tarOut.putArchiveEntry(entry);
        
        try (FileInputStream fis = new FileInputStream(file)) {
            byte[] buffer = new byte[1024];
            int len;
            while ((len = fis.read(buffer)) > 0) {
                tarOut.write(buffer, 0, len);
            }
        }
        
        tarOut.closeArchiveEntry();
    }
    
    /**
     * Extract subcategory from category name
     * E.g., "category_claimcenter" -> "claimcenter"
     */
    private String extractSubcategory(String category) {
        if (category.startsWith("category_")) {
            return category.substring("category_".length());
        }
        return category;
    }
    
    /**
     * Find existing TAR file for given category
     */
    private Path findExistingTar(String category) {
        if (samplePath == null || samplePath.isEmpty()) {
            return null;
        }
        
        // Look for TAR files in sample path
        File categoryDir = new File(samplePath, category);
        if (!categoryDir.exists() || !categoryDir.isDirectory()) {
            return null;
        }
        
        // Find first TAR file
        File[] tarFiles = categoryDir.listFiles((dir, name) -> name.endsWith(".tar"));
        if (tarFiles != null && tarFiles.length > 0) {
            return tarFiles[0].toPath();
        }
        
        return null;
    }
    
    /**
     * Get typical doc_filepath for category (for REUSE_EXISTING strategy)
     */
    private String getTypicalDocFilePath(String category) {
        String subcategory = extractSubcategory(category);
        return "docrepo/" + subcategory + "/sample_document_" + 
               java.util.UUID.randomUUID().toString() + ".pdf";
    }
}

================================

package com.abcdefg.abc.generator.utils;

import java.util.ArrayList;
import java.util.List;
import java.util.Random;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import com.abcdefg.abc.generator.config.ConfigurationLoader;

/**
 * Thread-safe manager for generating unique IDs
 * - wdr_id (file_id): Sequential unique across all CSVs
 * - doc_uid: "OT:{unique_number}" format (OpenText) or "{GUID}" (FileNet)
 * - claim_num: Grouped assignment (each claim has 10-15 documents)
 */
public class UniqueIDManager {
    
    private final AtomicLong currentWdrId;
    private final AtomicLong currentDocUid;
    private final List<String> claimNumbersPool;
    private final Random random;
    
    // Claim number grouping (realistic: each claim has multiple documents)
    private final AtomicInteger currentClaimIndex;
    private final AtomicInteger documentsForCurrentClaim;
    private int documentsPerClaim; // Will be randomly set between min and max
    private final int minDocumentsPerClaim;
    private final int maxDocumentsPerClaim;
    
    public UniqueIDManager(ConfigurationLoader config) {
        // Initialize starting IDs from config
        long startWdrId = config.getLongProperty("generator.start.wdr.id", 1L);
        long startDocUid = config.getLongProperty("generator.start.doc.uid", 1000001L);
        
        this.currentWdrId = new AtomicLong(startWdrId);
        this.currentDocUid = new AtomicLong(startDocUid);
        this.random = new Random();
        
        // Generate pool of claim numbers
        int poolSize = config.getIntProperty("generator.claim.numbers.pool.size", 100000);
        String prefix = config.getProperty("generator.claim.numbers.prefix", "00000000");
        
        this.claimNumbersPool = generateClaimNumbersPool(poolSize, prefix);
        
        // Initialize claim grouping (each claim has multiple documents)
        this.minDocumentsPerClaim = config.getIntProperty("generator.documents.per.claim.min", 10);
        this.maxDocumentsPerClaim = config.getIntProperty("generator.documents.per.claim.max", 15);
        this.currentClaimIndex = new AtomicInteger(0);
        this.documentsForCurrentClaim = new AtomicInteger(0);
        this.documentsPerClaim = minDocumentsPerClaim + random.nextInt(maxDocumentsPerClaim - minDocumentsPerClaim + 1);
        
        System.out.println("UniqueIDManager initialized:");
        System.out.println("  Start wdr_id: " + startWdrId);
        System.out.println("  Start doc_uid: " + startDocUid);
        System.out.println("  Claim numbers pool size: " + claimNumbersPool.size());
        System.out.println("  Documents per claim: " + minDocumentsPerClaim + "-" + maxDocumentsPerClaim);
    }
    
    /**
     * Generate pool of realistic claim numbers
     * Format: 19 digits like "0000000000001234567"
     */
    private List<String> generateClaimNumbersPool(int poolSize, String prefix) {
        List<String> pool = new ArrayList<>(poolSize);
        
        for (int i = 0; i < poolSize; i++) {
            // Generate 19-digit claim number: "0000000000001234567"
            // Use random 10-digit number (1000000000 to 9999999999)
            long randomNum = 1000000000L + random.nextLong() % 9000000000L;
            if (randomNum < 0) randomNum = -randomNum; // Handle negative
            
            // Format: 19 digits total with leading zeros
            String claimNum = String.format("%019d", randomNum);
            pool.add(claimNum);
        }
        
        return pool;
    }
    
    /**
     * Get next unique wdr_id (file_id)
     * Thread-safe increment
     * Returns 10-digit padded string (e.g., "0000000001")
     */
    public String getNextWdrId() {
        long id = currentWdrId.getAndIncrement();
        return String.format("%010d", id); // 10 digits with leading zeros
    }
    
    /**
     * Get next unique doc_uid based on orig_rep value
     * Thread-safe increment/generation
     * 
     * @param origRep "opentext" or "filenet"
     * @return "OT:{number}" for opentext, "{GUID}" for filenet
     */
    public String getNextDocUid(String origRep) {
        if ("filenet".equalsIgnoreCase(origRep)) {
            // FileNet format: {7F21D554-A295-4772-8D4D-843D90C56FE4}
            return "{" + java.util.UUID.randomUUID().toString().toUpperCase() + "}";
        } else {
            // OpenText format: OT:1000001
            long uid = currentDocUid.getAndIncrement();
            return "OT:" + uid;
        }
    }
    
    /**
     * Get the next claim number with realistic grouping
     * Each claim will have 10-15 documents before moving to next claim
     * Thread-safe implementation
     */
    public synchronized String getNextClaimNumber() {
        // Get current claim number
        String claimNumber = claimNumbersPool.get(currentClaimIndex.get());
        
        // Increment document counter for current claim
        int docsCount = documentsForCurrentClaim.incrementAndGet();
        
        // Check if we've reached the limit for this claim
        if (docsCount >= documentsPerClaim) {
            // Move to next claim
            currentClaimIndex.incrementAndGet();
            
            // Reset counter for new claim
            documentsForCurrentClaim.set(0);
            
            // Randomly determine documents for next claim (10-15)
            documentsPerClaim = minDocumentsPerClaim + random.nextInt(maxDocumentsPerClaim - minDocumentsPerClaim + 1);
            
            // Wrap around if we've exhausted the pool (shouldn't happen with 100K claims)
            if (currentClaimIndex.get() >= claimNumbersPool.size()) {
                currentClaimIndex.set(0);
            }
        }
        
        return claimNumber;
    }
    
    /**
     * @deprecated Use getNextClaimNumber() instead for realistic claim grouping
     */
    @Deprecated
    public synchronized String getRandomClaimNumber() {
        return getNextClaimNumber();
    }
    
    /**
     * Get next unique Filenet ID
     * Format: {UUID-like string}
     * @deprecated Use getNextDocUid("filenet") instead
     */
    @Deprecated
    public String getNextFilenetId() {
        return "{" + java.util.UUID.randomUUID().toString().toUpperCase() + "}";
    }
    
    /**
     * Get current wdr_id value (for statistics/reporting)
     */
    public long getCurrentWdrIdValue() {
        return currentWdrId.get();
    }
    
    /**
     * Get current doc_uid value (for statistics/reporting)
     */
    public long getCurrentDocUidValue() {
        return currentDocUid.get();
    }
    
    /**
     * Get total unique documents generated so far
     */
    public long getTotalDocumentsGenerated() {
        return currentWdrId.get() - 1;
    }
}



=========================================================

 <!-- CSV Processing -->
    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-csv</artifactId>
      <version>1.10.0</version>
    </dependency>
    
    <!-- TAR File Creation -->
    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-compress</artifactId>
      <version>1.24.0</version>
    </dependency>
    
    <!-- PDF Generation -->
    <dependency>
      <groupId>com.itextpdf</groupId>
      <artifactId>itextpdf</artifactId>
      <version>5.5.13.3</version>
    </dependency>
    
    <!-- JSON Processing -->
    <dependency>
      <groupId>com.google.code.gson</groupId>
      <artifactId>gson</artifactId>
      <version>2.10.1</version>
    </dependency>
    
    <!-- Guava (required by Apache Commons Compress) -->
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>32.1.3-jre</version>
    </dependency>
	
===============================================================================

# ===========================================================
# WawaSampleDataGenerator - Configuration
# ===========================================================
# Last Updated: 2025-10-12
# Purpose: Generate large-scale production-like sample data
# ===========================================================

# ===== VOLUME CONFIGURATION =====
# Number of CSV files to generate per category
generator.files.per.category=1000

# Number of rows (documents) per CSV file
generator.rows.per.file=10000

# Categories to generate
generator.categories=category_claimcenter,category_dcpfnol,category_dcpfnolclaimcenter

# ===== TAR FILE STRATEGY =====
# Strategy: SINGLE_PDF (one small PDF in each TAR) or REUSE_EXISTING (use existing TAR files)
generator.tar.strategy=SINGLE_PDF

# Path to existing sample data (for REUSE_EXISTING strategy)
generator.tar.sample.path=D:/Rameshwar/abcdefg_Template_BCKUP/Source

# Sample PDF file for SINGLE_PDF strategy (will be created if not exists)
generator.tar.sample.pdf.size.kb=2

# ===== DATA GENERATION SETTINGS =====
# Starting IDs (will auto-increment)
generator.start.wdr.id=1
generator.start.doc.uid=1000001
generator.start.archiveid=1

# Claim number generation (19 digits total)
generator.claim.numbers.pool.size=100000
# Prefix is not used anymore - claim numbers are 19 digits with leading zeros

# Documents per claim (realistic grouping: each claim has multiple documents)
generator.documents.per.claim.min=10
generator.documents.per.claim.max=15

# Originating repository mix (percentage)
generator.orig.rep.opentext.percentage=80
generator.orig.rep.filenet.percentage=20

# ===== FILE NAMING PATTERNS =====
# Date pattern for filenames (starting date)
generator.filename.date.start=10_2
generator.filename.date.increment=true

# ===== OUTPUT CONFIGURATION =====
# Output directory
generator.output.path=D:/Rameshwar/abcdefg_GENERATED/Source/

# ===== PERFORMANCE TUNING =====
# Thread pool size for parallel file generation
generator.thread.pool.size=10

# Batch write size (rows written at once to CSV)
generator.batch.write.size=1000

# Progress reporting interval (files)
generator.progress.report.interval=50

# ===== TIMESTAMP GENERATION =====
# Timestamp range for file_creation_ts
generator.timestamp.start.year=2023
generator.timestamp.end.year=2025

# ===== MIME TYPES =====
# For SINGLE_PDF strategy
generator.single.pdf.mimetype=application/pdf

# For REUSE_EXISTING strategy (will use existing variety)

# ===== METADATA JSON TEMPLATES =====
# Author names pool (comma-separated)
generator.authors=Auto-generated,nhilfers,IAA,XactAnalysis,Jamie Garrett,Amanda Lamperts,Jennifer Fockler,David Devine,Janice Jin,Natasha Richman

# Document types pool (comma-separated)
generator.document.types=iso,photo,PDF,appraisal,First notice of loss,email_type,text_message,Video,Settlement Demand/Claimant,Email,Invoice,med_payments,onespan_other

# Source systems pool (comma-separated)
generator.source.systems=DoubleDouble,ClaimsBulkUploader,WawanesaDataCapture,Datacap

# ===== VALIDATION =====
# Enable validation after generation
generator.validation.enabled=true

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage)
generator.memory.warning.threshold=80

# ===== LOGGING =====
# Log level (info, debug, warn, error)
generator.log.level=info

# ===== NOTES =====
# Estimated Generation Time:
# - 3,000 files x 10,000 rows = 30,000,000 documents
# - With 10 threads: ~30-60 minutes (depending on I/O speed)
#
# Output Structure:
# abcdefg_GENERATED/
# âââ Source/
#     âââ category_claimcenter/ (1,000 CSVs + 1,000 TARs)
#     âââ category_dcpfnol/ (1,000 CSVs + 1,000 TARs)
#     âââ category_dcpfnolclaimcenter/ (1,000 CSVs + 1,000 TARs)
#
# Total: 3,000 CSV files + 3,000 TAR files = 6,000 files



======================================================================================================

# ===========================================================
# WawaSampleDataGenerator - TEST Configuration
# ===========================================================
# For testing with small volumes (10 files, 100 rows each)
# ===========================================================

# ===== VOLUME CONFIGURATION (SMALL FOR TESTING) =====
generator.files.per.category=3
generator.rows.per.file=100
generator.categories=category_claimcenter,category_dcpfnol,category_dcpfnolclaimcenter

# ===== TAR FILE STRATEGY =====
generator.tar.strategy=SINGLE_PDF
generator.tar.sample.path=D:/Rameshwar/abcdefg_Template_BCKUP/Source
generator.tar.sample.pdf.size.kb=2

# ===== DATA GENERATION SETTINGS =====
generator.start.wdr.id=1
generator.start.doc.uid=1000001
generator.start.archiveid=1

generator.claim.numbers.pool.size=1000
# Claim numbers are 19 digits with leading zeros

# Documents per claim (realistic grouping: each claim has multiple documents)
generator.documents.per.claim.min=10
generator.documents.per.claim.max=15

generator.orig.rep.opentext.percentage=80
generator.orig.rep.filenet.percentage=20

# ===== FILE NAMING PATTERNS =====
generator.filename.date.start=10_2
generator.filename.date.increment=true

# ===== OUTPUT CONFIGURATION =====
generator.output.path=D:/Rameshwar/abcdefg_GENERATED_TEST/Source/

# ===== PERFORMANCE TUNING =====
generator.thread.pool.size=3
generator.batch.write.size=100
generator.progress.report.interval=5

# ===== TIMESTAMP GENERATION =====
generator.timestamp.start.year=2023
generator.timestamp.end.year=2025

# ===== MIME TYPES =====
generator.single.pdf.mimetype=application/pdf

# ===== METADATA JSON TEMPLATES =====
generator.authors=Auto-generated,nhilfers,IAA,XactAnalysis,Jamie Garrett
generator.document.types=iso,photo,PDF,appraisal,First notice of loss,email_type
generator.source.systems=DoubleDouble,ClaimsBulkUploader,WawanesaDataCapture

# ===== VALIDATION =====
generator.validation.enabled=true

# ===== MEMORY MANAGEMENT =====
generator.memory.warning.threshold=80

# ===== LOGGING =====
generator.log.level=info

*****************************************************************************************************************
package com.abcdefg.abc;

import java.io.File;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.connection.ConnectionManager;
import com.abcdefg.abc.constants.Constants;
import com.abcdefg.abc.reporting.GlobalProcessingReport;
import com.abcdefg.abc.utils.Utils;

public class DataTransformService {

    private static final Logger Logger = LogManager.getLogger(DataTransformService.class);

    public static void main(String[] args) {

        Logger.info("===== DataTransformUtility STARTED =====");
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        final ConnectionManager[] connManagerRef = new ConnectionManager[1];
        final GlobalProcessingReport report = new GlobalProcessingReport();
        final AtomicInteger totalFilesQueued = new AtomicInteger(0);

        try {
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            // Configure thread pool size (configurable via properties)
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService; // Assign to non-final variable for later use
            Logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Create final references for lambdas/shutdown hook
            final ExecutorService executorRef = executorService;
            
            // Start progress monitoring (configurable interval)
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                final ScheduledExecutorService progressMonitorRef = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        report.logProgress();
                        
                        // Log thread pool stats if executor is ThreadPoolExecutor
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            Logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            report.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        Logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                Logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            }
            
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination (e.g., Ctrl+C)
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                Logger.info("Shutdown hook triggered - cleaning up resources...");
                
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        Logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                Logger.info("Shutdown hook completed");
            }));

            System.out.println("JRE : " + System.getProperty("java.library.path"));
            System.out.println("config : " + System.getProperty("config.file"));
            System.out.println("log4j : " + System.getProperty("log4j.configurationFile"));
            String base_path = config.getProperty("app.base_path");
            String category_types = config.getProperty("app.category_types");

            if (isValidDirectory(base_path, Constants.sourceFolder)
                    && isValidDirectory(base_path, Constants.inprocessFolder)
                    && isValidDirectory(base_path, Constants.transformedFolder)
                    && isValidDirectory(base_path, Constants.completedFolder)
                    && isValidDirectory(base_path, Constants.errorFolder)
                    && isValidDirectory(base_path, Constants.archiveFolder)) {

                System.out.println("All Required directories are EXIST and VALID under base path '" + base_path + "'");

                if (category_types != null && category_types.trim().length() > 0) {
                    connManagerRef[0] = new ConnectionManager(config);
                    String[] folders = category_types.split(",");
                    
                    // First pass: count total files to process
                    Logger.info("Counting total files to process...");
                    for (int i = 0; i < folders.length; i++) {
                        if (folders[i] != null && isValidDirectory(base_path,
                                Constants.sourceFolder + File.separator + folders[i].trim())) {
                            String folderName = folders[i].trim();
                            try (DirectoryStream<Path> stream = Files.newDirectoryStream(
                                    Paths.get(base_path + File.separator + Constants.sourceFolder + File.separator + folderName),
                                    "*.csv")) {
                                for (Path filePath : stream) {
                                    totalFilesQueued.incrementAndGet();
                                }
                            } catch (Exception e) {
                                Logger.error("Error counting files in category: " + folderName, e);
                            }
                        }
                    }
                    report.setTotalFilesQueued(totalFilesQueued.get());
                    Logger.info("Total files to process: {}", totalFilesQueued.get());
                    
                    // Reset counter for actual processing
                    totalFilesQueued.set(0);

                    for (int i = 0; i < folders.length; i++) {
                        if (folders[i] != null && isValidDirectory(base_path,
                                Constants.sourceFolder + File.separator + folders[i].trim())) {
                            String folderName = folders[i].trim();

                            // Use try-with-resources to ensure DirectoryStream is properly closed
                            try (DirectoryStream<Path> directoryStream = Files.newDirectoryStream(
                                    Paths.get(base_path + File.separator + Constants.sourceFolder + File.separator + folderName),
                                    "*.csv")) {

                                Logger.info("Processing category: " + folderName);
                                System.out.println(folderName + " category processing launched...");
                                
                                int fileCount = 0;
                                for (Path filePath : directoryStream) {
                                    fileCount++;
                                    totalFilesQueued.incrementAndGet();
                                    
                                    try {
                                        Utils utils = new Utils(base_path);
                                        executor.submit(() -> {
                                            boolean success = false;
                                            try {
                                                Logger.info("Processing file: " + filePath.getFileName());
                                                success = utils.processCategoryFiles(filePath, folderName, connManagerRef[0]);
                                                
                                                if (success) {
                                                    // Record successful file processing
                                                    // Note: Utils.processCategoryFiles should be modified to return ProcessingResult
                                                    // For now, we just count the file as processed
                                                    report.recordFileSuccess(folderName, new GlobalProcessingReport.ProcessingResult());
                                                } else {
                                                    report.recordFileFailure(folderName, "Processing returned false");
                                                }
                                                
                                            } catch (Exception e) {
                                                Logger.error("Failed to process file: " + filePath.getFileName(), e);
                                                System.err.println("Failed to process file: " + filePath.getFileName());
                                                e.printStackTrabc();
                                                report.recordFileFailure(folderName, e.getMessage());
                                            }
                                        });
                                    } catch (Exception e) {
                                        Logger.error("Failed to submit file for processing: " + filePath.getFileName(), e);
                                        System.err.println("Failed to process file: " + filePath.getFileName());
                                        e.printStackTrabc();
                                        report.recordFileFailure(folderName, "Failed to submit: " + e.getMessage());
                                    }
                                }
                                
                                if (fileCount == 0) {
                                    Logger.info("No CSV files found in category: " + folderName);
                                    System.out.println("No Files found for Data transformation in " + folderName);
                                } else {
                                    Logger.info("Submitted " + fileCount + " files for processing in category: " + folderName);
                                }

                            } catch (Exception e2) {
                                Logger.error("Error processing category: " + folderName, e2);
                                e2.printStackTrabc();
                            }

                        } else {
                            Logger.warn("Invalid category folder: " + folders[i]);
                            System.out.println("mentioned category type " + folders[i] + " is not valid folder under "
                                    + base_path + Constants.sourceFolder);
                        }
                    }
                } else {
                    System.out.println("No valid category_types found in properties file to execute this program, terminated!!!");
                }

            } else {
                System.out.println("Required directories are missing under the basepath '" + base_path + "'");
                System.out.println("Please make sure '" + Constants.sourceFolder + "', '" + Constants.inprocessFolder
                        + "', '" + Constants.transformedFolder + "', '" + Constants.completedFolder + "', '"
                        + Constants.errorFolder + "', '" + Constants.archiveFolder + "' folders exist under base path '" + base_path + "'");
            }

        } catch (Exception e) {
            Logger.error("Error during processing", e);
            e.printStackTrabc();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    Logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                } catch (InterruptedException e) {
                    progressMonitor.shutdownNow();
                }
            }
            
            // Shutdown ExecutorService grabcfully
            if (executor != null) {
                try {
                    Logger.info("Shutting down executor service...");
                    executor.shutdown(); // Disable new tasks from being submitted
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        Logger.warn("Executor did not terminate in time, forcing shutdown...");
                        executor.shutdownNow(); // Cancel currently executing tasks
                        
                        // Wait a while for tasks to respond to being cancelled
                        if (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                            Logger.error("Executor did not terminate after forced shutdown");
                        }
                    }
                    Logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    Logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManagerRef[0] != null) {
                try {
                    Logger.info("Closing database connection pool...");
                    connManagerRef[0].close();
                    Logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    Logger.error("Error closing connection manager", e);
                    e.printStackTrabc();
                }
            }
            
            // Write final summary report
            try {
                Logger.info("===== FINAL SUMMARY =====");
                Logger.info(report.getSummary());
                
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                report.writeReportToFile(basePath);
                Logger.info("Summary report written to: {}", basePath);
            } catch (Exception e) {
                Logger.error("Failed to write summary report", e);
            }
            
            Logger.info("===== DataTransformUtility COMPLETED =====");
        }
    }

    private static boolean isValidDirectory(String base_path, String folderName) {
        boolean isExist = false;
        try {
            File dir = new File(base_path + folderName);
            if (dir.exists() && dir.isDirectory()) {
                isExist = true;
            } else {
                System.out.println("Expected Directory not exist : " + (base_path + File.separator + folderName));
            }
        } catch (Exception e) {
            e.printStackTrabc();
        }
        return isExist;
    }
}


==========================================

package com.abcdefg.abc.monitoring;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Monitors JVM memory usage and provides utilities for memory management
 * Helps prevent OutOfMemoryError by tracking and reporting memory usage
 */
public class MemoryMonitor {
    
    private static final Logger logger = LogManager.getLogger(MemoryMonitor.class);
    private Runtime runtime = Runtime.getRuntime();
    private double warningThreshold = 85.0; // Warn at 85% memory usage
    
    /**
     * Default constructor with 85% warning threshold
     */
    public MemoryMonitor() {
        this(85.0);
    }
    
    /**
     * Constructor with custom warning threshold
     * @param warningThreshold Percentage (0-100) at which to log warnings
     */
    public MemoryMonitor(double warningThreshold) {
        this.warningThreshold = warningThreshold;
        logger.info("MemoryMonitor initialized with warning threshold: {}%", warningThreshold);
    }
    
    /**
     * Log current memory usage statistics
     */
    public void logMemoryUsage() {
        long maxMemory = runtime.maxMemory() / (1024 * 1024);     // MB
        long totalMemory = runtime.totalMemory() / (1024 * 1024); // MB
        long freeMemory = runtime.freeMemory() / (1024 * 1024);   // MB
        long usedMemory = totalMemory - freeMemory;
        
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        logger.info("Memory: Used={}MB Free={}MB Total={}MB Max={}MB ({:.1f}% used)",
                   usedMemory, freeMemory, totalMemory, maxMemory, usagePercent);
        
        if (usagePercent > warningThreshold) {
            logger.warn("HIGH MEMORY USAGE: {:.1f}% - Consider increasing heap size or reducing batch sizes", 
                       usagePercent);
        }
    }
    
    /**
     * Check if memory usage is above warning threshold
     * @return true if memory usage exceeds warning threshold
     */
    public boolean isMemoryLow() {
        long maxMemory = runtime.maxMemory();
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long usedMemory = totalMemory - freeMemory;
        
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        return usagePercent > warningThreshold;
    }
    
    /**
     * Get current memory usage percentage
     * @return Current memory usage as percentage (0-100)
     */
    public double getMemoryUsagePercent() {
        long maxMemory = runtime.maxMemory();
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long usedMemory = totalMemory - freeMemory;
        
        return (usedMemory * 100.0) / maxMemory;
    }
    
    /**
     * Get current used memory in megabytes
     * @return Used memory in MB
     */
    public long getUsedMemoryMB() {
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        return (totalMemory - freeMemory) / (1024 * 1024);
    }
    
    /**
     * Get maximum available memory in megabytes
     * @return Max memory in MB
     */
    public long getMaxMemoryMB() {
        return runtime.maxMemory() / (1024 * 1024);
    }
    
    /**
     * Get free memory in megabytes
     * @return Free memory in MB
     */
    public long getFreeMemoryMB() {
        return runtime.freeMemory() / (1024 * 1024);
    }
    
    /**
     * Force garbage collection
     * Note: This is a suggestion to the JVM, not a guarantee
     */
    public void forceGarbageCollection() {
        logger.info("Requesting garbage collection...");
        long beforeUsed = getUsedMemoryMB();
        System.gc();
        try { 
            Thread.sleep(100); // Give GC time to run
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        long afterUsed = getUsedMemoryMB();
        long freed = beforeUsed - afterUsed;
        logger.info("Garbage collection completed. Freed approximately {}MB", freed);
    }
    
    /**
     * Get formatted memory status string
     * @return Memory status as formatted string
     */
    public String getMemoryStatus() {
        long maxMemory = runtime.maxMemory() / (1024 * 1024);
        long totalMemory = runtime.totalMemory() / (1024 * 1024);
        long freeMemory = runtime.freeMemory() / (1024 * 1024);
        long usedMemory = totalMemory - freeMemory;
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        return String.format("Memory: %dMB/%dMB (%.1f%% used)", usedMemory, maxMemory, usagePercent);
    }
    
    /**
     * Log detailed memory breakdown
     */
    public void logDetailedMemoryInfo() {
        long maxMemory = runtime.maxMemory() / (1024 * 1024);
        long totalMemory = runtime.totalMemory() / (1024 * 1024);
        long freeMemory = runtime.freeMemory() / (1024 * 1024);
        long usedMemory = totalMemory - freeMemory;
        long availableMemory = maxMemory - usedMemory;
        
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        logger.info("===== DETAILED MEMORY INFO =====");
        logger.info("Max Heap Size:      {}MB", maxMemory);
        logger.info("Current Heap Size:  {}MB", totalMemory);
        logger.info("Used Memory:        {}MB", usedMemory);
        logger.info("Free Memory:        {}MB", freeMemory);
        logger.info("Available Memory:   {}MB", availableMemory);
        logger.info("Usage Percentage:   {:.2f}%", usagePercent);
        logger.info("Warning Threshold:  {:.2f}%", warningThreshold);
        
        if (usagePercent > warningThreshold) {
            logger.warn("Memory usage is above warning threshold!");
            logger.warn("Consider:");
            logger.warn("  - Increasing JVM heap size (-Xmx)");
            logger.warn("  - Reducing batch size (db.batch.insert.size)");
            logger.warn("  - Reducing thread pool size (app.thread.pool.size)");
        }
        logger.info("================================");
    }
}

==================================================

package com.abcdefg.abc.reporting;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Thread-safe global statistics collector for DataTransformUtility
 * Aggregates processing statistics from all threads and generates summary reports
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // Global counters (thread-safe)
    private final AtomicLong totalFilesQueued = new AtomicLong(0);
    private final AtomicLong totalFilesProcessed = new AtomicLong(0);
    private final AtomicLong totalFilesFailed = new AtomicLong(0);
    private final AtomicLong totalRowsProcessed = new AtomicLong(0);
    private final AtomicLong totalRowsSuccess = new AtomicLong(0);
    private final AtomicLong totalRowsFailure = new AtomicLong(0);
    private final AtomicLong totalRowsSkipped = new AtomicLong(0);
    
    // Per-category statistics (thread-safe)
    private final ConcurrentHashMap<String, CategoryStats> categoryStats = new ConcurrentHashMap<>();
    
    // Timing
    private final long startTime;
    private long endTime;
    
    // Thread pool stats
    private volatile int threadPoolSize = 0;
    private volatile int maxActiveThreads = 0;
    private volatile long peakQueueSize = 0;
    
    public GlobalProcessingReport() {
        this.startTime = System.currentTimeMillis();
        logger.info("GlobalProcessingReport initialized");
    }
    
    /**
     * Set total number of files to process (call at startup)
     */
    public void setTotalFilesQueued(long total) {
        totalFilesQueued.set(total);
        logger.info("Total files queued for processing: {}", total);
    }
    
    /**
     * Record a successfully processed file
     */
    public void recordFileSuccess(String category, ProcessingResult result) {
        totalFilesProcessed.incrementAndGet();
        totalRowsProcessed.addAndGet(result.totalRows);
        totalRowsSuccess.addAndGet(result.successCount);
        totalRowsFailure.addAndGet(result.failureCount);
        totalRowsSkipped.addAndGet(result.skippedEmptyLines);
        
        // Update category stats
        categoryStats.computeIfAbsent(category, k -> new CategoryStats(category))
                     .recordFile(result);
        
        if (logger.isDebugEnabled()) {
            logger.debug("File processed: category={}, rows={}, success={}, failed={}", 
                        category, result.totalRows, result.successCount, result.failureCount);
        }
    }
    
    /**
     * Record a failed file (couldn't process)
     */
    public void recordFileFailure(String category, String errorReason) {
        totalFilesFailed.incrementAndGet();
        categoryStats.computeIfAbsent(category, k -> new CategoryStats(category))
                     .recordFileFailure(errorReason);
        
        logger.warn("File failed: category={}, reason={}", category, errorReason);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, long queueSize) {
        this.threadPoolSize = poolSize;
        this.maxActiveThreads = Math.max(this.maxActiveThreads, activeThreads);
        this.peakQueueSize = Math.max(this.peakQueueSize, queueSize);
    }
    
    /**
     * Log current progress to console/log file
     */
    public void logProgress() {
        long processed = totalFilesProcessed.get();
        long failed = totalFilesFailed.get();
        long total = totalFilesQueued.get();
        long remaining = total - processed - failed;
        
        double pct = (total > 0) ? ((processed + failed) * 100.0 / total) : 0;
        long elapsed = System.currentTimeMillis() - startTime;
        long eta = ((processed + failed) > 0) ? 
            (elapsed * remaining / (processed + failed)) : 0;
        
        logger.info("===== PROGRESS REPORT =====");
        logger.info("Files: {}/{} processed ({:.1f}%) | {} failed | {} remaining | ETA: {} min",
                   processed, total, String.format("%.1f", pct), failed, remaining, eta / 60000);
        logger.info("Rows: {} success | {} failed | {} skipped",
                   totalRowsSuccess.get(), totalRowsFailure.get(), totalRowsSkipped.get());
        
        double successRate = (totalRowsProcessed.get() > 0) ? 
            (totalRowsSuccess.get() * 100.0 / totalRowsProcessed.get()) : 0;
        logger.info("Success Rate: {:.2f}%", String.format("%.2f", successRate));
        logger.info("===========================");
    }
    
    /**
     * Finalize and write comprehensive summary report to file
     */
    public void writeReportToFile(String outputDirectory) {
        endTime = System.currentTimeMillis();
        long durationMs = endTime - startTime;
        
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd_HHmmss");
        String timestamp = sdf.format(new Date(endTime));
        String reportFileName = outputDirectory + "/DataTransformUtility_Summary_Report_" + timestamp + ".csv";
        
        logger.info("Writing summary report to: {}", reportFileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportFileName))) {
            
            // ===== EXECUTION SUMMARY =====
            writer.write("Execution Summary\n");
            writer.write("Metric,Value\n");
            
            SimpleDateFormat fullDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
            writer.write("Start Time," + fullDateFormat.format(new Date(startTime)) + "\n");
            writer.write("End Time," + fullDateFormat.format(new Date(endTime)) + "\n");
            writer.write("Total Duration," + formatDuration(durationMs) + "\n");
            writer.write("Total Files Queued," + totalFilesQueued.get() + "\n");
            writer.write("Total Files Processed," + totalFilesProcessed.get() + "\n");
            writer.write("Total Files Failed," + totalFilesFailed.get() + "\n");
            writer.write("Total Rows Processed," + totalRowsProcessed.get() + "\n");
            writer.write("Total Rows Inserted," + totalRowsSuccess.get() + "\n");
            writer.write("Total Rows Failed," + totalRowsFailure.get() + "\n");
            writer.write("Total Rows Skipped," + totalRowsSkipped.get() + "\n");
            
            double successRate = (totalRowsProcessed.get() > 0) ? 
                (totalRowsSuccess.get() * 100.0 / totalRowsProcessed.get()) : 0;
            writer.write("Success Rate," + String.format("%.2f%%", successRate) + "\n");
            
            long rowsPerSecond = (durationMs > 0) ? 
                (totalRowsSuccess.get() * 1000 / durationMs) : 0;
            writer.write("Average Processing Speed," + rowsPerSecond + " rows/sec\n");
            
            double avgFileTime = (totalFilesProcessed.get() > 0) ? 
                (durationMs / 1000.0 / totalFilesProcessed.get()) : 0;
            writer.write("Average File Processing Time," + String.format("%.2f", avgFileTime) + " seconds\n");
            
            writer.write("\n");
            
            // ===== CATEGORY BREAKDOWN =====
            writer.write("Category Breakdown\n");
            writer.write("Category,Files Processed,Files Failed,Rows Success,Rows Failed,Success Rate\n");
            
            for (Map.Entry<String, CategoryStats> entry : categoryStats.entrySet()) {
                CategoryStats stats = entry.getValue();
                double catSuccessRate = (stats.totalRows > 0) ? 
                    (stats.successRows * 100.0 / stats.totalRows) : 0;
                
                writer.write(String.format("%s,%d,%d,%d,%d,%.2f%%\n",
                    stats.categoryName,
                    stats.filesProcessed.get(),
                    stats.filesFailed.get(),
                    stats.successRows,
                    stats.failedRows,
                    catSuccessRate));
            }
            
            writer.write("\n");
            
            // ===== THREAD POOL STATISTICS =====
            writer.write("Thread Pool Statistics\n");
            writer.write("Metric,Value\n");
            writer.write("Thread Pool Size," + threadPoolSize + "\n");
            writer.write("Max Active Threads," + maxActiveThreads + "\n");
            writer.write("Peak Queue Size," + peakQueueSize + "\n");
            
            writer.write("\n");
            
            // ===== MEMORY STATISTICS =====
            Runtime runtime = Runtime.getRuntime();
            long maxMemoryMB = runtime.maxMemory() / (1024 * 1024);
            long totalMemoryMB = runtime.totalMemory() / (1024 * 1024);
            long freeMemoryMB = runtime.freeMemory() / (1024 * 1024);
            long usedMemoryMB = totalMemoryMB - freeMemoryMB;
            
            writer.write("Memory Statistics\n");
            writer.write("Metric,Value\n");
            writer.write("Current Used Memory," + usedMemoryMB + "MB\n");
            writer.write("Max Heap Size," + maxMemoryMB + "MB\n");
            
            writer.write("\n");
            
            logger.info("Summary report written successfully: {}", reportFileName);
            
        } catch (IOException e) {
            logger.error("Failed to write summary report", e);
        }
    }
    
    /**
     * Get current statistics as formatted string
     */
    public String getSummary() {
        long processed = totalFilesProcessed.get();
        long total = totalFilesQueued.get();
        double pct = (total > 0) ? (processed * 100.0 / total) : 0;
        
        return String.format("Files: %d/%d (%.1f%%) | Rows: Success=%d Failed=%d",
                           processed, total, pct, 
                           totalRowsSuccess.get(), totalRowsFailure.get());
    }
    
    /**
     * Format duration in human-readable format
     */
    private String formatDuration(long durationMs) {
        long seconds = durationMs / 1000;
        long minutes = seconds / 60;
        long hours = minutes / 60;
        
        if (hours > 0) {
            return String.format("%d hours %d minutes", hours, minutes % 60);
        } else if (minutes > 0) {
            return String.format("%d minutes %d seconds", minutes, seconds % 60);
        } else {
            return String.format("%d seconds", seconds);
        }
    }
    
    // ===== INNER CLASS: ProcessingResult =====
    
    /**
     * Result data from processing a single file
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
    }
    
    // ===== INNER CLASS: CategoryStats =====
    
    /**
     * Statistics for a specific category
     */
    private static class CategoryStats {
        private final String categoryName;
        private final AtomicLong filesProcessed = new AtomicLong(0);
        private final AtomicLong filesFailed = new AtomicLong(0);
        private long totalRows = 0;
        private long successRows = 0;
        private long failedRows = 0;
        
        public CategoryStats(String categoryName) {
            this.categoryName = categoryName;
        }
        
        public synchronized void recordFile(ProcessingResult result) {
            filesProcessed.incrementAndGet();
            totalRows += result.totalRows;
            successRows += result.successCount;
            failedRows += result.failureCount;
        }
        
        public void recordFileFailure(String errorReason) {
            filesFailed.incrementAndGet();
        }
    }
}



====================================================

package com.abcdefg.abc.utils;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.lang.reflect.Field;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;

import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.json.JSONObject;
import com.abcdefg.abc.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.connection.ConnectionManager;
import com.abcdefg.abc.constants.Constants;
import com.abcdefg.abc.model.ClaimDocumentDTO;

public class Utils {

    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private String base_path;
    private PropertiesConfigLoader config;

    public Utils(String base_path) {
        this.config = PropertiesConfigLoader.getInstance();
        this.base_path = base_path;
    }

    private boolean isFileExistAndValid(String filePath) {
        try {
            File file = new File(filePath);
            if (file.exists()) {
                return true;
            } else {
                logger.warn("Expected file does not exist: {}", filePath);
                return false;
            }
        } catch (Exception e) {
            logger.error("Error checking file existence: {}", filePath, e);
            return false;
        }
    }

    public static String getFileNameWithoutExtension(String fileName) {
        if (fileName == null || fileName.isEmpty()) {
            return fileName;
        }

        int dotIndex = fileName.lastIndexOf('.');
        if (dotIndex > 0 && dotIndex < fileName.length() - 1) {
            return fileName.substring(0, dotIndex);
        }

        return fileName;
    }

    public boolean processCategoryFiles(Path metadata_file, String category_type, ConnectionManager connectionManager) {
        String metadata_file_path = null;
        String tar_file_path = null;
        
        try {
            metadata_file_path = metadata_file.getParent() + File.separator + metadata_file.getFileName().toString();
            String changed_tar_file = metadata_file.getFileName().toString().replabc(Constants.metadata, Constants.documents);
            tar_file_path = metadata_file.getParent() + File.separator + getFileNameWithoutExtension(changed_tar_file) + ".tar";
            
            logger.info("================================================================================");
            logger.info("Processing File Pair:");
            logger.info("  CSV: {}", metadata_file_path);
            logger.info("  TAR: {}", tar_file_path);
            logger.info("================================================================================");
            
            // Validate both files exist
            if (!isFileExistAndValid(metadata_file_path) || !isFileExistAndValid(tar_file_path)) {
                logger.error("File pair incomplete - skipping processing");
                return false;
            }
            
            // STEP 1: Extract TAR file (critical - must succeed before CSV processing)
            logger.info("[STEP 1] Extracting TAR file...");
            String tarExtractionPath = getExtractOfTarFile(tar_file_path, category_type);
            
            if (tarExtractionPath == null) {
                // TAR extraction failed - move both files to Error folder
                logger.error("TAR extraction failed - moving files to Error folder");
                moveFilesToError(tar_file_path, metadata_file_path, category_type, 
                    "TAR extraction failed - cannot process without content files");
                return false;
            }
            
            logger.info("[SUCCESS] TAR extracted successfully to: {}", tarExtractionPath);
            
            // STEP 2: Parse and transform CSV data (row-by-row with success/failure tracking)
            logger.info("[STEP 2] Processing CSV data...");
            ProcessingResult result;
            
            // Extract TAR filename for database tracking
            File tarFile = new File(tar_file_path);
            String tarFileName = tarFile.getName();
            
            try (Connection conn = connectionManager.getConnection()) {
                result = parseAndTransformDataWithTracking(metadata_file, category_type, conn, tarFileName, tarExtractionPath);
                logger.info("[RESULT] Processed: {} rows", result.totalRows);
                logger.info("         Success: {}", result.successCount);
                logger.info("         Failed: {}", result.failureCount);
            }
            
            // STEP 3: Archive or delete source files after successful processing
            if (result.totalRows > 0) {
                logger.info("[STEP 3] Managing source files...");
                handleSourceFilesAfterProcessing(tar_file_path, metadata_file_path, category_type);
            }
            
            logger.info("[COMPLETE] File pair processing finished");
            logger.info("================================================================================");
            return true;
            
        } catch (Exception e) {
            logger.error("Unexpected error processing file pair", e);
            
            // On fatal error, try to move files to Error folder
            if (tar_file_path != null && metadata_file_path != null) {
                try {
                    moveFilesToError(tar_file_path, metadata_file_path, category_type, 
                        "Fatal error: " + e.getMessage());
                } catch (Exception moveError) {
                    logger.error("Failed to move files to Error folder: {}", moveError.getMessage(), moveError);
                }
            }
            return false;
        }
    }

    /**
     * Extracts TAR file and returns the extraction path
     * @param sourceTarPath Path to the TAR file
     * @param category_type Category type (e.g., category_claimcenter)
     * @return Full path where TAR was extracted, or null if extraction failed
     */
    private String getExtractOfTarFile(String sourceTarPath, String category_type) {
        String tempDirPath = base_path + Constants.inprocessFolder + File.separator + category_type;
        String targetDirPath = base_path + Constants.transformedFolder + File.separator + category_type;

        logger.debug("Temp extraction path: {}", tempDirPath);
        logger.debug("Target path: {}", targetDirPath);
        
        File tarFile = new File(sourceTarPath);
        if (!tarFile.exists()) {
            logger.error("TAR file not found: {}", sourceTarPath);
            return null;
        }

        String tarFileName = tarFile.getName().replabcFirst("[.][^.]+$", "");
        Path tempExtractDir = Paths.get(tempDirPath, tarFileName);
        Path targetDir = Paths.get(targetDirPath, tarFileName);

        try {
            Files.createDirectories(tempExtractDir);
            extractTar(tarFile, tempExtractDir);
            
            // Flatten if TAR contains a single root folder with the same name
            flattenSingleRootFolder(tempExtractDir);
            
            if (Files.exists(targetDir)) {
                deleteDirectory(targetDir);
            }
            Files.move(tempExtractDir, targetDir, StandardCopyOption.REPLACE_EXISTING);
            logger.info("TAR extraction and move completed successfully");
            logger.info("TAR extracted to: {}", targetDir.toString());
            
            // Return the absolute extraction path (with forward slashes for consistency)
            return targetDir.toString().replabc('\\', '/');
        } catch (IOException e) {
            logger.error("Error during TAR extraction or move: {}", e.getMessage(), e);
            return null;
        }
    }
    
    /**
     * Flattens extraction if TAR contains a single root folder
     * Example: If extracted/claim_docs/claim_docs/files exists,
     * moves contents up: extracted/claim_docs/files
     */
    private void flattenSingleRootFolder(Path extractedDir) throws IOException {
        File[] contents = extractedDir.toFile().listFiles();
        
        // Check if there's exactly one item and it's a directory
        if (contents != null && contents.length == 1 && contents[0].isDirectory()) {
            File singleFolder = contents[0];
            
            logger.debug("Found single root folder: {}", singleFolder.getName());
            
            // Move all contents of the single folder up one level
            File[] innerContents = singleFolder.listFiles();
            if (innerContents != null) {
                for (File file : innerContents) {
                    Path targetPath = extractedDir.resolve(file.getName());
                    Files.move(file.toPath(), targetPath, StandardCopyOption.REPLACE_EXISTING);
                }
                
                // Delete the now-empty single folder
                if (singleFolder.delete()) {
                    logger.info("Flattened single root folder structure");
                }
            }
        }
    }

    public static void extractTar(File tarFile, Path outputDir) throws IOException {
        try (FileInputStream fis = new FileInputStream(tarFile);
             BufferedInputStream bis = new BufferedInputStream(fis);
             TarArchiveInputStream tarInput = new TarArchiveInputStream(bis)) {

            TarArchiveEntry entry;
            while ((entry = tarInput.getNextTarEntry()) != null) {
                Path outputPath = outputDir.resolve(entry.getName()).normalize();

                // Prevent Zip Slip vulnerability
                if (!outputPath.startsWith(outputDir)) {
                    throw new IOException("Bad entry: " + entry.getName());
                }

                if (entry.isDirectory()) {
                    Files.createDirectories(outputPath);
                } else {
                    Files.createDirectories(outputPath.getParent());
                    try (OutputStream out = Files.newOutputStream(outputPath)) {
                        byte[] buffer = new byte[4096];
                        int len;
                        while ((len = tarInput.read(buffer)) != -1) {
                            out.write(buffer, 0, len);
                        }
                    }
                }
            }
        }
    }

    public static void deleteDirectory(Path path) throws IOException {
        if (Files.exists(path)) {
            Files.walk(path)
                 .sorted(Comparator.reverseOrder())
                 .map(Path::toFile)
                 .forEach(File::delete);
        }
    }


    /** Safer common loader (uses optString; won't throw if a key is missing) */
    private void loadCommonPropertiesSafe(ClaimDocumentDTO dto, JSONObject root, JSONObject meta) {
        dto.setDocumentTitle(meta.optString("document_name", ""));
        dto.setDocumentType(meta.optString("document_type", ""));
        dto.setAuthor(meta.optString("author", ""));
        dto.setOrigDateCreated(meta.optString("create_ts", ""));
        dto.setMimeType(root.optString("mime_type", ""));
        dto.setClaimNumber(meta.optString("claim_num", ""));
        dto.setGwDocumentID(root.optString("external_alias", ""));
    }

    public static Boolean writeDataToCSVFile(Object obj, BufferedWriter writer, boolean writeHeader) throws IOException {
        if (obj == null) return false;

        Class<?> clazz = obj.getClass();
        Field[] fields = clazz.getDeclaredFields();

        try {
            if (writeHeader) {
                List<String> headers = new ArrayList<>();
                for (Field field : fields) {
                    headers.add(field.getName());
                }
                writer.write(String.join(",", headers));
                writer.newLine();
            }

            List<String> values = new ArrayList<>();
            for (Field field : fields) {
                field.setAccessible(true);
                Object value = field.get(obj);
                values.add(value != null ? value.toString() : "");
            }
            writer.write(String.join(",", values));
            writer.newLine();
            return true;
        } catch (IllegalAccessException e) {
            throw new IOException("Failed to access field values", e);
        }
    }
    
    /**
     * New method: Parses CSV and tracks success/failure per row
     * Writes to separate success and failed CSV files
     * @param metadata_file Path to the CSV metadata file
     * @param category_type Category type (e.g., category_claimcenter)
     * @param conn Database connection
     * @param tarFileName Name of the TAR file (for tracking)
     * @param tarExtractionPath Full path where TAR was extracted (for building complete content paths)
     */
    private ProcessingResult parseAndTransformDataWithTracking(Path metadata_file, String category_type, Connection conn, String tarFileName, String tarExtractionPath)
            throws FileNotFoundException, IOException {
        
        ProcessingResult result = new ProcessingResult();
        BufferedReader br = null;
        BufferedWriter successWriter = null;
        BufferedWriter failedWriter = null;
        
        String csvFileName = metadata_file.getFileName().toString();
        String baseFileName = getFileNameWithoutExtension(csvFileName);
        
        // Success and Failed file paths
        String completedDirPath = base_path + Constants.completedFolder + File.separator + category_type;
        String errorDirPath = base_path + Constants.errorFolder + File.separator + category_type;
        String successFilePath = completedDirPath + File.separator + baseFileName + "_success.csv";
        String failedFilePath = errorDirPath + File.separator + baseFileName + "_failed.csv";
        
        // Buffer for failed rows (only create file if there are failures)
        List<String> failedRows = new ArrayList<>();
        String headerLine = null;
        
        try {
            // Create directories
            Files.createDirectories(Paths.get(completedDirPath));
            Files.createDirectories(Paths.get(errorDirPath));
            
            // Open source CSV
            br = new BufferedReader(
                    new InputStreamReader(
                    new FileInputStream(metadata_file.getParent() + File.separator + csvFileName),
                    StandardCharsets.UTF_8));

            // Open success writer upfront (we expect most rows to succeed)
            successWriter = new BufferedWriter(
                    new java.io.OutputStreamWriter(
                    new java.io.FileOutputStream(successFilePath),
                    StandardCharsets.UTF_8));

            // Read header
            headerLine = br.readLine();
            if (headerLine == null) {
                logger.warn("Empty CSV file: {}", metadata_file);
                return result;
            }
            
            // Write header to success file
            successWriter.write(headerLine);
            successWriter.newLine();
            
            java.util.List<String> header = parseCsvRow(headerLine);
            java.util.Map<String, Integer> hidx = indexByName(header);

            // Get column indices
            Integer metaIdx = hidx.get("metadata_json");
            Integer catIdx  = hidx.get("category");
            Integer mimeIdx = hidx.get("mimetype");
            Integer pathIdx = hidx.get("doc_filepath");
            Integer origIdx = hidx.get("orig_rep");
            Integer dUidIdx = hidx.get("doc_uid");
            Integer tsIdx   = hidx.get("file_creation_ts");
            Integer yrIdx   = hidx.get("file_creation_year");
            Integer fileIdIdx = hidx.get("wdr_id");  // Check for wdr_id column (primary)
            if (fileIdIdx == null) fileIdIdx = hidx.get("wcir_id");  // Fallback to wcir_id
            if (fileIdIdx == null) fileIdIdx = hidx.get("file_id");  // Fallback to file_id

            if (metaIdx == null) {
                logger.error("Required column 'metadata_json' not found in CSV: {}", csvFileName);
                return result;
            }

            // Use passed TAR filename instead of searching for it
            logger.debug("Using TAR filename: {}", tarFileName);
            
            // Load batch size from configuration
            String batchSizeStr = config.getProperty("db.batch.insert.size");
            int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
            logger.info("Using batch insert size: {}", batchSize);
            
            // Buffers for batch processing
            List<ClaimDocumentDTO> batchDTOs = new ArrayList<>(batchSize);
            List<String> batchLines = new ArrayList<>(batchSize);

            String line = null;
            String currentLine = null;  // Captured line for error handling
            int rowNum = 1; // header is row 0
            int skippedEmptyLines = 0;

            while ((line = br.readLine()) != null) {
                rowNum++;

                // Silently skip completely empty lines (user-added blank rows at end of file, etc.)
                // These are NOT errors - just ignore them completely
                if (line.trim().length() == 0) {
                    skippedEmptyLines++;
                    continue;  // Don't count, don't log, just skip
                }
                
                // Capture current line for error handling (in case exception corrupts it)
                currentLine = line;
                result.totalRows++;
                
                if (logger.isDebugEnabled()) {
                    logger.debug("Row {} content: {}", rowNum, 
                        line.length() > 100 ? line.substring(0, 100) + "..." : line);
                }
                
                try {
                    // Parse CSV row
                    java.util.List<String> cells = parseCsvRow(currentLine);
                    
                    // Validate we have cells
                    if (cells == null || cells.isEmpty()) {
                        throw new Exception("Row is empty or cannot be parsed");
                    }
                    
                    // Validate metadata_json exists
                    String cell_metadata = getCell(cells, metaIdx);
                    if (cell_metadata == null || cell_metadata.trim().isEmpty()) {
                        throw new Exception("metadata_json column is empty or missing");
                    }
                    
                    // Parse JSON
                    JSONObject jsonObject = parseJsonLenient(cell_metadata);
                if (jsonObject == null) {
                    String unwrapped = unwrapOnce(cell_metadata);
                    jsonObject = parseJsonLenient(unwrapped);
                }
                if (jsonObject == null) {
                        throw new Exception("Invalid JSON in metadata_json - cannot parse");
                }

                    JSONObject metadataJson = jsonObject.optJSONObject("metadata");
                if (metadataJson == null) metadataJson = tryFindMetadataObject(jsonObject);
                if (metadataJson == null) {
                        throw new Exception("No 'metadata' object found in JSON");
                    }
                    
                    // Build DTO
                    ClaimDocumentDTO dto = new ClaimDocumentDTO();
                    loadCommonPropertiesSafe(dto, jsonObject, metadataJson);
                    
                // Try to get file_id from multiple possible locations (CSV column takes priority)
                String fileId = "";
                
                // First, try CSV column (wdr_id, wcir_id, or file_id)
                if (fileIdIdx != null) {
                    fileId = safeTrim(getCell(cells, fileIdIdx));
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from CSV column)", rowNum, fileId);
                    }
                }
                
                // If not in CSV, try JSON locations (note: file_id is numeric in JSON)
                if (fileId.isEmpty()) {
                    long numericId = metadataJson.optLong("file_id", 0L);
                    if (numericId > 0) {
                        fileId = String.valueOf(numericId);
                        logger.debug("Row {}: file_id = {} (from metadata_json.metadata.file_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    long numericId = jsonObject.optLong("file_id", 0L);
                    if (numericId > 0) {
                        fileId = String.valueOf(numericId);
                        logger.debug("Row {}: file_id = {} (from metadata_json.file_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    // Fallback to string-based lookups for wcir_id
                    fileId = metadataJson.optString("wcir_id", "");
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from metadata_json.metadata.wcir_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    fileId = jsonObject.optString("wcir_id", "");
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from metadata_json.wcir_id)", rowNum, fileId);
                    }
                }
                
                dto.setFile_id(fileId);
                
                if (fileId.isEmpty()) {
                    logger.warn("Row {}: file_id not found in any expected location", rowNum);
                }
                    
                    if (catIdx != null) dto.setCategory_type(safeTrim(getCell(cells, catIdx)));
                    
                    // doc_filepath -> Build complete path by merging TAR extraction path + CSV relative path
                    String csvDocFilepath = safeTrim(getCell(cells, pathIdx));
                    String normalizedCsvPath = (csvDocFilepath == null) ? "" : csvDocFilepath.replabc('\\', '/');
                    
                    // Extract just the filename for ContentRetrivalName
                    int lastSlash = normalizedCsvPath.lastIndexOf('/');
                    String fileName = (lastSlash >= 0) ? normalizedCsvPath.substring(lastSlash + 1) : normalizedCsvPath;
                    dto.setContentRetrivalName(fileName);
                    
                    // Build complete content path: TAR extraction folder + CSV relative path
                    // This allows users to copy-paste the path directly into Explorer to access the file
                    String completeContentPath = "";
                    if (normalizedCsvPath != null && !normalizedCsvPath.isEmpty()) {
                        // Remove leading slash if present (to avoid double slashes)
                        String cleanCsvPath = normalizedCsvPath.startsWith("/") ? normalizedCsvPath.substring(1) : normalizedCsvPath;
                        completeContentPath = tarExtractionPath + "/" + cleanCsvPath;
                    } else {
                        // Fallback: if doc_filepath is empty, use just the filename in extraction folder
                        completeContentPath = tarExtractionPath + "/" + fileName;
                    }
                    dto.setContentFilePath(completeContentPath);
                    
                    if (logger.isDebugEnabled()) {
                        logger.debug("Row {}: CSV doc_filepath='{}' -> Complete path='{}'", 
                                    rowNum, csvDocFilepath, completeContentPath);
                    }

                    if (mimeIdx != null) dto.setContentType(safeTrim(getCell(cells, mimeIdx)));

                    dto.setOrigDateCreated(safeTrim(getCell(cells, tsIdx)));
                    dto.setFile_creation_year(safeTrim(getCell(cells, yrIdx)));
                String origRep = safeTrim(getCell(cells, origIdx));
                String docUid  = safeTrim(getCell(cells, dUidIdx));
                    dto.setOriginatingrepo_ext(origRep);
                    dto.setDoc_uid(docUid);

                String extId = buildExternalID(origRep, docUid);
                    dto.setExternalID(extId);
                    
                    //truncate Claimnumber to 9 char
                    dto.setClaimNumber(truncateClaimNumber(dto.getClaimNumber()));
                    dto.setCsv_file_Name(csvFileName);
                    dto.setTar_file_name(tarFileName);
                    
                    //set default values for below
                    dto.setInputMethod(Constants.WAWANESA);
                    dto.setDoNotCreateActivity("true");
                    
                    // Add to batch buffers (instead of immediate insert)
                    batchDTOs.add(dto);
                    batchLines.add(currentLine);
                    
                    // When batch is full, process it
                    if (batchDTOs.size() >= batchSize) {
                        processBatchWithFallback(batchDTOs, batchLines, conn, batchSize,
                                                 successWriter, failedRows, result);
                        
                        // Clear buffers for next batch
                        batchDTOs.clear();
                        batchLines.clear();
                        
                        if (logger.isDebugEnabled()) {
                            logger.debug("Processed batch of {} rows, total success: {}, total failed: {}", 
                                        batchSize, result.successCount, result.failureCount);
                        }
                    }
                    
                } catch (Exception e) {
                    // FAILURE: Buffer failed row (will write to file at end if any failures)
                    String errorMessage = compact(e.getMessage());
                    
                    // Use currentLine (captured at start) to ensure we have the original row
                    // Even if 'line' variable gets modified during processing
                    String safeCurrentLine = (currentLine != null) ? currentLine : "";
                    
                    if (safeCurrentLine.isEmpty()) {
                        logger.error("Row {}: currentLine is empty! Original line was: '{}'", rowNum, line);
                    }
                    
                    // Store: original row + comma + escaped error message
                    String failedRow = safeCurrentLine + ",\"" + errorMessage.replabc("\"", "\"\"") + "\"";
                    failedRows.add(failedRow);
                    
                    result.failureCount++;
                    logger.warn("Row {} failed: {}", rowNum, errorMessage);
                    
                    // Continue to next row (exception is caught, loop continues)
                }
            }
            
            // Process remaining batch (if any rows left)
            if (!batchDTOs.isEmpty()) {
                logger.info("Processing final batch of {} rows", batchDTOs.size());
                processBatchWithFallback(batchDTOs, batchLines, conn, batchSize,
                                         successWriter, failedRows, result);
            }
            
            // Write failed rows to file ONLY if there are failures
            if (!failedRows.isEmpty()) {
                logger.info("Creating failed file with {} rows", failedRows.size());
                failedWriter = new BufferedWriter(
                    new java.io.OutputStreamWriter(
                        new java.io.FileOutputStream(failedFilePath),
                        StandardCharsets.UTF_8));
                
                // Write header with error_message column
                failedWriter.write(headerLine + ",error_message");
                failedWriter.newLine();
                
                // Write all failed rows
                for (String failedRow : failedRows) {
                    failedWriter.write(failedRow);
                    failedWriter.newLine();
                }
                
                logger.info("Failed file created: {}", failedFilePath);
            } else {
                logger.info("All rows processed successfully - no failed file created");
            }
            
            logger.info("Success file created: {}", successFilePath);
            
            // Report skipped empty lines if any (informational only)
            if (skippedEmptyLines > 0) {
                logger.info("Skipped {} empty line(s) in CSV (blank rows - not errors)", skippedEmptyLines);
            }
            
        } catch (Exception e) {
            logger.error("Error processing CSV: {}", csvFileName, e);
        } finally {
            try {
                if (successWriter != null) successWriter.close();
                if (failedWriter != null) failedWriter.close();
                if (br != null) br.close();
            } catch (Exception e2) {
                e2.printStackTrabc();
            }
        }
        
        return result;
    }
    
 private String truncateClaimNumber(String claimNumber) {
		if(claimNumber != null && claimNumber.trim().length()>9) {
			claimNumber = claimNumber.trim();
			claimNumber = claimNumber.substring(claimNumber.length()-9);
		}
		return claimNumber;
	}

	// Inserts only the columns approved for DoubleDouble Phase-1 + DateProcessed
    private void insertDoubleDoubleRow(ClaimDocumentDTO dto, Connection conn) throws java.sql.SQLException {
        final String sql =
            "INSERT INTO dbo.Wawa_Doc_Migration_Transit_Data (" +
            "  originatingrepo_ext, doc_uid, externalID, category_type," +
            "  contentFilePath, contentRetrievalName, contentType," +
            "  OrigDateCreated, file_creation_year," +
            "  file_id, claimNumber, documentTitle, mimeType,doNotCreateActivity,inputMethod," +
            "  csv_file_Name, tar_file_name" +
            ") VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)";

        try (java.sql.PreparedStatement ps = conn.prepareStatement(sql)) {

            conn.setAutoCommit(true); // single-row inserts

            int i = 1;
            ps.setString(i++, nz(dto.getOriginatingrepo_ext()));   // originatingrepo_ext
            ps.setString(i++, nz(dto.getDoc_uid()));               // doc_uid
            ps.setString(i++, nz(dto.getExternalID()));            // externalID
            ps.setString(i++, nz(dto.getCategory_type()));         // category_type

            ps.setString(i++, nz(dto.getContentFilePath()));       // contentFilePath
            ps.setString(i++, nz(dto.getContentRetrivalName()));   // contentRetrievalName (DB has the 'e')
            ps.setString(i++, nz(dto.getContentType()));           // contentType

            ps.setString(i++, nz(dto.getOrigDateCreated()));       // OrigDateCreated
            ps.setString(i++, nz(dto.getFile_creation_year()));    // file_creation_year

            ps.setString(i++, nz(dto.getFile_id()));               // file_id
            ps.setString(i++, nz(dto.getClaimNumber()));           // claimNumber
            ps.setString(i++, nz(dto.getDocumentTitle()));         // documentTitle
            ps.setString(i++, nz(dto.getMimeType()));              // mimeType (from JSON)

            ps.setString(i++, nz(dto.getDoNotCreateActivity()));         // doNotCreateActivity
            ps.setString(i++, nz(dto.getInputMethod()));              // inputMethod

            ps.setString(i++, nz(dto.getCsv_file_Name()));         // csv_file_Name
            ps.setString(i++, nz(dto.getTar_file_name()));         // tar_file_name

            ps.executeUpdate();
        }
    }
    
    /**
     * Insert multiple rows using JDBC batch insert for high performance
     * This method provides ~100x speedup compared to single-row inserts
     * 
     * @param dtoList List of DTOs to insert
     * @param conn Database connection
     * @param batchSize Number of rows per batch (e.g., 1000)
     * @return Number of rows successfully inserted
     * @throws SQLException if batch insert fails
     */
    private int insertDoubleDoubleRowsBatch(List<ClaimDocumentDTO> dtoList, Connection conn, int batchSize) 
            throws SQLException {
        
        if (dtoList == null || dtoList.isEmpty()) {
            return 0;
        }
        
        final String sql =
            "INSERT INTO dbo.Wawa_Doc_Migration_Transit_Data (" +
            "  originatingrepo_ext, doc_uid, externalID, category_type," +
            "  contentFilePath, contentRetrievalName, contentType," +
            "  OrigDateCreated, file_creation_year," +
            "  file_id, claimNumber, documentTitle, mimeType,doNotCreateActivity,inputMethod," +
            "  csv_file_Name, tar_file_name" +
            ") VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)";
        
        conn.setAutoCommit(false); // Enable batch mode
        int totalInserted = 0;
        
        try (PreparedStatement ps = conn.prepareStatement(sql)) {
            int currentBatchSize = 0;
            
            for (ClaimDocumentDTO dto : dtoList) {
                // Set all 17 parameters
                int i = 1;
                ps.setString(i++, nz(dto.getOriginatingrepo_ext()));   // 1
                ps.setString(i++, nz(dto.getDoc_uid()));               // 2
                ps.setString(i++, nz(dto.getExternalID()));            // 3
                ps.setString(i++, nz(dto.getCategory_type()));         // 4
                ps.setString(i++, nz(dto.getContentFilePath()));       // 5
                ps.setString(i++, nz(dto.getContentRetrivalName()));   // 6
                ps.setString(i++, nz(dto.getContentType()));           // 7
                ps.setString(i++, nz(dto.getOrigDateCreated()));       // 8
                ps.setString(i++, nz(dto.getFile_creation_year()));    // 9
                ps.setString(i++, nz(dto.getFile_id()));               // 10
                ps.setString(i++, nz(dto.getClaimNumber()));           // 11
                ps.setString(i++, nz(dto.getDocumentTitle()));         // 12
                ps.setString(i++, nz(dto.getMimeType()));              // 13
                ps.setString(i++, nz(dto.getDoNotCreateActivity()));   // 14
                ps.setString(i++, nz(dto.getInputMethod()));           // 15
                ps.setString(i++, nz(dto.getCsv_file_Name()));         // 16
                ps.setString(i++, nz(dto.getTar_file_name()));         // 17
                
                ps.addBatch(); // Add to batch
                currentBatchSize++;
                
                // Execute batch when it reaches configured size
                if (currentBatchSize >= batchSize) {
                    int[] results = ps.executeBatch();
                    totalInserted += results.length;
                    conn.commit();
                    
                    if (logger.isDebugEnabled()) {
                        logger.debug("Batch committed: {} rows", results.length);
                    }
                    
                    currentBatchSize = 0;
                }
            }
            
            // Execute remaining batch (if any)
            if (currentBatchSize > 0) {
                int[] results = ps.executeBatch();
                totalInserted += results.length;
                conn.commit();
                
                if (logger.isDebugEnabled()) {
                    logger.debug("Final batch committed: {} rows", results.length);
                }
            }
            
            logger.info("Batch insert completed: {} rows inserted", totalInserted);
            return totalInserted;
            
        } catch (SQLException e) {
            logger.error("Batch insert failed, rolling back", e);
            conn.rollback();
            throw e;
        }
    }
    
    /**
     * Process a batch of DTOs with individual fallback on batch failure
     * This ensures we can identify which specific rows failed
     * 
     * @param dtoList List of DTOs to process
     * @param linesList Corresponding CSV lines (for success tracking)
     * @param conn Database connection
     * @param batchSize Batch size for inserts
     * @param successWriter Writer for successful rows
     * @param failedRows List to accumulate failed rows
     * @param result Processing result object to update
     * @throws IOException if writing success rows fails
     */
    private void processBatchWithFallback(List<ClaimDocumentDTO> dtoList, 
                                          List<String> linesList, 
                                          Connection conn, 
                                          int batchSize,
                                          BufferedWriter successWriter,
                                          List<String> failedRows,
                                          ProcessingResult result) throws IOException {
        try {
            // Try batch insert first (fast path)
            int inserted = insertDoubleDoubleRowsBatch(dtoList, conn, batchSize);
            
            // All succeeded - write to success file
            for (String line : linesList) {
                successWriter.write(line);
                successWriter.newLine();
            }
            result.successCount += inserted;
            
        } catch (SQLException batchException) {
            // Batch failed - fall back to individual inserts to identify failed rows
            logger.warn("Batch insert failed, falling back to individual inserts: {}", 
                       batchException.getMessage());
            
            for (int i = 0; i < dtoList.size(); i++) {
                ClaimDocumentDTO dto = dtoList.get(i);
                String line = linesList.get(i);
                
                try {
                    insertDoubleDoubleRow(dto, conn); // Use existing single-row method
                    successWriter.write(line);
                    successWriter.newLine();
                    result.successCount++;
                    
                } catch (SQLException rowException) {
                    String errorMessage = compact(rowException.getMessage());
                    String failedRow = line + ",\"" + errorMessage.replabc("\"", "\"\"") + "\"";
                    failedRows.add(failedRow);
                    result.failureCount++;
                    logger.warn("Row failed: {}", errorMessage);
                }
            }
        }
    }
 
    
 // RFC-ish CSV row splitter (handles quotes/commas)
    private java.util.List<String> parseCsvRow(String line) {
        java.util.List<String> out = new java.util.ArrayList<>();
        if (line == null) return out;
        StringBuilder sb = new StringBuilder();
        boolean inQuotes = false;
        for (int i = 0; i < line.length(); i++) {
            char c = line.charAt(i);
            if (c == '"') {
                if (inQuotes && i + 1 < line.length() && line.charAt(i + 1) == '"') {
                    sb.append('"'); i++; // escaped quote
                } else {
                    inQuotes = !inQuotes;
                }
            } else if (c == ',' && !inQuotes) {
                out.add(sb.toString());
                sb.setLength(0);
            } else {
                sb.append(c);
            }
        }
        out.add(sb.toString());
        return out;
    }
    private java.util.Map<String, Integer> indexByName(java.util.List<String> header) {
        java.util.Map<String, Integer> m = new java.util.HashMap<>();
        for (int i = 0; i < header.size(); i++) m.put(header.get(i), i);
        return m;
    }
    private String getCell(java.util.List<String> row, Integer idx) {
        if (idx == null || idx < 0 || idx >= row.size()) return null;
        return row.get(idx);
    }
    private String safeTrim(String s) { return s == null ? "" : s.trim(); }
    private String nz(String s) { return s == null ? "" : s; }

    private org.json.JSONObject parseJsonLenient(String s) {
        if (s == null) return null;
        String t = s.trim();
        if (t.isEmpty()) return null;
        try {
            return new org.json.JSONObject(t);
        } catch (org.json.JSONException e1) {
            try { return new org.json.JSONObject(t.replabc('\'', '"')); }
            catch (org.json.JSONException e2) { return null; }
        }
    }
    private String unwrapOnce(String s) {
        if (s == null) return null;
        String t = s.trim();
        if (t.length() >= 2 &&
            ((t.startsWith("\"") && t.endsWith("\"")) || (t.startsWith("'") && t.endsWith("'")))) {
            return t.substring(1, t.length()-1);
        }
        return s;
    }
    private org.json.JSONObject tryFindMetadataObject(org.json.JSONObject root) {
        for (String k : root.keySet()) {
            try {
                Object v = root.get(k);
                if (v instanceof org.json.JSONObject) {
                    org.json.JSONObject jo = (org.json.JSONObject) v;
                    if ("metadata".equalsIgnoreCase(k)) return jo;
                    org.json.JSONObject deeper = tryFindMetadataObject(jo);
                    if (deeper != null) return deeper;
                }
            } catch (Exception ignore) {}
        }
        return null;
    }
    private String compact(String msg) {
        if (msg == null) return "";
        return msg.replabcAll("\\s+", " ").trim();
    }

    /** externalID rule: opentext => doc_uid as-is; filenet => "filenet:" + doc_uid */
    private String buildExternalID(String origRep, String docUid) {
        String or = safeTrim(origRep);
        String du = safeTrim(docUid);
        if (or.equalsIgnoreCase(Constants.OPENTEXT)) {
            du = du.replabcAll("OT", Constants.OPENTEXT_NUMARIC_VALUE);
            return du;
        } else if (or.equalsIgnoreCase(Constants.FILENET)) {
            return Constants.FILENET_NUMARIC_VALUE + ":" + du;
        } else {
            return (or.isEmpty() ? "unknown" : or) + ":" + du;
        }
    }



    // ========================================================================
    // FILE LIFECYCLE MANAGEMENT METHODS
    // ========================================================================
    
    /**
     * Inner class to track processing results per CSV file
     */
    private static class ProcessingResult {
        int totalRows = 0;
        int successCount = 0;
        int failureCount = 0;
    }
    
    /**
     * Moves both TAR and CSV files to Error folder when processing fails
     */
    private void moveFilesToError(String tarPath, String csvPath, String category, String reason) {
        try {
            // Create error info file
            String errorDirPath = base_path + Constants.errorFolder + File.separator + category;
            Files.createDirectories(Paths.get(errorDirPath));
            
            File tarFile = new File(tarPath);
            File csvFile = new File(csvPath);
            
            // Move TAR file
            if (tarFile.exists()) {
                Path targetTar = Paths.get(errorDirPath, tarFile.getName());
                Files.move(tarFile.toPath(), targetTar, StandardCopyOption.REPLACE_EXISTING);
                logger.info("Moved TAR to Error folder: {}", targetTar);
            }
            
            // Move CSV file
            if (csvFile.exists()) {
                Path targetCsv = Paths.get(errorDirPath, csvFile.getName());
                Files.move(csvFile.toPath(), targetCsv, StandardCopyOption.REPLACE_EXISTING);
                logger.info("Moved CSV to Error folder: {}", targetCsv);
            }
            
            // Create error log file
            String errorLogPath = errorDirPath + File.separator + 
                getFileNameWithoutExtension(csvFile.getName()) + "_error.log";
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogPath))) {
                writer.write("Error Timestamp: " + new java.util.Date());
                writer.newLine();
                writer.write("Reason: " + reason);
                writer.newLine();
                writer.write("TAR File: " + tarFile.getName());
                writer.newLine();
                writer.write("CSV File: " + csvFile.getName());
                writer.newLine();
            }
            logger.info("Created error log: {}", errorLogPath);
            
        } catch (Exception e) {
            logger.error("Failed to move files to Error folder: {}", e.getMessage(), e);
        }
    }
    
    /**
     * Archives or deletes source files after successful processing
     * Controlled by property: app.archive.after.processing (true/false)
     */
    private void handleSourceFilesAfterProcessing(String tarPath, String csvPath, String category) {
        try {
            String archiveAfterProcessing = config.getProperty("app.archive.after.processing");
            boolean shouldArchive = archiveAfterProcessing != null && 
                                   archiveAfterProcessing.trim().equalsIgnoreCase("true");
            
            File tarFile = new File(tarPath);
            File csvFile = new File(csvPath);
            
            if (shouldArchive) {
                // Archive mode: Move to Archive folder
                String archiveDirPath = base_path + Constants.archiveFolder + File.separator + category;
                Files.createDirectories(Paths.get(archiveDirPath));
                
                if (tarFile.exists()) {
                    Path targetTar = Paths.get(archiveDirPath, tarFile.getName());
                    Files.move(tarFile.toPath(), targetTar, StandardCopyOption.REPLACE_EXISTING);
                    logger.info("Archived TAR: {}", targetTar);
                }
                
                if (csvFile.exists()) {
                    Path targetCsv = Paths.get(archiveDirPath, csvFile.getName());
                    Files.move(csvFile.toPath(), targetCsv, StandardCopyOption.REPLACE_EXISTING);
                    logger.info("Archived CSV: {}", targetCsv);
                }
            } else {
                // Delete mode: Remove source files
                if (tarFile.exists() && tarFile.delete()) {
                    logger.info("Deleted TAR: {}", tarPath);
                }
                if (csvFile.exists() && csvFile.delete()) {
                    logger.info("Deleted CSV: {}", csvPath);
                }
            }
        } catch (Exception e) {
            logger.error("Failed to handle source files: {}", e.getMessage(), e);
        }
    }

}



=================================================

<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">

    <Properties>
        <!-- Base log directory -->
        <Property name="LOG_DIR">D:\Rameshwar\DataTransformUtility\logs</Property>
        <!-- Log pattern for detailed debugging -->
        <Property name="LOG_PATTERN">%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n</Property>
        <!-- Log pattern for console (shorter) -->
        <Property name="CONSOLE_PATTERN">%d{HH:mm:ss.SSS} %-5level %logger{36} - %msg%n</Property>
    </Properties>

    <Appenders>
        <!-- ============================================ -->
        <!-- Main Application Log (Rolling File)         -->
        <!-- ============================================ -->
        <RollingFile name="MainLogger" 
                     fileName="${LOG_DIR}/DataTransformUtility.log"
                     filePattern="${LOG_DIR}/DataTransformUtility-%d{yyyy-MM-dd}-%i.log.gz"
                     immediateFlush="true">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <!-- Roll over daily -->
                <TimeBasedTriggeringPolicy />
                <!-- Roll over when file reaches 100MB -->
                <SizeBasedTriggeringPolicy size="100 MB"/>
            </Policies>
            <!-- Keep max 10 rolled files -->
            <DefaultRolloverStrategy max="10"/>
        </RollingFile>

        <!-- ============================================ -->
        <!-- Batch Processing Log (NEW)                  -->
        <!-- Tracks batch insert performance             -->
        <!-- ============================================ -->
        <RollingFile name="BatchLogger" 
                     fileName="${LOG_DIR}/BatchProcessing.log"
                     filePattern="${LOG_DIR}/BatchProcessing-%d{yyyy-MM-dd}.log.gz"
                     immediateFlush="false">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <TimeBasedTriggeringPolicy />
                <SizeBasedTriggeringPolicy size="50 MB"/>
            </Policies>
            <DefaultRolloverStrategy max="5"/>
        </RollingFile>

        <!-- ============================================ -->
        <!-- Progress & Performance Log (NEW)            -->
        <!-- Tracks progress reports and performance     -->
        <!-- ============================================ -->
        <RollingFile name="ProgressLogger" 
                     fileName="${LOG_DIR}/Progress.log"
                     filePattern="${LOG_DIR}/Progress-%d{yyyy-MM-dd}.log.gz"
                     immediateFlush="true">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <TimeBasedTriggeringPolicy />
                <SizeBasedTriggeringPolicy size="20 MB"/>
            </Policies>
            <DefaultRolloverStrategy max="5"/>
        </RollingFile>

        <!-- ============================================ -->
        <!-- Error Log (Errors and Warnings Only)        -->
        <!-- ============================================ -->
        <RollingFile name="ErrorLogger" 
                     fileName="${LOG_DIR}/Errors.log"
                     filePattern="${LOG_DIR}/Errors-%d{yyyy-MM-dd}.log.gz"
                     immediateFlush="true">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <TimeBasedTriggeringPolicy />
                <SizeBasedTriggeringPolicy size="50 MB"/>
            </Policies>
            <DefaultRolloverStrategy max="10"/>
            <!-- Only log WARN and above -->
            <ThresholdFilter level="WARN" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>

        <!-- ============================================ -->
        <!-- Console Appender (Real-time monitoring)     -->
        <!-- ============================================ -->
        <Console name="ConsoleLogger" target="SYSTEM_OUT">
            <PatternLayout pattern="${CONSOLE_PATTERN}"/>
            <!-- Show INFO and above on console -->
            <ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/>
        </Console>

        <!-- ============================================ -->
        <!-- Async Appender (Better Performance)         -->
        <!-- Wraps file appenders for async writing      -->
        <!-- ============================================ -->
        <Async name="AsyncMain">
            <AppenderRef ref="MainLogger"/>
        </Async>

        <Async name="AsyncBatch">
            <AppenderRef ref="BatchLogger"/>
        </Async>

        <Async name="AsyncProgress">
            <AppenderRef ref="ProgressLogger"/>
        </Async>

    </Appenders>

    <Loggers>
        <!-- ============================================ -->
        <!-- Application Package Loggers                 -->
        <!-- ============================================ -->
        
        <!-- Main Application Logger -->
        <Logger name="com.abcdefg.abc" level="info" additivity="false">
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="ErrorLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Utils Package - Includes batch processing (NEW) -->
        <Logger name="com.abcdefg.abc.utils" level="info" additivity="false">
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="AsyncBatch"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Global Reporting Package (NEW) -->
        <Logger name="com.abcdefg.abc.reporting" level="info" additivity="false">
            <AppenderRef ref="AsyncProgress"/>
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Memory Monitoring Package (NEW) -->
        <Logger name="com.abcdefg.abc.monitoring" level="info" additivity="false">
            <AppenderRef ref="AsyncProgress"/>
            <AppenderRef ref="AsyncMain"/>
        </Logger>

        <!-- Connection Manager - Database operations -->
        <Logger name="com.abcdefg.abc.connection" level="info" additivity="false">
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- ============================================ -->
        <!-- Third-Party Library Loggers                 -->
        <!-- ============================================ -->
        
        <!-- HikariCP Connection Pool -->
        <Logger name="com.zaxxer.hikari" level="warn" additivity="false">
            <AppenderRef ref="AsyncMain"/>
        </Logger>

        <!-- Apache Commons Compress (TAR extraction) -->
        <Logger name="org.apache.commons.compress" level="warn" additivity="false">
            <AppenderRef ref="AsyncMain"/>
        </Logger>

        <!-- ============================================ -->
        <!-- Root Logger (Catch-all)                     -->
        <!-- ============================================ -->
        <Root level="info">
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="ErrorLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Root>

        <!-- ============================================ -->
        <!-- DEBUG Mode (Uncomment for troubleshooting)  -->
        <!-- ============================================ -->
        <!-- 
        <Logger name="com.abcdefg.abc.utils" level="debug" additivity="false">
            <AppenderRef ref="AsyncMain"/>
            <AppenderRef ref="AsyncBatch"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>
        -->

    </Loggers>

    <!-- ================================================ -->
    <!-- NOTES                                            -->
    <!-- ================================================ -->
    <!-- 
    1. Log Rotation:
       - Main log rotates daily or at 100MB
       - Batch log rotates daily or at 50MB
       - Progress log rotates daily or at 20MB
       - Keeps last 5-10 files based on type
    
    2. Log Files Generated:
       - DataTransformUtility.log      : Main application log
       - BatchProcessing.log            : Batch insert operations (NEW)
       - Progress.log                   : Progress reports & performance (NEW)
       - Errors.log                     : Errors and warnings only
       - *-yyyy-MM-dd.log.gz            : Rolled/archived logs
    
    3. Performance:
       - Async appenders improve performance
       - immediateFlush=false for batch logs (better performance)
       - immediateFlush=true for main/error logs (data safety)
    
    4. Log Levels:
       - ERROR: Critical failures
       - WARN:  Non-critical issues
       - INFO:  Important milestones (default)
       - DEBUG: Detailed debugging (enable via commented section)
    
    5. Console Output:
       - Shows INFO and above
       - Real-time monitoring during processing
       - Includes progress reports every 5 minutes
    
    6. Troubleshooting:
       - For detailed debugging, set level="debug" on specific loggers
       - Check Errors.log for all warnings and errors
       - Check BatchProcessing.log for batch insert details
       - Check Progress.log for performance metrics
    
    7. Disk Spabc:
       - Main log: ~100MB × 10 = 1GB max
       - Batch log: ~50MB × 5 = 250MB max
       - Progress log: ~20MB × 5 = 100MB max
       - Total: ~1.35GB max (compressed)
    
    -->

</Configuration>


================================================================================================================================

# ===========================================================
# DataTransformUtility - Production Configuration
# Updated: October 11, 2025 (Phase 1 Implementation)
# ===========================================================

# ------------------------------------------------------------
# APPLICATION PATHS
# ------------------------------------------------------------
app.base_path=D:/Rameshwar/abcdefg/

# Comma-separated list of category types to process
# Must match folder names under Source/ directory
app.category_types=category_claimcenter,category_dcpfnol,category_dcpfnolclaimcenter

# ------------------------------------------------------------
# CSV PROCESSING OPTIONS
# ------------------------------------------------------------

# Skip the header row when processing CSV files
# true = CSV has header row (skip first line)
# false = CSV has no header (process all lines as data)
app.skipHeaderRow=true

# ------------------------------------------------------------
# FILE LIFECYCLE MANAGEMENT
# ------------------------------------------------------------

# Archive or delete source files after successful processing
# true  = Move processed TAR and CSV files to Archive/{category}/ folder
# false = Delete processed TAR and CSV files from Source/{category}/ folder
app.archive.after.processing=true

# Clean up extracted files after processing to save disk spabc
# true  = Delete extracted document files from Transformed/{category}/ after archiving
# false = Keep extracted files in Transformed/{category}/ folder
# Recommended: true for production (saves disk spabc)
app.cleanup.extracted.files.after.processing=false

# ------------------------------------------------------------
# PERFORMANCE & THREADING (Phase 1 - NEW)
# ------------------------------------------------------------

# Thread pool size for parallel file processing
# Higher values = more concurrent file processing = faster completion
# Recommendation: 2x CPU cores for I/O bound tasks
# Examples:
#   - 8 CPU cores  → thread.pool.size=10-15
#   - 16 CPU cores → thread.pool.size=20-30
#   - 32 CPU cores → thread.pool.size=30-50
# For 25M documents: Recommended 20
# Default: 5
app.thread.pool.size=20

# Progress logging interval (minutes)
# How often to log overall processing progress (files, rows, ETA)
# Set to 0 to disable periodic progress logging
# For 25M documents: Recommended 3-5 minutes
# Default: 5
app.progress.log.interval.minutes=5

# ------------------------------------------------------------
# DATABASE CONNECTION (SQL Server)
# ------------------------------------------------------------

# SQL Server hostname or IP address
# Use double backslash (\\) for named instances
db.serverName=RAMESHWAR\\SQLEXPRESS

# SQL Server port
db.port=1433

# Database name
db.dbName=RAMDB

# Connection pool size (number of concurrent database connections)
# IMPORTANT: Should be >= app.thread.pool.size for optimal performance
# Recommendation: Set to app.thread.pool.size + 10 for buffer
# Example: If thread.pool.size=20, set pool.size=30
# For 25M documents: Recommended 30
# Default: 10
db.pool.size=30

# Database batch insert size (Phase 1 - NEW - CRITICAL FOR PERFORMANCE)
# Number of rows to insert in a single batch operation
# Higher values = faster inserts but more memory per thread
# Performance Impact: 100x faster than single-row inserts!
# Recommendations:
#   - Testing/Debug: 100-500   (easier to debug failures)
#   - Production:    1000-2000 (optimal performance)
#   - High-end:      5000      (only if you have 64GB+ RAM)
# For 25M documents: Recommended 1000
# Default: 1000
db.batch.insert.size=1000

# ------------------------------------------------------------
# PERFORMANCE EXPECTATIONS (With Phase 1 Implementation)
# ------------------------------------------------------------
# 
# For 25M documents with recommended settings above:
#   - Processing Time: 12-15 minutes (vs 14 hours without batch inserts)
#   - Throughput:      ~30,000-40,000 rows/sec average
#   - Memory Usage:    8-12GB peak (set JVM -Xmx16G)
#   - Thread Pool:     20 concurrent threads
#   - Success Rate:    99%+ expected
# 
# Progress reports will be logged every 5 minutes showing:
#   - Files processed / remaining
#   - Rows success / failed / skipped
#   - Success rate percentage
#   - Estimated time to completion (ETA)
#   - Thread pool utilization
#   - Memory usage
# 
# Final summary report will be generated at:
#   D:/Rameshwar/abcdefg/DataTransformUtility_Summary_Report_[timestamp].csv
#
# ------------------------------------------------------------

# ------------------------------------------------------------
# JVM SETTINGS RECOMMENDATION
# ------------------------------------------------------------
# For 25M documents, use these JVM settings:
# 
# java -Xms4G \
#      -Xmx16G \
#      -XX:+UseG1GC \
#      -XX:MaxGCPauseMillis=200 \
#      -XX:+HeapDumpOnOutOfMemoryError \
#      -XX:HeapDumpPath=D:\Rameshwar\DataTransformUtility\dumps \
#      -Djava.library.path="C:\Program Files\Microsoft JDBC Driver 12.10 for SQL Server\auth\x64" \
#      -Dconfig.file=D:\Rameshwar\DataTransformUtility\config\DataTransformUtility.properties \
#      -Dlog4j.configurationFile=D:\Rameshwar\DataTransformUtility\config\log4j2.xml \
#      -jar target\DataTransformUtility-1.0-SNAPSHOT.jar
#
# ------------------------------------------------------------

# ===========================================================
# NOTES
# ===========================================================
# 
# 1. Database Batch Inserts (NEW):
#    - Provides 100x performance improvement
#    - If batch fails, automatically falls back to individual inserts
#    - Failed rows are identified and written to *_failed.csv
# 
# 2. Thread Pool Sizing:
#    - More threads = faster processing (up to a point)
#    - Too many threads can cause contention
#    - Monitor "Thread Pool" stats in progress reports
# 
# 3. Global Reporting (NEW):
#    - Summary report generated automatically at end
#    - Contains: total files, rows, success/failure rates, timing
#    - Location: {app.base_path}/DataTransformUtility_Summary_Report_[timestamp].csv
# 
# 4. Progress Monitoring (NEW):
#    - Logs progress every N minutes (configurable)
#    - Shows ETA, success rate, thread pool stats, memory usage
#    - Helps track long-running jobs
# 
# 5. Memory Management:
#    - Monitor logs for "HIGH MEMORY USAGE" warnings
#    - If memory exceeds 85%, consider:
#      * Increasing JVM heap size (-Xmx)
#      * Reducing batch size (db.batch.insert.size)
#      * Reducing thread pool size (app.thread.pool.size)
# 
# ===========================================================


================================================================================================================================

@echo off
REM ============================================================================
REM DataTransformUtility - Batch Execution Script
REM ============================================================================
REM Simplified version with DEV/UAT and PROD profiles
REM ============================================================================

setlocal

REM ============================================================================
REM ENVIRONMENT SELECTION - CHANGE THIS FOR YOUR ENVIRONMENT
REM ============================================================================
REM Options: DEV, UAT, or PROD
REM DEV/UAT: 8 cores, 32GB RAM
REM PROD:    16 cores, 64GB RAM, 25M documents

set ENVIRONMENT=PROD

REM ============================================================================
REM BASIC CONFIGURATION
REM ============================================================================

set WORK_DIR=D:\Rameshwar\DataTransformUtility
cd /d "%WORK_DIR%"

set JAR_FILE=%WORK_DIR%\target\DataTransformUtility-0.0.1-SNAPSHOT.jar
set CONFIG_DIR=%WORK_DIR%\config
set PROPERTIES_FILE=%CONFIG_DIR%\DataTransformUtility.properties
set LOG4J_CONFIG=%CONFIG_DIR%\Log4j2.xml

REM ============================================================================
REM MEMORY SETTINGS BY ENVIRONMENT
REM ============================================================================

if /I "%ENVIRONMENT%"=="PROD" (
    echo Selected Environment: PRODUCTION
    echo   Server: 16 cores, 64GB RAM
    echo   Expected Volume: 25M documents
    echo.
    
    REM PROD: 16 cores, 64GB RAM, 25M documents
    set MIN_HEAP=12g
    set MAX_HEAP=24g
    set YOUNG_GEN=8g
    set METASPACE=512m
    set GC_THREADS=8
    set PARALLEL_THREADS=16
    
) else (
    echo Selected Environment: DEV/UAT
    echo   Server: 8 cores, 32GB RAM
    echo   Expected Volume: Test data
    echo.
    
    REM DEV/UAT: 8 cores, 32GB RAM
    set MIN_HEAP=6g
    set MAX_HEAP=12g
    set YOUNG_GEN=4g
    set METASPACE=256m
    set GC_THREADS=4
    set PARALLEL_THREADS=8
)

REM ============================================================================
REM BUILD JVM ARGUMENTS
REM ============================================================================

REM Memory Settings
set JVM_ARGS=-Xms%MIN_HEAP% -Xmx%MAX_HEAP%
set JVM_ARGS=%JVM_ARGS% -XX:NewSize=%YOUNG_GEN% -XX:MaxNewSize=%YOUNG_GEN%
set JVM_ARGS=%JVM_ARGS% -XX:MetaspabcSize=%METASPACE% -XX:MaxMetaspabcSize=%METASPACE%

REM Garbage Collection (G1GC - Best for large datasets)
set JVM_ARGS=%JVM_ARGS% -XX:+UseG1GC
set JVM_ARGS=%JVM_ARGS% -XX:MaxGCPauseMillis=200
set JVM_ARGS=%JVM_ARGS% -XX:ConcGCThreads=%GC_THREADS%
set JVM_ARGS=%JVM_ARGS% -XX:ParallelGCThreads=%PARALLEL_THREADS%

REM Performance Optimizations
set JVM_ARGS=%JVM_ARGS% -XX:+UseStringDeduplication
set JVM_ARGS=%JVM_ARGS% -XX:+UseCompressedOops
set JVM_ARGS=%JVM_ARGS% -Xss1m

REM Error Handling
set JVM_ARGS=%JVM_ARGS% -XX:+HeapDumpOnOutOfMemoryError
set JVM_ARGS=%JVM_ARGS% -XX:HeapDumpPath=%WORK_DIR%\heapdumps
set JVM_ARGS=%JVM_ARGS% -XX:+ExitOnOutOfMemoryError

REM Config Files
set JVM_ARGS=%JVM_ARGS% -Dconfig.properties.path="%PROPERTIES_FILE%"
set JVM_ARGS=%JVM_ARGS% -Dlog4j.configurationFile="%LOG4J_CONFIG%"

REM ============================================================================
REM PRE-EXECUTION CHECKS
REM ============================================================================

echo ============================================================================
echo DataTransformUtility - Starting Execution
echo ============================================================================
echo Environment: %ENVIRONMENT%
echo Start Time: %date% %time%
echo.

if not exist "%JAR_FILE%" (
    echo [ERROR] JAR file not found: %JAR_FILE%
    echo Please build the project first: mvn clean package
    pause
    exit /b 1
)

if not exist "%PROPERTIES_FILE%" (
    echo [ERROR] Properties file not found: %PROPERTIES_FILE%
    pause
    exit /b 1
)

if not exist "%LOG4J_CONFIG%" (
    echo [ERROR] Log4j config not found: %LOG4J_CONFIG%
    pause
    exit /b 1
)

echo Memory Configuration:
echo   Min Heap: %MIN_HEAP%
echo   Max Heap: %MAX_HEAP%
echo   Young Gen: %YOUNG_GEN%
echo   GC Threads: %GC_THREADS%
echo   Parallel Threads: %PARALLEL_THREADS%
echo.

echo ============================================================================
echo Executing...
echo ============================================================================
echo.

REM ============================================================================
REM EXECUTE
REM ============================================================================

java %JVM_ARGS% -jar "%JAR_FILE%"

set EXIT_CODE=%ERRORLEVEL%

REM ============================================================================
REM SUMMARY
REM ============================================================================

echo.
echo ============================================================================
echo Execution Summary
echo ============================================================================
echo End Time: %date% %time%
echo Exit Code: %EXIT_CODE%
echo.

if %EXIT_CODE% EQU 0 (
    echo [SUCCESS] DataTransformUtility completed successfully!
    echo Check logs: %WORK_DIR%\logs\
) else (
    echo [ERROR] Failed with exit code: %EXIT_CODE%
    echo Check error log: %WORK_DIR%\logs\Errors.log
)

echo ============================================================================
pause

endlocal
exit /b %EXIT_CODE%
*************************************************************************************************************

package com.abcdefg.abc.merge;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.merge.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.merge.connection.ConnectionManager;
import com.abcdefg.abc.merge.constants.Constants;
import com.abcdefg.abc.merge.utils.GlobalProcessingReport;
import com.abcdefg.abc.merge.utils.Utils;


public class DoubleDoubleDataMergeService {

    private static final Logger Logger = LogManager.getLogger(DoubleDoubleDataMergeService.class);

    public static void main(String[] args) {

        Logger.info("Application started");
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        final ConnectionManager[] connManagerRef = new ConnectionManager[1];
        
        // Initialize global processing report
        GlobalProcessingReport report = new GlobalProcessingReport();
        AtomicInteger totalFilesQueued = new AtomicInteger(0);

        try {
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService; // Assign to non-final variable for shutdown hook
            
            Logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        report.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            Logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            report.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        Logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                Logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                Logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination (e.g., Ctrl+C)
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                Logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        Logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                Logger.info("Shutdown hook completed");
            }));

            Logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            Logger.info("Config File: {}", System.getProperty("config.file"));
            Logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String base_path = config.getProperty("app.base_path");
            Logger.info("Base Path: {}", base_path);

            // Validate all required directories exist
            if (isValidDirectory(base_path, Constants.INPUT_FOLDER)
                    && isValidDirectory(base_path, Constants.INPROGRESS_FOLDER)
                    && isValidDirectory(base_path, Constants.COMPLETED_FOLDER)
                    && isValidDirectory(base_path, Constants.FAILED_FOLDER)
                    && isValidDirectory(base_path, Constants.ARCHIVE_FOLDER)) {

                Logger.info("All required directories validated successfully under base path: {}", base_path);

                // Initialize connection manager
                connManagerRef[0] = new ConnectionManager(config);
                Logger.info("Database connection pool initialized");
                
                // Initialize utility class
                Utils utils = new Utils(config, connManagerRef[0]);
                
                // Process all CSV files in Input folder
                Path inputDir = Paths.get(base_path, Constants.INPUT_FOLDER);
                
                // First pass: Count total files to queue
                try (DirectoryStream<Path> stream = Files.newDirectoryStream(inputDir, "conv_claims_extract_*.csv")) {
                    for (Path csvFile : stream) {
                        if (Files.isRegularFile(csvFile)) {
                            totalFilesQueued.incrementAndGet();
                            report.incrementFilesQueued();
                        }
                    }
                    Logger.info("Found {} CSV file(s) to process", totalFilesQueued.get());
                } catch (IOException e) {
                    Logger.error("Error counting CSV files in Input directory", e);
                }
                
                // Second pass: Process files
                processCSVFiles(inputDir, utils, executor, report);

            } else {
                Logger.error("Required directories are missing under the base path: {}", base_path);
                Logger.error("Application terminated due to missing directories");
            }

        } catch (Exception e) {
            Logger.error("Error during processing", e);
            e.printStackTrabc();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    Logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    Logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    Logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService grabcfully
            if (executor != null) {
                try {
                    Logger.info("Shutting down executor service...");
                    executor.shutdown(); // Disable new tasks from being submitted
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        Logger.warn("Executor did not terminate in time, forcing shutdown...");
                        executor.shutdownNow(); // Cancel currently executing tasks
                        
                        // Wait a while for tasks to respond to being cancelled
                        if (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                            Logger.error("Executor did not terminate after forced shutdown");
                        }
                    }
                    Logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    Logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManagerRef[0] != null) {
                try {
                    Logger.info("Closing database connection pool...");
                    connManagerRef[0].close();
                    Logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    Logger.error("Error closing connection manager", e);
                    e.printStackTrabc();
                }
            }
            
            // Write final summary report
            try {
                Logger.info("===== FINAL SUMMARY =====");
                Logger.info(report.getSummary());
                
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                report.writeReportToFile(basePath);
                Logger.info("Summary report written to: {}", basePath);
            } catch (Exception e) {
                Logger.error("Failed to write summary report", e);
            }
            
            Logger.info("Application completed");
        }
    }

    /**
     * Process all CSV files in the Input directory using multi-threading
     */
    private static void processCSVFiles(Path inputDir, Utils utils, ExecutorService executor, GlobalProcessingReport report) {
        Logger.info("Scanning Input directory for CSV files: {}", inputDir);
        
        int fileCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(inputDir, "conv_claims_extract_*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    fileCount++;
                    String fileName = csvFile.getFileName().toString();
                    Logger.info("Submitting CSV file for processing: {}", fileName);
                    
                    // Submit each CSV file for parallel processing
                    executor.submit(() -> {
                        try {
                            Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                            
                            // Record success in global report
                            report.recordFileSuccess(fileName, 
                                new GlobalProcessingReport.ProcessingResult(
                                    result.totalRows, 
                                    result.successCount, 
                                    result.failureCount, 
                                    result.skippedEmptyLines
                                )
                            );
                            
                        } catch (Exception e) {
                            Logger.error("Error processing CSV file: {}", fileName, e);
                            report.recordFileFailure(fileName, e.getMessage());
                        }
                    });
                }
            }
            
            if (fileCount == 0) {
                Logger.warn("No CSV files found in Input directory: {}", inputDir);
            } else {
                Logger.info("Total {} CSV file(s) submitted for processing", fileCount);
            }
            
        } catch (IOException e) {
            Logger.error("Error reading Input directory: {}", inputDir, e);
        }
    }
    
    /**
     * Validate that a directory exists under the base path
     */
    private static boolean isValidDirectory(String base_path, String folderName) {
        boolean isExist = false;
        try {
            File dir = new File(base_path + folderName);
            if (dir.exists() && dir.isDirectory()) {
                isExist = true;
                Logger.debug("Directory validated: {}", dir.getAbsolutePath());
            } else {
                Logger.error("Expected directory does not exist: {}", 
                           base_path + File.separator + folderName);
            }
        } catch (Exception e) {
            Logger.error("Error validating directory: {}", folderName, e);
        }
        return isExist;
    }
}


===============================================

package com.abcdefg.abc.merge.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // File-level counters
    private final AtomicInteger totalFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalFilesFailed = new AtomicInteger(0);
    
    // Row-level counters
    private final AtomicLong totalRowsProcessed = new AtomicLong(0);
    private final AtomicLong totalRowsUpdated = new AtomicLong(0);
    private final AtomicLong totalRowsFailed = new AtomicLong(0);
    private final AtomicLong totalRowsSkipped = new AtomicLong(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a file has been queued for processing
     */
    public void incrementFilesQueued() {
        totalFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful file processing with statistics
     */
    public void recordFileSuccess(String fileName, ProcessingResult result) {
        totalFilesProcessed.incrementAndGet();
        totalFilesSuccess.incrementAndGet();
        
        totalRowsProcessed.addAndGet(result.totalRows);
        totalRowsUpdated.addAndGet(result.successCount);
        totalRowsFailed.addAndGet(result.failureCount);
        totalRowsSkipped.addAndGet(result.skippedEmptyLines);
        
        logger.info("File completed: {} | Rows: Total={}, Updated={}, Failed={}, Skipped={}", 
                   fileName, result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
    }
    
    /**
     * Record failed file processing
     */
    public void recordFileFailure(String fileName, String errorMessage) {
        totalFilesProcessed.incrementAndGet();
        totalFilesFailed.incrementAndGet();
        
        logger.error("File failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalFilesQueued.get(), totalFilesProcessed.get(), totalFilesQueued.get(),
                   totalFilesSuccess.get(), totalFilesFailed.get());
        logger.info("Rows: Processed={}, Updated={}, Failed={}, Skipped={}", 
                   totalRowsProcessed.get(), totalRowsUpdated.get(), totalRowsFailed.get(), totalRowsSkipped.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- File Statistics ---\n");
        sb.append(String.format("Total Files Queued: %d\n", totalFilesQueued.get()));
        sb.append(String.format("Total Files Processed: %d\n", totalFilesProcessed.get()));
        sb.append(String.format("Files Success: %d\n", totalFilesSuccess.get()));
        sb.append(String.format("Files Failed: %d\n", totalFilesFailed.get()));
        sb.append("\n--- Row Statistics ---\n");
        sb.append(String.format("Total Rows Processed: %d\n", totalRowsProcessed.get()));
        sb.append(String.format("Total Rows Updated: %d\n", totalRowsUpdated.get()));
        sb.append(String.format("Total Rows Failed: %d\n", totalRowsFailed.get()));
        sb.append(String.format("Total Rows Skipped: %d\n", totalRowsSkipped.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long rowsPerSecond = totalRowsProcessed.get() / totalDuration.getSeconds();
            long rowsPerMinute = totalRowsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d rows/sec, %d rows/min\n", rowsPerSecond, rowsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataMerge_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write file statistics
            writer.newLine();
            writer.write("--- File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Files Queued,%d", totalFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total Files Processed,%d", totalFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("Files Success,%d", totalFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("Files Failed,%d", totalFilesFailed.get()));
            writer.newLine();
            
            // Write row statistics
            writer.newLine();
            writer.write("--- Row Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Rows Processed,%d", totalRowsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Updated,%d", totalRowsUpdated.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Failed,%d", totalRowsFailed.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Skipped,%d", totalRowsSkipped.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long rowsPerSecond = totalRowsProcessed.get() / totalDuration.getSeconds();
                long rowsPerMinute = totalRowsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Rows Per Second,%d", rowsPerSecond));
                writer.newLine();
                writer.write(String.format("Rows Per Minute,%d", rowsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
    /**
     * Inner class for tracking individual file processing results
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
        
        public ProcessingResult() {}
        
        public ProcessingResult(int totalRows, int successCount, int failureCount, int skippedEmptyLines) {
            this.totalRows = totalRows;
            this.successCount = successCount;
            this.failureCount = failureCount;
            this.skippedEmptyLines = skippedEmptyLines;
        }
    }
}


============================================

package com.abcdefg.abc.merge.utils;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Utility class for monitoring JVM memory usage
 */
public class MemoryMonitor {
    
    private static final Logger logger = LogManager.getLogger(MemoryMonitor.class);
    private static final long MB = 1024 * 1024;
    
    /**
     * Get current memory usage information
     */
    public static MemoryInfo getMemoryInfo() {
        Runtime runtime = Runtime.getRuntime();
        
        long maxMemory = runtime.maxMemory();
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long usedMemory = totalMemory - freeMemory;
        
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        return new MemoryInfo(maxMemory, totalMemory, freeMemory, usedMemory, usagePercent);
    }
    
    /**
     * Check if memory usage is above threshold
     */
    public static boolean isMemoryHigh(int thresholdPercent) {
        MemoryInfo info = getMemoryInfo();
        return info.usagePercent >= thresholdPercent;
    }
    
    /**
     * Log current memory statistics
     */
    public static void logMemoryStats() {
        MemoryInfo info = getMemoryInfo();
        
        logger.info("Memory: Used={} MB, Total={} MB, Max={} MB, Usage={:.1f}%", 
                   info.usedMemory / MB, 
                   info.totalMemory / MB, 
                   info.maxMemory / MB,
                   info.usagePercent);
        
        // Warn if memory usage is high
        if (info.usagePercent >= 80) {
            logger.warn("HIGH MEMORY USAGE WARNING: {:.1f}% of max heap", info.usagePercent);
        }
    }
    
    /**
     * Suggest garbage collection if needed
     */
    public static void suggestGCIfNeeded(int thresholdPercent) {
        if (isMemoryHigh(thresholdPercent)) {
            logger.warn("Memory usage above {}%, suggesting garbage collection", thresholdPercent);
            System.gc();
            
            // Log memory after GC
            try {
                Thread.sleep(100); // Give GC time to run
                MemoryInfo afterGC = getMemoryInfo();
                logger.info("After GC: Memory usage: {:.1f}%", afterGC.usagePercent);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
    }
    
    /**
     * Memory information holder
     */
    public static class MemoryInfo {
        public final long maxMemory;
        public final long totalMemory;
        public final long freeMemory;
        public final long usedMemory;
        public final double usagePercent;
        
        public MemoryInfo(long maxMemory, long totalMemory, long freeMemory, long usedMemory, double usagePercent) {
            this.maxMemory = maxMemory;
            this.totalMemory = totalMemory;
            this.freeMemory = freeMemory;
            this.usedMemory = usedMemory;
            this.usagePercent = usagePercent;
        }
    }
}

==============================================

package com.abcdefg.abc.merge.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.merge.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.merge.connection.ConnectionManager;
import com.abcdefg.abc.merge.constants.Constants;
import com.abcdefg.abc.merge.model.DoubleDoubleDocumentDTO;

/**
 * Utility class for processing DoubleDouble CSV extracts and updating database records
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
    }
    
    /**
     * Process a single CSV file from Input folder
     * Returns ProcessingResult for global reporting
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== Starting processing of CSV file: {} =====", csvFileName);
        
        Path inprogressDir = Paths.get(basePath, Constants.INPROGRESS_FOLDER);
        Path inprogressFile = inprogressDir.resolve(csvFileName);
        
        try {
            // Step 1: Move file to Inprogress folder
            logger.info("Moving {} to Inprogress folder", csvFileName);
            Files.move(csvFilePath, inprogressFile, StandardCopyOption.REPLACE_EXISTING);
            logger.info("File moved to: {}", inprogressFile);
            
            // Step 2: Process the CSV file
            ProcessingResult result = processCSVWithTracking(inprogressFile);
            
            // Step 3: Handle file lifecycle based on results
            handleFileLifecycle(inprogressFile, result);
            
            logger.info("===== Completed processing of CSV file: {} =====", csvFileName);
            logger.info("Total Rows: {}, Success: {}, Failed: {}, Empty Lines Skipped: {}", 
                       result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to Failed folder on critical error
            try {
                Path failedDir = Paths.get(basePath, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(inprogressFile, failedFile, StandardCopyOption.REPLACE_EXISTING);
                
                // Create error log file
                createErrorLogFile(failedDir, csvFileName, "Critical processing error: " + e.getMessage());
                
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder: {}", csvFileName, ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    /**
     * Process CSV file with row-level tracking
     */
    private ProcessingResult processCSVWithTracking(Path csvFilePath) throws IOException {
        ProcessingResult result = new ProcessingResult();
        String csvFileName = csvFilePath.getFileName().toString();
        
        // Prepare output files for success and failure tracking
        Path completedDir = Paths.get(basePath, Constants.COMPLETED_FOLDER);
        Path failedDir = Paths.get(basePath, Constants.FAILED_FOLDER);
        
        String baseFileName = csvFileName.replabc(".csv", "");
        Path successFile = completedDir.resolve(baseFileName + "_SUCCESS.csv");
        Path failedFile = failedDir.resolve(baseFileName + "_FAILED.csv");
        
        List<String> successRows = new ArrayList<>();
        List<String> failedRows = new ArrayList<>();
        
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = skipHeaderProp == null ? true : Boolean.parseBoolean(skipHeaderProp);
        String headerLine = null;
        
        // Get batch size from configuration
        String batchSizeStr = config.getProperty("db.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        logger.info("Processing CSV file: {}", csvFilePath);
        logger.info("Delimiter: pipe (|), Skip header: {}, Batch size: {}", skipHeaderRow, batchSize);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()));
             Connection conn = connectionManager.getConnection()) {
            
            String line;
            int rowNum = 0;
            
            // Read and process header
            if (skipHeaderRow && (line = reader.readLine()) != null) {
                headerLine = line.trim();
                logger.debug("Header: {}", headerLine);
            }
            
            // Build header index map
            Map<String, Integer> headerIndex = buildHeaderIndex(headerLine);
            
            // Validate required columns exist
            if (!validateRequiredColumns(headerIndex, csvFileName)) {
                result.failureCount = 1;
                return result;
            }
            
            // Batch processing buffers
            List<DoubleDoubleDocumentDTO> batchDTOs = new ArrayList<>();
            List<String> batchLines = new ArrayList<>();
            
            // Process each row
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().length() == 0) {
                    result.skippedEmptyLines++;
                    continue;
                }
                
                result.totalRows++;
                String currentLine = line;
                
                try {
                    // Parse CSV row
                    String[] cells = parseCSVLine(line);
                    
                    // Build DTO from CSV row
                    DoubleDoubleDocumentDTO dto = buildDTOFromCSV(cells, headerIndex, csvFileName);
                    
                    dto.setDataMerged(true);
                    
                    // Validate DTO
                    validateDTO(dto, rowNum);
                    
                    // Add to batch
                    batchDTOs.add(dto);
                    batchLines.add(currentLine);
                    
                    // Process batch when it reaches batch size
                    if (batchDTOs.size() >= batchSize) {
                        processBatchWithFallback(batchDTOs, batchLines, conn, batchSize, 
                                               successRows, failedRows, result);
                        batchDTOs.clear();
                        batchLines.clear();
                        
                        logger.debug("Processed batch at row {}", rowNum);
                    }
                    
                } catch (Exception e) {
                    result.failureCount++;
                    String errorMsg = e.getMessage().replabc("\"", "'").replabc("|", ";");
                    failedRows.add(currentLine + "|" + errorMsg);
                    logger.error("Row {}: Failed to process - {}", rowNum, e.getMessage());
                }
            }
            
            // Process remaining batch
            if (!batchDTOs.isEmpty()) {
                processBatchWithFallback(batchDTOs, batchLines, conn, batchSize, 
                                       successRows, failedRows, result);
                logger.debug("Processed final batch of {} rows", batchDTOs.size());
            }
            
            logger.info("CSV processing completed: Total={}, Success={}, Failed={}, Skipped Empty={}", 
                       result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
            
        } catch (SQLException e) {
            logger.error("Database connection error while processing CSV: {}", csvFileName, e);
            throw new IOException("Database error: " + e.getMessage(), e);
        }
        
        // Write success file
        if (!successRows.isEmpty()) {
            writeSuccessFile(successFile, headerLine, successRows);
            logger.info("Created success file with {} rows: {}", successRows.size(), successFile);
        }
        
        // Write failed file (only if there are failures)
        if (!failedRows.isEmpty()) {
            writeFailedFile(failedFile, headerLine, failedRows);
            logger.info("Created failed file with {} rows: {}", failedRows.size(), failedFile);
        }
        
        return result;
    }
    
    /**
     * Build header index map for CSV columns
     */
    private Map<String, Integer> buildHeaderIndex(String headerLine) {
        Map<String, Integer> index = new HashMap<>();
        if (headerLine == null || headerLine.trim().isEmpty()) {
            return index;
        }
        
        String[] headers = headerLine.split("\\|", -1);
        for (int i = 0; i < headers.length; i++) {
            String header = headers[i].trim().replabc("\"", "");
            index.put(header, i);
        }
        
        logger.debug("Header index built with {} columns", index.size());
        return index;
    }
    
    /**
     * Validate that all required columns exist in CSV
     */
    private boolean validateRequiredColumns(Map<String, Integer> headerIndex, String csvFileName) {
    	// NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
    	String[] requiredColumns = {
    	    "NAME", "DOCUMENT_TYPE", "CLAIM_NUMBER", "POLICY_NUMBER", "DOCUMENT_SUBTYPE",
    	    "AUTHOR", "DOCUMENT_DESCRIPTION", "STATUS", "EXTERNALID", "ID", "CLAIMID"
    	};
        
        List<String> missingColumns = new ArrayList<>();
        for (String col : requiredColumns) {
            if (!headerIndex.containsKey(col)) {
                missingColumns.add(col);
            }
        }
        
        if (!missingColumns.isEmpty()) {
            logger.error("CSV file {} is missing required columns: {}", csvFileName, missingColumns);
            return false;
        }
        
        return true;
    }
    
    /**
     * Parse CSV line handling pipe delimiter
     */
    private String[] parseCSVLine(String line) {
        // Simple split by pipe - handles basic CSV
        return line.split("\\|", -1);
    }
    
    /**
     * Build DTO from CSV row
     */
    private DoubleDoubleDocumentDTO buildDTOFromCSV(String[] cells, Map<String, Integer> headerIndex, String csvFileName) {
        DoubleDoubleDocumentDTO dto = new DoubleDoubleDocumentDTO();

        // NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
        dto.setDocumentTitle(safeTrim(getCell(cells, headerIndex.get("NAME"))));
        dto.setDocumentType(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_TYPE"))));
        dto.setClaimNumber(safeTrim(getCell(cells, headerIndex.get("CLAIM_NUMBER"))));
        dto.setPolicyNumber(safeTrim(getCell(cells, headerIndex.get("POLICY_NUMBER"))));
        dto.setDocumentSubtype(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_SUBTYPE"))));
        dto.setAuthor(safeTrim(getCell(cells, headerIndex.get("AUTHOR"))));
        dto.setDocumentDescription(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_DESCRIPTION"))));
        dto.setClaimType(safeTrim(getCell(cells, headerIndex.get("STATUS"))));
        dto.setExternalID(safeTrim(getCell(cells, headerIndex.get("EXTERNALID"))));
        dto.setGwDocumentID(safeTrim(getCell(cells, headerIndex.get("ID"))));
        dto.setClaimID(safeTrim(getCell(cells, headerIndex.get("CLAIMID"))));
        dto.setCsvFileName(csvFileName);

        return dto;
    }

    
    /**
     * Validate DTO has required fields
     */
    private void validateDTO(DoubleDoubleDocumentDTO dto, int rowNum) throws Exception {
        if (dto.getExternalID() == null || dto.getExternalID().trim().isEmpty()) {
            throw new Exception("Missing required field: externalID");
        }
    }
    
    /**
     * Update database record matching externalID
     */
    private int updateDatabaseRecord(DoubleDoubleDocumentDTO dto, Connection conn) throws SQLException {
        // NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
        String updateSQL = "UPDATE " + dbTableName + " SET " +
            "documentTitle = ?, " +
            "documentType = ?, " +
            "documentSubtype = ?, " +
            "claimID = ?, " +
            "claimNumber = ?, " +
            "policyNumber = ?, " +
            "author = ?, " +
            "documentDescription = ?, " +
            "claimType = ?, " +
            "gwDocumentID = ?, " +
            "isDataMerged = ?, " +
            "CC_Extract_csv_file_name = ? " +
            "WHERE externalID = ?";

        try (PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            pstmt.setString(1, dto.getDocumentTitle());
            pstmt.setString(2, dto.getDocumentType());
            pstmt.setString(3, dto.getDocumentSubtype());
            pstmt.setString(4, dto.getClaimID());
            pstmt.setString(5, dto.getClaimNumber());
            pstmt.setString(6, dto.getPolicyNumber());
            pstmt.setString(7, dto.getAuthor());
            pstmt.setString(8, dto.getDocumentDescription());
            pstmt.setString(9, dto.getClaimType());
            pstmt.setString(10, dto.getGwDocumentID());
            pstmt.setBoolean(11, true);
            pstmt.setString(12, dto.getCsvFileName());
            pstmt.setString(13, dto.getExternalID()); // WHERE clause

            System.out.println("UPDATE SQL executed: rows affected for externalID=" + dto.getExternalID());
            int rowsUpdated = pstmt.executeUpdate();

            if (logger.isDebugEnabled()) {
                logger.debug("UPDATE SQL executed: {} rows affected for externalID={}", rowsUpdated, dto.getExternalID());
            }

            return rowsUpdated;
        }
    }
    
    /**
     * Update multiple database records using JDBC batch processing
     * Significantly faster than individual UPDATEs for large datasets
     * 
     * @param dtoList List of DTOs to update
     * @param conn Database connection
     * @param batchSize Maximum batch size
     * @return Number of rows successfully updated
     */
    private int updateDatabaseRecordsBatch(List<DoubleDoubleDocumentDTO> dtoList, Connection conn, int batchSize) 
            throws SQLException {
        
        if (dtoList == null || dtoList.isEmpty()) {
            return 0;
        }
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
            "documentTitle = ?, " +
            "documentType = ?, " +
            "documentSubtype = ?, " +
            "claimID = ?, " +
            "claimNumber = ?, " +
            "policyNumber = ?, " +
            "author = ?, " +
            "documentDescription = ?, " +
            "claimType = ?, " +
            "gwDocumentID = ?, " +
            "isDataMerged = ?, " +
            "CC_Extract_csv_file_name = ? " +
            "WHERE externalID = ?";
        
        int totalUpdated = 0;
        
        try (PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            int batchCount = 0;
            
            for (DoubleDoubleDocumentDTO dto : dtoList) {
                pstmt.setString(1, dto.getDocumentTitle());
                pstmt.setString(2, dto.getDocumentType());
                pstmt.setString(3, dto.getDocumentSubtype());
                pstmt.setString(4, dto.getClaimID());
                pstmt.setString(5, dto.getClaimNumber());
                pstmt.setString(6, dto.getPolicyNumber());
                pstmt.setString(7, dto.getAuthor());
                pstmt.setString(8, dto.getDocumentDescription());
                pstmt.setString(9, dto.getClaimType());
                pstmt.setString(10, dto.getGwDocumentID());
                pstmt.setBoolean(11, true);
                pstmt.setString(12, dto.getCsvFileName());
                pstmt.setString(13, dto.getExternalID());
                
                pstmt.addBatch();
                batchCount++;
                
                // Execute batch when reaching batch size
                if (batchCount >= batchSize) {
                    int[] updateCounts = pstmt.executeBatch();
                    totalUpdated += countSuccessfulUpdates(updateCounts);
                    batchCount = 0;
                    
                    logger.debug("Executed batch of {} updates", batchSize);
                }
            }
            
            // Execute remaining batch
            if (batchCount > 0) {
                int[] updateCounts = pstmt.executeBatch();
                totalUpdated += countSuccessfulUpdates(updateCounts);
                
                logger.debug("Executed final batch of {} updates", batchCount);
            }
            
            logger.info("Batch UPDATE completed: {} records updated from {} DTOs", totalUpdated, dtoList.size());
            
            return totalUpdated;
        }
    }
    
    /**
     * Count successful updates from batch execution results
     */
    private int countSuccessfulUpdates(int[] updateCounts) {
        int count = 0;
        for (int updateCount : updateCounts) {
            if (updateCount > 0) {
                count++;
            }
        }
        return count;
    }
    
    /**
     * Process a batch of DTOs with fallback to individual processing on failure
     */
    private void processBatchWithFallback(List<DoubleDoubleDocumentDTO> dtoList,
                                          List<String> linesList,
                                          Connection conn,
                                          int batchSize,
                                          List<String> successRows,
                                          List<String> failedRows,
                                          ProcessingResult result) throws SQLException {
        
        if (dtoList.isEmpty()) {
            return;
        }
        
        try {
            // Attempt batch UPDATE
            int successCount = updateDatabaseRecordsBatch(dtoList, conn, batchSize);
            
            // If all records updated successfully, add to success rows
            if (successCount == dtoList.size()) {
                successRows.addAll(linesList);
                result.successCount += successCount;
                logger.debug("Batch UPDATE successful: {}/{} records", successCount, dtoList.size());
            } else {
                // Partial success - fall back to individual processing to identify failures
                logger.warn("Batch UPDATE partial success: {}/{} - falling back to individual processing", 
                           successCount, dtoList.size());
                processBatchIndividually(dtoList, linesList, conn, successRows, failedRows, result);
            }
            
        } catch (SQLException e) {
            // Batch failed - fall back to individual processing
            logger.warn("Batch UPDATE failed, falling back to individual processing: {}", e.getMessage());
            processBatchIndividually(dtoList, linesList, conn, successRows, failedRows, result);
        }
    }
    
    /**
     * Process DTOs individually (fallback method)
     */
    private void processBatchIndividually(List<DoubleDoubleDocumentDTO> dtoList,
                                         List<String> linesList,
                                         Connection conn,
                                         List<String> successRows,
                                         List<String> failedRows,
                                         ProcessingResult result) {
        
        for (int i = 0; i < dtoList.size(); i++) {
            DoubleDoubleDocumentDTO dto = dtoList.get(i);
            String line = linesList.get(i);
            
            try {
                int rowsUpdated = updateDatabaseRecord(dto, conn);
                
                if (rowsUpdated > 0) {
                    result.successCount++;
                    successRows.add(line);
                } else {
                    result.failureCount++;
                    String errorMsg = "No record found with externalID=" + dto.getExternalID();
                    failedRows.add(line + "|" + errorMsg);
                    logger.warn("No record found for externalID={}", dto.getExternalID());
                }
                
            } catch (Exception e) {
                result.failureCount++;
                String errorMsg = e.getMessage().replabc("\"", "'").replabc("|", ";");
                failedRows.add(line + "|" + errorMsg);
                logger.error("Failed to update externalID={}: {}", dto.getExternalID(), e.getMessage());
            }
        }
    }

    
    /**
     * Write success rows to file
     */
    private void writeSuccessFile(Path successFile, String headerLine, List<String> successRows) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
            // Write header
            if (headerLine != null) {
                writer.write(headerLine);
                writer.newLine();
            }
            
            // Write success rows
            for (String row : successRows) {
                writer.write(row);
                writer.newLine();
            }
        }
    }
    
    /**
     * Write failed rows to file with error messages
     */
    private void writeFailedFile(Path failedFile, String headerLine, List<String> failedRows) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
            // Write header with error_message column
            if (headerLine != null) {
                writer.write(headerLine + "|error_message");
                writer.newLine();
            }
            
            // Write failed rows (already have error message appended)
            for (String row : failedRows) {
                writer.write(row);
                writer.newLine();
            }
        }
    }
    
    /**
     * Handle file lifecycle after processing
     */
    private void handleFileLifecycle(Path inprogressFile, ProcessingResult result) throws IOException {
        String csvFileName = inprogressFile.getFileName().toString();
        String archiveProp = config.getProperty("app.archive.after.processing");
        boolean archiveAfterProcessing = archiveProp == null ? true : Boolean.parseBoolean(archiveProp);
        
        if (archiveAfterProcessing) {
            // Move to Archive folder
            Path archiveDir = Paths.get(basePath, Constants.ARCHIVE_FOLDER);
            Path archiveFile = archiveDir.resolve(csvFileName);
            
            Files.move(inprogressFile, archiveFile, StandardCopyOption.REPLACE_EXISTING);
            logger.info("Moved processed file to Archive: {}", archiveFile);
        } else {
            // Delete the file
            Files.delete(inprogressFile);
            logger.info("Deleted processed file from Inprogress: {}", csvFileName);
        }
    }
    
    /**
     * Create error log file for critical failures
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replabc(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + java.time.LocalDateTime.now());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file for {}", csvFileName, e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        // Handle "null" string as empty
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
    
    /**
     * Safe get cell from array
     */
    private String getCell(String[] cells, Integer index) {
        if (index == null || index < 0 || index >= cells.length) {
            return "";
        }
        return cells[index];
    }
}

==================================================================================================================================================
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">

    <Appenders>
        <!-- Main Application Log with Rolling -->
        <RollingFile name="FileLogger" 
                     fileName="D:\Rameshwar\CCDataMergeUtility\logs\CCDataMergeUtility.log"
                     filePattern="D:\Rameshwar\CCDataMergeUtility\logs\CCDataMergeUtility-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="100MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Batch Processing Log (detailed batch operations) -->
        <RollingFile name="BatchProcessingLogger" 
                     fileName="D:\Rameshwar\CCDataMergeUtility\logs\BatchProcessing.log"
                     filePattern="D:\Rameshwar\CCDataMergeUtility\logs\BatchProcessing-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Progress Monitoring Log -->
        <RollingFile name="ProgressLogger" 
                     fileName="D:\Rameshwar\CCDataMergeUtility\logs\Progress.log"
                     filePattern="D:\Rameshwar\CCDataMergeUtility\logs\Progress-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="20MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Error-Only Log -->
        <RollingFile name="ErrorLogger" 
                     fileName="D:\Rameshwar\CCDataMergeUtility\logs\Errors.log"
                     filePattern="D:\Rameshwar\CCDataMergeUtility\logs\Errors-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
            <ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>

        <!-- Console Appender -->
        <Console name="ConsoleLogger" target="SYSTEM_OUT">
            <PatternLayout pattern="%d{HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
        </Console>
    </Appenders>

    <Loggers>
        <!-- Batch Processing Logger (Utils class) -->
        <Logger name="com.abcdefg.abc.merge.utils.Utils" level="info" additivity="false">
            <AppenderRef ref="BatchProcessingLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Progress Monitoring Logger -->
        <Logger name="com.abcdefg.abc.merge.utils.GlobalProcessingReport" level="info" additivity="false">
            <AppenderRef ref="ProgressLogger"/>
            <AppenderRef ref="FileLogger"/>
        </Logger>

        <!-- Memory Monitoring Logger -->
        <Logger name="com.abcdefg.abc.merge.utils.MemoryMonitor" level="info" additivity="false">
            <AppenderRef ref="ProgressLogger"/>
            <AppenderRef ref="FileLogger"/>
        </Logger>

        <!-- Main Service Logger -->
        <Logger name="com.abcdefg.abc.merge.DoubleDoubleDataMergeService" level="info" additivity="false">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Root logger -->
        <Root level="info">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Root>
    </Loggers>

</Configuration>



==================================================================================================================================================

# ===========================================================
# CCDataMergeUtility - Production Configuration
# ===========================================================
# Last Updated: 2025-10-12
# Optimized for high-volume production processing (2M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base_path=D:/Rameshwar/DoubleDoubleDataMerge/
app.skipHeaderRow=true
app.archive.after.processing=true

# ===== PERFORMANCE TUNING =====
# Thread Pool Size: Number of parallel CSV file processors
# Recommended: 20-30 for production (2M documents/day)
# Default: 5 for development/testing
app.thread.pool.size=20

# Progress Monitoring: Log progress every N minutes
# Set to 0 to disable progress monitoring
# Recommended: 5 minutes for production visibility
app.progress.log.interval.minutes=5

# ===== DATABASE SETTINGS =====
db.serverName=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.dbName=RAMDB

# Database Connection Pool Size
# IMPORTANT: Should be >= app.thread.pool.size + 10 (buffer for overhead)
# Recommended: 30-40 for production with 20 threads
db.pool.size=30

# Target table for UPDATE operations
db.staging.claimcenterdbtable=[RAMDB].[dbo].[Wawa_Doc_Migration_Transit_Data]

# Batch UPDATE Size: Number of rows per JDBC batch
# CRITICAL FOR PERFORMANCE: Batch UPDATEs are 50-100x faster than individual UPDATEs
# Recommended: 1000-2000 rows per batch
# Default: 1000
db.batch.update.size=1000

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage of max heap)
# Warning logged when memory usage exceeds this threshold
app.memory.warning.threshold=80

# ===== NOTES =====
# Performance Estimates for 2M documents/day:
# - With batch UPDATEs (1000/batch): ~15-20 minutes total
# - Thread pool of 20: Processes ~200 files in parallel
# - Connection pool of 30: Adequate for 20 threads + overhead
#
# JVM Recommended Settings:
# -Xms4G -Xmx8G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# Global Processing Report:
# - Written to: {app.base_path}/CCDataMerge_Summary_Report_{timestamp}.csv
# - Contains: Total files, rows, success/failure counts, processing time, throughput


====================================================================================================================================

@echo off
REM ============================================================================
REM CCDataMergeUtility - Batch Execution Script
REM ============================================================================
REM Simplified version with DEV/UAT and PROD profiles
REM ============================================================================

setlocal

REM ============================================================================
REM ENVIRONMENT SELECTION - CHANGE THIS FOR YOUR ENVIRONMENT
REM ============================================================================
REM Options: DEV, UAT, or PROD
REM DEV/UAT: 8 cores, 32GB RAM
REM PROD:    16 cores, 64GB RAM, 2M documents/day

set ENVIRONMENT=PROD

REM ============================================================================
REM BASIC CONFIGURATION
REM ============================================================================

set WORK_DIR=D:\Rameshwar\CCDataMergeUtility
cd /d "%WORK_DIR%"

set JAR_FILE=%WORK_DIR%\target\CCDataMergeUtility-0.0.1-SNAPSHOT.jar
set CONFIG_DIR=%WORK_DIR%\config
set PROPERTIES_FILE=%CONFIG_DIR%\CCDataMergeUtility.properties
set LOG4J_CONFIG=%CONFIG_DIR%\Log4j2.xml

REM ============================================================================
REM MEMORY SETTINGS BY ENVIRONMENT
REM ============================================================================

if /I "%ENVIRONMENT%"=="PROD" (
    echo Selected Environment: PRODUCTION
    echo   Server: 16 cores, 64GB RAM
    echo   Expected Volume: 2M documents/day
    echo.
    
    REM PROD: 16 cores, 64GB RAM, 2M documents
    set MIN_HEAP=6g
    set MAX_HEAP=12g
    set YOUNG_GEN=4g
    set METASPACE=256m
    set GC_THREADS=4
    set PARALLEL_THREADS=8
    
) else (
    echo Selected Environment: DEV/UAT
    echo   Server: 8 cores, 32GB RAM
    echo   Expected Volume: Test data
    echo.
    
    REM DEV/UAT: 8 cores, 32GB RAM
    set MIN_HEAP=3g
    set MAX_HEAP=6g
    set YOUNG_GEN=2g
    set METASPACE=256m
    set GC_THREADS=2
    set PARALLEL_THREADS=4
)

REM ============================================================================
REM BUILD JVM ARGUMENTS
REM ============================================================================

REM Memory Settings
set JVM_ARGS=-Xms%MIN_HEAP% -Xmx%MAX_HEAP%
set JVM_ARGS=%JVM_ARGS% -XX:NewSize=%YOUNG_GEN% -XX:MaxNewSize=%YOUNG_GEN%
set JVM_ARGS=%JVM_ARGS% -XX:MetaspabcSize=%METASPACE% -XX:MaxMetaspabcSize=%METASPACE%

REM Garbage Collection (G1GC - Best for large datasets)
set JVM_ARGS=%JVM_ARGS% -XX:+UseG1GC
set JVM_ARGS=%JVM_ARGS% -XX:MaxGCPauseMillis=200
set JVM_ARGS=%JVM_ARGS% -XX:ConcGCThreads=%GC_THREADS%
set JVM_ARGS=%JVM_ARGS% -XX:ParallelGCThreads=%PARALLEL_THREADS%

REM Performance Optimizations
set JVM_ARGS=%JVM_ARGS% -XX:+UseStringDeduplication
set JVM_ARGS=%JVM_ARGS% -XX:+UseCompressedOops
set JVM_ARGS=%JVM_ARGS% -Xss1m

REM Error Handling
set JVM_ARGS=%JVM_ARGS% -XX:+HeapDumpOnOutOfMemoryError
set JVM_ARGS=%JVM_ARGS% -XX:HeapDumpPath=%WORK_DIR%\heapdumps
set JVM_ARGS=%JVM_ARGS% -XX:+ExitOnOutOfMemoryError

REM Config Files
set JVM_ARGS=%JVM_ARGS% -Dconfig.properties.path="%PROPERTIES_FILE%"
set JVM_ARGS=%JVM_ARGS% -Dlog4j.configurationFile="%LOG4J_CONFIG%"

REM ============================================================================
REM PRE-EXECUTION CHECKS
REM ============================================================================

echo ============================================================================
echo CCDataMergeUtility - Starting Execution
echo ============================================================================
echo Environment: %ENVIRONMENT%
echo Start Time: %date% %time%
echo.

if not exist "%JAR_FILE%" (
    echo [ERROR] JAR file not found: %JAR_FILE%
    echo Please build the project first: mvn clean package
    pause
    exit /b 1
)

if not exist "%PROPERTIES_FILE%" (
    echo [ERROR] Properties file not found: %PROPERTIES_FILE%
    pause
    exit /b 1
)

if not exist "%LOG4J_CONFIG%" (
    echo [ERROR] Log4j config not found: %LOG4J_CONFIG%
    pause
    exit /b 1
)

echo Memory Configuration:
echo   Min Heap: %MIN_HEAP%
echo   Max Heap: %MAX_HEAP%
echo   Young Gen: %YOUNG_GEN%
echo   GC Threads: %GC_THREADS%
echo   Parallel Threads: %PARALLEL_THREADS%
echo.

echo ============================================================================
echo Executing...
echo ============================================================================
echo.

REM ============================================================================
REM EXECUTE
REM ============================================================================

java %JVM_ARGS% -jar "%JAR_FILE%"

set EXIT_CODE=%ERRORLEVEL%

REM ============================================================================
REM SUMMARY
REM ============================================================================

echo.
echo ============================================================================
echo Execution Summary
echo ============================================================================
echo End Time: %date% %time%
echo Exit Code: %EXIT_CODE%
echo.

if %EXIT_CODE% EQU 0 (
    echo [SUCCESS] CCDataMergeUtility completed successfully!
    echo Check logs: %WORK_DIR%\logs\
    echo Check report: %WORK_DIR%\reports\
) else (
    echo [ERROR] Failed with exit code: %EXIT_CODE%
    echo Check error log: %WORK_DIR%\logs\Errors.log
)

echo ============================================================================
pause

endlocal
exit /b %EXIT_CODE%

**************************************************************************************************************

package com.abcdefg.abc.index;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.index.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.index.connection.ConnectionManager;
import com.abcdefg.abc.index.constants.Constants;
import com.abcdefg.abc.index.utils.GlobalProcessingReport;
import com.abcdefg.abc.index.utils.Utils;

/**
 * CCDataIndexAndPackagingUtility - Main Service Class
 * 
 * Purpose:
 * - Process CSV files from CCDataMergeUtility output
 * - Apply batching/indexing logic to documents
 * - Update database with batch/set metadata
 * - Generate packaging CSV files for migration
 * 
 * Processing Flow:
 * Step 1: Validate/Create folder structure
 * Step 2: Move CSV files from Completed → Indexing\Data
 * Step 3: Extract claim numbers → Query DB → Apply batching logic
 * Step 4: Update database with indexing metadata
 * Step 5a: Write success/failed tracking files → Archive CSV
 * Step 5b: Generate packaging CSV files (pipe-delimited, max 50K rows)
 */
public class CCDataIndexPackagingService {

    private static final Logger logger = LogManager.getLogger(CCDataIndexPackagingService.class);

    public static void main(String[] args) {

        logger.info("===== CCDataIndexAndPackagingUtility STARTED =====");
        ConnectionManager connManager = null;
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        
        // Initialize global processing report
        GlobalProcessingReport globalReport = new GlobalProcessingReport();

        try {
            // Load configuration
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            logger.info("=== CONFIGURATION ===");
            logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            logger.info("Config File: {}", System.getProperty("config.file"));
            logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String basePath = config.getProperty("app.base_path");
            logger.info("Base Path: {}", basePath);
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService;
            
            logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            final ConnectionManager[] connManagerRef = new ConnectionManager[1];
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        globalReport.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            globalReport.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                logger.info("Shutdown hook completed");
            }));
            
            // STEP 1: Validate and create required folder structure
            logger.info("=== STEP 1: Validating Folder Structure ===");
            if (!validateAndCreateFolderStructure(basePath)) {
                logger.error("Failed to validate/create folder structure. Exiting...");
                return;
            }
            logger.info("Folder structure validated successfully");
            
            // Initialize connection manager
            connManager = new ConnectionManager(config);
            connManagerRef[0] = connManager;
            logger.info("Database connection pool initialized");
            
            // Initialize utility class
            Utils utils = new Utils(config, connManager);
            
            // STEP 2: Move CSV files from Completed → Indexing\Data
            logger.info("=== STEP 2: Moving CSV Files ===");
            int movedFileCount = moveCSVFilesToIndexingData(basePath);
            logger.info("Moved {} CSV file(s) to Indexing\\Data folder", movedFileCount);
            
            if (movedFileCount == 0) {
                logger.warn("No CSV files found to process. Exiting...");
                return;
            }
            
            // STEP 3-5: Process all CSV files in Indexing\Data folder (MULTI-THREADED)
            logger.info("=== STEP 3-5: Processing CSV Files (Multi-Threaded) ===");
            Path indexingDataDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
            int processedFileCount = processCSVFiles(indexingDataDir, utils, executor, globalReport);
            
            logger.info("=== PROCESSING COMPLETE ===");
            logger.info("Total CSV files processed: {}", processedFileCount);
            logger.info("===== CCDataIndexAndPackagingUtility COMPLETED SUCCESSFULLY =====");

        } catch (Exception e) {
            logger.error("Critical error in CCDataIndexPackagingService", e);
            e.printStackTrabc();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService grabcfully
            if (executor != null) {
                try {
                    logger.info("Shutting down executor service...");
                    executor.shutdown();
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        logger.warn("Executor did not terminate in time, forcing shutdown...");
                        executor.shutdownNow();
                        
                        if (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                            logger.error("Executor did not terminate after forced shutdown");
                        }
                    }
                    logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManager != null) {
                try {
                    logger.info("Closing database connection pool...");
                    connManager.close();
                    logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    logger.error("Error closing connection manager", e);
                }
            }
            
            // Write final summary report
            try {
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                
                logger.info("===== FINAL SUMMARY =====");
                logger.info(globalReport.getSummary());
                
                globalReport.writeReportToFile(basePath);
                logger.info("Summary report written to: {}", basePath);
            } catch (Exception e) {
                logger.error("Failed to write summary report", e);
            }
            
            logger.info("Application terminated");
        }
    }

    // ==================== STEP 1: VALIDATE AND CREATE FOLDERS ====================
    
    /**
     * Step 1: Validate and create required folder structure
     * 
     * Required folders:
     * - D:\Rameshwar\DoubleDoubleDataMerge\Completed (source)
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Data
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Archive
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Completed
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Failed
     * - D:\Rameshwar\DoubleDoubleDataMerge\Packaging
     */
    private static boolean validateAndCreateFolderStructure(String basePath) {
        boolean success = true;
        
        // Define required folders
        String[] requiredFolders = {
            Constants.COMPLETED_SOURCE_FOLDER,
            Constants.INDEXING_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.DATA_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.ARCHIVE_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.COMPLETED_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.FAILED_FOLDER,
            Constants.PACKAGING_FOLDER
        };
        
        for (String folderName : requiredFolders) {
            File folder = new File(basePath, folderName);
            
            if (folder.exists() && folder.isDirectory()) {
                logger.info("✓ Folder exists: {}", folder.getAbsolutePath());
            } else {
                // Create folder
                if (folder.mkdirs()) {
                    logger.info("✓ Folder created: {}", folder.getAbsolutePath());
                } else {
                    logger.error("✗ Failed to create folder: {}", folder.getAbsolutePath());
                    success = false;
                }
            }
        }
        
        return success;
    }
    
    // ==================== STEP 2: MOVE CSV FILES ====================
    
    /**
     * Step 2: Move all CSV files from Completed → Indexing\Data
     * 
     * Source: D:\Rameshwar\DoubleDoubleDataMerge\Completed
     * Target: D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Data
     */
    private static int moveCSVFilesToIndexingData(String basePath) {
        Path sourceDir = Paths.get(basePath, Constants.COMPLETED_SOURCE_FOLDER);
        Path targetDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
        
        logger.info("Source Directory: {}", sourceDir);
        logger.info("Target Directory: {}", targetDir);
        
        int movedCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    try {
                        String fileName = csvFile.getFileName().toString();
                        Path targetFile = targetDir.resolve(fileName);
                        
                        Files.move(csvFile, targetFile, StandardCopyOption.REPLACE_EXISTING);
                        logger.info("Moved: {} → {}", fileName, targetDir);
                        movedCount++;
                        
                    } catch (IOException e) {
                        logger.error("Failed to move file: {}", csvFile.getFileName(), e);
                    }
                }
            }
        } catch (IOException e) {
            logger.error("Error reading source directory: {}", sourceDir, e);
        }
        
        return movedCount;
    }
    
    // ==================== STEP 3-5: PROCESS CSV FILES (MULTI-THREADED) ====================
    
    /**
     * Step 3-5: Process all CSV files in Indexing\Data folder using multi-threading
     * 
     * For each CSV file (in parallel):
     * - Extract claim numbers
     * - Query database for documents
     * - Apply batching/indexing logic
     * - Update database (BATCH UPDATEs)
     * - Write tracking files
     * - Generate packaging CSV files
     * - Archive processed CSV
     */
    private static int processCSVFiles(Path indexingDataDir, Utils utils, 
                                      ExecutorService executor, GlobalProcessingReport globalReport) {
        logger.info("Scanning Indexing\\Data directory for CSV files: {}", indexingDataDir);
        
        // Collect all CSV files
        List<Path> csvFiles = new ArrayList<>();
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexingDataDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    csvFiles.add(csvFile);
                    globalReport.incrementCSVFilesQueued();
                }
            }
        } catch (IOException e) {
            logger.error("Error reading Indexing\\Data directory: {}", indexingDataDir, e);
            return 0;
        }
        
        if (csvFiles.isEmpty()) {
            logger.warn("No CSV files found in Indexing\\Data directory: {}", indexingDataDir);
            return 0;
        }
        
        logger.info("Found {} CSV file(s) to process with multi-threading", csvFiles.size());
        
        // Process each CSV file in parallel
        for (Path csvFile : csvFiles) {
            final String fileName = csvFile.getFileName().toString();
            
            executor.submit(() -> {
                try {
                    logger.info("Thread-{}: Processing CSV file: {}", 
                               Thread.currentThread().getId(), fileName);
                    
                    // Process the CSV file (Steps 3-5)
                    Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                    
                    // Record success in global report
                    globalReport.recordCSVFileSuccess(fileName, result);
                    
                    logger.info("Thread-{}: Completed CSV file: {}", 
                               Thread.currentThread().getId(), fileName);
                    
                } catch (Exception e) {
                    logger.error("Thread-{}: Error processing CSV file: {}", 
                                Thread.currentThread().getId(), fileName, e);
                    globalReport.recordCSVFileFailure(fileName, e.getMessage());
                }
            });
        }
        
        // Wait for all tasks to complete
        executor.shutdown();
        try {
            logger.info("Waiting for all CSV files to complete processing...");
            executor.awaitTermination(24, TimeUnit.HOURS); // Max 24 hours
            logger.info("All CSV files processing completed");
        } catch (InterruptedException e) {
            logger.error("Processing interrupted", e);
            executor.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        return csvFiles.size();
    }
}


=======================================

package com.abcdefg.abc.index.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // CSV file-level counters
    private final AtomicInteger totalCSVFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesFailed = new AtomicInteger(0);
    
    // Claim-level counters
    private final AtomicInteger totalClaimsProcessed = new AtomicInteger(0);
    
    // Document-level counters
    private final AtomicLong totalDocumentsProcessed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexFailed = new AtomicLong(0);
    private final AtomicLong totalDocumentsPackaged = new AtomicLong(0);
    
    // Batch counters
    private final AtomicInteger totalBatchesCreated = new AtomicInteger(0);
    private final AtomicInteger totalPackagingFilesCreated = new AtomicInteger(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a CSV file has been queued for processing
     */
    public void incrementCSVFilesQueued() {
        totalCSVFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful CSV file processing
     */
    public void recordCSVFileSuccess(String fileName, Utils.ProcessingResult result) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesSuccess.incrementAndGet();
        
        totalClaimsProcessed.addAndGet(result.totalClaimNumbers);
        totalDocumentsProcessed.addAndGet(result.totalDocuments);
        totalDocumentsIndexed.addAndGet(result.indexedSuccessCount);
        totalDocumentsIndexFailed.addAndGet(result.indexedFailureCount);
        totalBatchesCreated.addAndGet(result.batchIDsCreated.size());
        totalPackagingFilesCreated.addAndGet(result.packagingFilesCreated);
        
        logger.info("CSV completed: {} | Claims={}, Docs={}, Indexed={}, Failed={}, Batches={}, PkgFiles={}", 
                   fileName, result.totalClaimNumbers, result.totalDocuments, 
                   result.indexedSuccessCount, result.indexedFailureCount,
                   result.batchIDsCreated.size(), result.packagingFilesCreated);
    }
    
    /**
     * Record failed CSV file processing
     */
    public void recordCSVFileFailure(String fileName, String errorMessage) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesFailed.incrementAndGet();
        
        logger.error("CSV failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("CSV Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalCSVFilesQueued.get(), totalCSVFilesProcessed.get(), totalCSVFilesQueued.get(),
                   totalCSVFilesSuccess.get(), totalCSVFilesFailed.get());
        logger.info("Claims: Processed={}", totalClaimsProcessed.get());
        logger.info("Documents: Processed={}, Indexed={}, Failed={}", 
                   totalDocumentsProcessed.get(), totalDocumentsIndexed.get(), totalDocumentsIndexFailed.get());
        logger.info("Batches: Created={}, Packaging Files={}", 
                   totalBatchesCreated.get(), totalPackagingFilesCreated.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- CSV File Statistics ---\n");
        sb.append(String.format("Total CSV Files Queued: %d\n", totalCSVFilesQueued.get()));
        sb.append(String.format("Total CSV Files Processed: %d\n", totalCSVFilesProcessed.get()));
        sb.append(String.format("CSV Files Success: %d\n", totalCSVFilesSuccess.get()));
        sb.append(String.format("CSV Files Failed: %d\n", totalCSVFilesFailed.get()));
        sb.append("\n--- Claim Statistics ---\n");
        sb.append(String.format("Total Claims Processed: %d\n", totalClaimsProcessed.get()));
        sb.append("\n--- Document Statistics ---\n");
        sb.append(String.format("Total Documents Processed: %d\n", totalDocumentsProcessed.get()));
        sb.append(String.format("Total Documents Indexed: %d\n", totalDocumentsIndexed.get()));
        sb.append(String.format("Total Documents Index Failed: %d\n", totalDocumentsIndexFailed.get()));
        sb.append("\n--- Batch & Packaging Statistics ---\n");
        sb.append(String.format("Total Batches Created: %d\n", totalBatchesCreated.get()));
        sb.append(String.format("Total Packaging Files Created: %d\n", totalPackagingFilesCreated.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
            long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d docs/sec, %d docs/min\n", docsPerSecond, docsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataIndexPackaging_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write CSV file statistics
            writer.newLine();
            writer.write("--- CSV File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total CSV Files Queued,%d", totalCSVFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Processed,%d", totalCSVFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Success,%d", totalCSVFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Failed,%d", totalCSVFilesFailed.get()));
            writer.newLine();
            
            // Write claim statistics
            writer.newLine();
            writer.write("--- Claim Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Claims Processed,%d", totalClaimsProcessed.get()));
            writer.newLine();
            
            // Write document statistics
            writer.newLine();
            writer.write("--- Document Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Documents Processed,%d", totalDocumentsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Indexed,%d", totalDocumentsIndexed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Index Failed,%d", totalDocumentsIndexFailed.get()));
            writer.newLine();
            
            // Write batch statistics
            writer.newLine();
            writer.write("--- Batch & Packaging Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Batches Created,%d", totalBatchesCreated.get()));
            writer.newLine();
            writer.write(String.format("Total Packaging Files Created,%d", totalPackagingFilesCreated.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
                long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Documents Per Second,%d", docsPerSecond));
                writer.newLine();
                writer.write(String.format("Documents Per Minute,%d", docsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
}


================================
package com.abcdefg.abc.index.utils;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Utility class for monitoring JVM memory usage
 */
public class MemoryMonitor {
    
    private static final Logger logger = LogManager.getLogger(MemoryMonitor.class);
    private static final long MB = 1024 * 1024;
    
    /**
     * Get current memory usage information
     */
    public static MemoryInfo getMemoryInfo() {
        Runtime runtime = Runtime.getRuntime();
        
        long maxMemory = runtime.maxMemory();
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long usedMemory = totalMemory - freeMemory;
        
        double usagePercent = (usedMemory * 100.0) / maxMemory;
        
        return new MemoryInfo(maxMemory, totalMemory, freeMemory, usedMemory, usagePercent);
    }
    
    /**
     * Check if memory usage is above threshold
     */
    public static boolean isMemoryHigh(int thresholdPercent) {
        MemoryInfo info = getMemoryInfo();
        return info.usagePercent >= thresholdPercent;
    }
    
    /**
     * Log current memory statistics
     */
    public static void logMemoryStats() {
        MemoryInfo info = getMemoryInfo();
        
        logger.info("Memory: Used={} MB, Total={} MB, Max={} MB, Usage={:.1f}%", 
                   info.usedMemory / MB, 
                   info.totalMemory / MB, 
                   info.maxMemory / MB,
                   info.usagePercent);
        
        // Warn if memory usage is high
        if (info.usagePercent >= 80) {
            logger.warn("HIGH MEMORY USAGE WARNING: {:.1f}% of max heap", info.usagePercent);
        }
    }
    
    /**
     * Suggest garbage collection if needed
     */
    public static void suggestGCIfNeeded(int thresholdPercent) {
        if (isMemoryHigh(thresholdPercent)) {
            logger.warn("Memory usage above {}%, suggesting garbage collection", thresholdPercent);
            System.gc();
            
            // Log memory after GC
            try {
                Thread.sleep(100); // Give GC time to run
                MemoryInfo afterGC = getMemoryInfo();
                logger.info("After GC: Memory usage: {:.1f}%", afterGC.usagePercent);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
    }
    
    /**
     * Memory information holder
     */
    public static class MemoryInfo {
        public final long maxMemory;
        public final long totalMemory;
        public final long freeMemory;
        public final long usedMemory;
        public final double usagePercent;
        
        public MemoryInfo(long maxMemory, long totalMemory, long freeMemory, long usedMemory, double usagePercent) {
            this.maxMemory = maxMemory;
            this.totalMemory = totalMemory;
            this.freeMemory = freeMemory;
            this.usedMemory = usedMemory;
            this.usagePercent = usagePercent;
        }
    }
}


============================================

package com.abcdefg.abc.index.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.List;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.index.constants.Constants;
import com.abcdefg.abc.index.model.DoubleDoubleDocumentDTO;

/**
 * Thread-safe packaging file writer
 * Handles writing documents to packaging CSV files with 50K row limit per file
 */
public class PackagingFileWriter {
    
    private static final Logger logger = LogManager.getLogger(PackagingFileWriter.class);
    
    private final String basePath;
    private final Object writeLock = new Object(); // Synchronization lock
    private BufferedWriter currentWriter = null;
    private int currentFileRows = 0;
    private int fileCount = 0;
    private final int MAX_ROWS;
    
    public PackagingFileWriter(String basePath) {
        this.basePath = basePath;
        this.MAX_ROWS = Constants.MAX_PACKAGING_FILE_ROWS;
    }
    
    /**
     * Thread-safe method to write documents to packaging files
     * Multiple threads can call this method simultaneously
     * 
     * @param documents List of documents to write
     * @return Number of documents written
     * @throws IOException If file I/O error occurs
     */
    public int writeDocuments(List<DoubleDoubleDocumentDTO> documents) throws IOException {
        if (documents == null || documents.isEmpty()) {
            return 0;
        }
        
        synchronized (writeLock) { // Only one thread at a time
            
            for (DoubleDoubleDocumentDTO doc : documents) {
                // Create new file if needed
                if (currentWriter == null || currentFileRows >= MAX_ROWS) {
                    closeCurrentFile();
                    openNewFile();
                }
                
                // Write document row
                currentWriter.write(doc.toPackagingCSVLine());
                currentWriter.newLine();
                currentFileRows++;
            }
            
            // Flush to ensure data is written
            if (currentWriter != null) {
                currentWriter.flush();
            }
            
            logger.debug("Thread-{}: Wrote {} documents to packaging files", 
                        Thread.currentThread().getId(), documents.size());
            
            return documents.size();
        }
    }
    
    /**
     * Open a new packaging file
     */
    private void openNewFile() throws IOException {
        fileCount++;
        String timestamp = generateTimestamp();
        String fileName = Constants.PACKAGING_FILE_PREFIX + timestamp + 
                        Constants.PACKAGING_FILE_EXTENSION;
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        Path filePath = packagingDir.resolve(fileName);
        
        currentWriter = new BufferedWriter(new FileWriter(filePath.toFile()));
        currentFileRows = 0;
        
        // Write header
        currentWriter.write(DoubleDoubleDocumentDTO.getPackagingCSVHeader());
        currentWriter.newLine();
        
        logger.info("Created packaging file {}: {}", fileCount, fileName);
    }
    
    /**
     * Close current packaging file
     */
    private void closeCurrentFile() throws IOException {
        if (currentWriter != null) {
            currentWriter.close();
            logger.info("Closed packaging file {} with {} rows", fileCount, currentFileRows);
            currentWriter = null;
        }
    }
    
    /**
     * Close the writer and flush any remaining data
     * Must be called when done writing
     */
    public void close() throws IOException {
        synchronized (writeLock) {
            closeCurrentFile();
        }
    }
    
    /**
     * Get the number of packaging files created
     */
    public int getFileCount() {
        synchronized (writeLock) {
            return fileCount;
        }
    }
    
    /**
     * Generate timestamp with milliseconds
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
}

====================================================

package com.abcdefg.abc.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Set;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.index.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.index.connection.ConnectionManager;
import com.abcdefg.abc.index.constants.Constants;
import com.abcdefg.abc.index.model.DoubleDoubleDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int setDocCount;
    private boolean forceSingleSet;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load set configuration (with default)
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        // Load force single set configuration (with default)
        String forceSingleSetStr = config.getProperty("app.batch.force.single.set");
        this.forceSingleSet = (forceSingleSetStr != null) ? 
            Boolean.parseBoolean(forceSingleSetStr) : Constants.DEFAULT_FORCE_SINGLE_SET;
        
        logger.info("Utils initialized with setDocCount={}, forceSingleSet={}", setDocCount, forceSingleSet);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
        
        /**
         * Merge another result into this one (for multi-threaded aggregation)
         */
        public void merge(ProcessingResult other) {
            this.totalClaimNumbers += other.totalClaimNumbers;
            this.totalDocuments += other.totalDocuments;
            this.indexedSuccessCount += other.indexedSuccessCount;
            this.indexedFailureCount += other.indexedFailureCount;
            this.packagingFilesCreated += other.packagingFilesCreated;
            this.batchIDsCreated.addAll(other.batchIDsCreated);
        }
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Process each claim individually:
     *   * Query DB for documents of one claim
     *   * Apply batching/indexing logic
     *   * Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} =====", csvFileName);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return result;
            }
            
            // STEP 3: Process each claim individually
            int claimIndex = 0;
            for (String claimNumber : claimNumbers) {
                claimIndex++;
                logger.info("Processing claim {}/{}: {}", claimIndex, claimNumbers.size(), claimNumber);
                
                // Query DB for documents of this specific claim
                List<DoubleDoubleDocumentDTO> documents = fetchDocumentsForOneClaim(claimNumber);
                
                if (documents.isEmpty()) {
                    logger.warn("No documents found for claim: {} (isDataMerged=1)", claimNumber);
                    continue;
                }
                
                logger.info("Retrieved {} documents for claim: {}", documents.size(), claimNumber);
                result.totalDocuments += documents.size();
                
                // Apply batching and indexing logic (one batch per claim or multiple based on config)
                applyBatchingLogicForClaim(documents, result, claimNumber);
                
                // Update database with indexing metadata
                updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                
                logger.info("Completed claim: {} - Batches created: {}", claimNumber, 
                           result.batchIDsCreated.size() - (claimIndex - 1));
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLACE_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive, supports both camelCase and UNDERSCORE formats)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replabc("\"", "");
                    String normalizedHeader = header.toUpperCase().replabc("_", "");
                    
                    // Support both "claimNumber" and "CLAIM_NUMBER" formats
                    if (normalizedHeader.equals("CLAIMNUMBER")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' or 'CLAIM_NUMBER' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    logger.error("Supported formats: claimNumber, ClaimNumber, CLAIM_NUMBER, claim_number");
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents of a single claim with isDataMerged = 1
     * This method is called per claim to support one-batch-per-claim logic
     */
    private List<DoubleDoubleDocumentDTO> fetchDocumentsForOneClaim(String claimNumber) throws SQLException {
        List<DoubleDoubleDocumentDTO> documents = new ArrayList<>();
        
        String selectSQL = "SELECT externalID, claimNumber, claimID, gwDocumentID, " +
                          "amount, author, claimant, coverage, customerID, " +
                          "documentDescription, documentSubtype, documentTitle, documentType, " +
                          "doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, " +
                          "insuredName, mimeType, OrigDateCreated, policyNumber, " +
                          "primaryMembershipNumber, reviewed, sensitive, " +
                          "ClbSecurityController, contentFilePath, contentRetrievalName " +
                          "FROM " + dbTableName + " " +
                          "WHERE claimNumber = ? AND isDataMerged = 1 " +
                          "ORDER BY externalID";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            pstmt.setString(1, claimNumber);
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    DoubleDoubleDocumentDTO dto = new DoubleDoubleDocumentDTO();
                    
                    // Core fields
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    
                    // Packaging fields
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getString("doNotCreateActivity"));
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getString("sensitive"));
                    dto.setClbSecurityController(rs.getString("ClbSecurityController"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    documents.add(dto);
                }
            }
        }
        
        logger.debug("Fetched {} documents for claimNumber: {}", documents.size(), claimNumber);
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents for a single claim
     * 
     * Case 1: forceSingleSet = true
     *   - Create multiple batches, each with 1 set (max setDocCount docs per batch)
     *   - Example: 27 docs → Batch1(25 docs, 1 set) + Batch2(2 docs, 1 set)
     * 
     * Case 2: forceSingleSet = false
     *   - Create one batch per claim with multiple sets (max setDocCount docs per set)
     *   - Example: 27 docs → Batch1(27 docs, Set1: 25 docs + Set2: 2 docs)
     * 
     * @param documents List of documents for this claim
     * @param result ProcessingResult to track batch IDs
     * @param claimNumber Current claim number being processed
     */
    private void applyBatchingLogicForClaim(List<DoubleDoubleDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        logger.debug("Applying batching logic for claim: {} with {} documents (forceSingleSet={})", 
                    claimNumber, totalDocs, forceSingleSet);
        
        if (forceSingleSet) {
            // CASE 1: Create multiple batches, each with 1 set (max setDocCount docs per batch)
            applyForceSingleSetLogic(documents, result, claimNumber);
        } else {
            // CASE 2: Create one batch per claim with multiple sets
            applyMultiSetPerBatchLogic(documents, result, claimNumber);
        }
    }
    
    /**
     * Case 1: Force single set per batch
     * - Create multiple batches for the claim
     * - Each batch contains exactly 1 set (max setDocCount docs)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 25 docs (setID=1, setDocCount=25, batchDocCount=25)
     *   Batch 2: 2 docs (setID=1, setDocCount=2, batchDocCount=2)
     */
    private void applyForceSingleSetLogic(List<DoubleDoubleDocumentDTO> documents, 
                                          ProcessingResult result, 
                                          String claimNumber) {
        
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        int batchNumber = 1;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size (max setDocCount)
            int currentBatchSize = Math.min(setDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID();
            result.batchIDsCreated.add(batchID);
            
            logger.info("Claim: {} - Batch {}: batchID={}, docs={}, sets=1", 
                       claimNumber, batchNumber, batchID, currentBatchSize);
            
            // Process documents in current batch (single set)
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                DoubleDoubleDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata (always setID = 1 for this case)
                doc.setSetID(1);
                doc.setSetDocCount(currentBatchSize);
                doc.setSetDocIndex(setDocIndex);
                
                setDocIndex++;
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
            batchNumber++;
        }
        
        logger.info("Claim: {} - Created {} batches (forceSingleSet=true)", claimNumber, batchNumber - 1);
    }
    
    /**
     * Case 2: Multiple sets per batch
     * - Create one batch for the claim
     * - Batch contains multiple sets (max setDocCount docs per set)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 27 docs (batchDocCount=27)
     *     - Set 1: setID=1, setDocCount=25, setDocIndex=1-25
     *     - Set 2: setID=2, setDocCount=2, setDocIndex=1-2
     */
    private void applyMultiSetPerBatchLogic(List<DoubleDoubleDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        
        // Generate unique batch ID and job ID (one per claim)
        String batchID = generateBatchID();
        String jobID = generateJobID();
        result.batchIDsCreated.add(batchID);
        
        // Calculate number of sets needed
        int numberOfSets = (int) Math.ceil((double) totalDocs / setDocCount);
        
        logger.info("Claim: {} - Batch: batchID={}, docs={}, sets={}", 
                   claimNumber, batchID, totalDocs, numberOfSets);
        
        // Process all documents in this single batch
        int setID = 1;
        int setDocIndex = 1;
        int docsInCurrentSet = 0;
        
        for (DoubleDoubleDocumentDTO doc : documents) {
            docsInCurrentSet++;
            
            // Calculate set doc count for current set
            int remainingDocs = totalDocs - ((setID - 1) * setDocCount);
            int currentSetDocCount = Math.min(setDocCount, remainingDocs);
            
            // Set batch-level metadata
            doc.setBatchID(batchID);
            doc.setJobID(jobID);
            doc.setBatchDocCount(totalDocs);
            
            // Set set-level metadata
            doc.setSetID(setID);
            doc.setSetDocCount(currentSetDocCount);
            doc.setSetDocIndex(setDocIndex);
            
            // Increment set document index
            setDocIndex++;
            
            // Move to next set if current set is full
            if (setDocIndex > setDocCount) {
                logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
                setID++;
                setDocIndex = 1;
                docsInCurrentSet = 0;
            }
        }
        
        // Log last set if it has documents
        if (docsInCurrentSet > 0) {
            logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
        }
        
        logger.info("Claim: {} - Created 1 batch with {} sets (forceSingleSet=false)", claimNumber, numberOfSets);
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID() {
        return Constants.JOB_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata using JDBC batch processing
     * Falls back to individual UPDATEs if batch fails
     */
    private void updateDatabaseWithIndexing(List<DoubleDoubleDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        // Get batch size from config (default 1000)
        String batchSizeStr = config.getProperty("db.indexing.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        try {
            // Try batch UPDATE first (fast path)
            updateDatabaseWithIndexingBatch(documents, successLines, failedLines, result, batchSize);
            
        } catch (SQLException e) {
            logger.warn("Batch UPDATE failed, falling back to individual UPDATEs: {}", e.getMessage());
            // Fallback to individual UPDATEs
            updateDatabaseWithIndexingIndividual(documents, successLines, failedLines, result);
        }
    }
    
    /**
     * Batch UPDATE implementation (100x faster than individual UPDATEs)
     */
    private void updateDatabaseWithIndexingBatch(List<DoubleDoubleDocumentDTO> documents,
                                                 List<String> successLines,
                                                 List<String> failedLines,
                                                 ProcessingResult result,
                                                 int batchSize) throws SQLException {
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
                          "batchDocCount = ?, batchID = ?, jobID = ?, " +
                          "setDocCount = ?, setID = ?, SetDocIndex = ?, isIndexed = 1 " +
                          "WHERE externalID = ?";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            
            conn.setAutoCommit(false);
            int count = 0;
            int batchStartIndex = 0;
            
            for (int i = 0; i < documents.size(); i++) {
                DoubleDoubleDocumentDTO doc = documents.get(i);
                
                pstmt.setInt(1, doc.getBatchDocCount());
                pstmt.setString(2, doc.getBatchID());
                pstmt.setString(3, doc.getJobID());
                pstmt.setInt(4, doc.getSetDocCount());
                pstmt.setInt(5, doc.getSetID());
                pstmt.setInt(6, doc.getSetDocIndex());
                pstmt.setString(7, doc.getExternalID());
                pstmt.addBatch();
                count++;
                
                // Execute batch when size reached
                if (count % batchSize == 0) {
                    int[] results = pstmt.executeBatch();
                    conn.commit();
                    
                    // Track results
                    for (int j = 0; j < results.length; j++) {
                        DoubleDoubleDocumentDTO batchDoc = documents.get(batchStartIndex + j);
                        if (results[j] > 0) {
                            result.indexedSuccessCount++;
                            successLines.add(batchDoc.getExternalID());
                        } else {
                            result.indexedFailureCount++;
                            failedLines.add(batchDoc.getExternalID() + "|No record found");
                        }
                    }
                    
                    logger.debug("Executed batch of {} indexing UPDATEs", count);
                    batchStartIndex = i + 1;
                }
            }
            
            // Execute remaining batch
            if (count % batchSize != 0) {
                int[] results = pstmt.executeBatch();
                conn.commit();
                
                // Track results for remaining batch
                int remainingStart = (count / batchSize) * batchSize;
                for (int j = 0; j < results.length; j++) {
                    DoubleDoubleDocumentDTO batchDoc = documents.get(remainingStart + j);
                    if (results[j] > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(batchDoc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(batchDoc.getExternalID() + "|No record found");
                    }
                }
                
                logger.debug("Executed final batch of {} indexing UPDATEs", count % batchSize);
            }
            
            logger.info("Batch indexing UPDATE completed: {} documents indexed, {} failed", 
                       result.indexedSuccessCount, result.indexedFailureCount);
        }
    }
    
    /**
     * Individual UPDATE fallback (for when batch fails)
     */
    private void updateDatabaseWithIndexingIndividual(List<DoubleDoubleDocumentDTO> documents,
                                                      List<String> successLines,
                                                      List<String> failedLines,
                                                      ProcessingResult result) {
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
                          "batchDocCount = ?, batchID = ?, jobID = ?, " +
                          "setDocCount = ?, setID = ?, SetDocIndex = ?, isIndexed = 1 " +
                          "WHERE externalID = ?";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            
            conn.setAutoCommit(false);
            
            for (DoubleDoubleDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Individual indexing UPDATE completed (fallback)");
            
        } catch (SQLException e) {
            logger.error("Individual UPDATE fallback also failed", e);
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replabc(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        String selectSQL = "SELECT externalID, claimNumber, claimID, gwDocumentID, " +
                          "amount, author, batchDocCount, batchID, claimant, coverage, customerID, " +
                          "documentDescription, documentSubtype, documentTitle, documentType, " +
                          "doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, " +
                          "insuredName, jobID, mimeType, OrigDateCreated, policyNumber, " +
                          "primaryMembershipNumber, reviewed, sensitive, setDocCount, setID, SetDocIndex, " +
                          "ClbSecurityController, contentFilePath, contentRetrievalName " +
                          "FROM " + dbTableName + " " +
                          "WHERE batchID IN (" + inClause + ") " +
                          "AND isDataMerged = 1 AND isIndexed = 1 AND isProcessed = 0 " +
                          "ORDER BY batchID, setID, SetDocIndex";
        
        List<DoubleDoubleDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    DoubleDoubleDocumentDTO dto = new DoubleDoubleDocumentDTO();
                    
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getString("doNotCreateActivity"));
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getString("sensitive"));
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    dto.setClbSecurityController(rs.getString("ClbSecurityController"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<DoubleDoubleDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (DoubleDoubleDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(DoubleDoubleDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<DoubleDoubleDocumentDTO> documents) throws SQLException {
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
                          "isProcessed = 1, DateProcessed = ? " +
                          "WHERE externalID = ?";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            
            conn.setAutoCommit(false);
            Date currentDate = new Date();
            
            for (DoubleDoubleDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLACE_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replabc(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
}


=============================================================================================================================================================================

<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">

    <Appenders>
        <!-- Main Application Log with Rolling -->
        <RollingFile name="FileLogger" 
                     fileName="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\CCDataIndexAndPackagingUtiltity.log"
                     filePattern="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\CCDataIndexAndPackagingUtiltity-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="100MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Batch Processing Log (detailed batch operations) -->
        <RollingFile name="BatchProcessingLogger" 
                     fileName="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\BatchProcessing.log"
                     filePattern="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\BatchProcessing-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Progress Monitoring Log -->
        <RollingFile name="ProgressLogger" 
                     fileName="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\Progress.log"
                     filePattern="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\Progress-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="20MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Error-Only Log -->
        <RollingFile name="ErrorLogger" 
                     fileName="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\Errors.log"
                     filePattern="D:\Rameshwar\CCDataIndexAndPackagingUtiltity\logs\Errors-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
            <ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>

        <!-- Console Appender -->
        <Console name="ConsoleLogger" target="SYSTEM_OUT">
            <PatternLayout pattern="%d{HH:mm:ss} [%t] %-5level %logger{36} - %msg%n"/>
        </Console>
    </Appenders>

    <Loggers>
        <!-- Batch Processing Logger (Utils class) -->
        <Logger name="com.abcdefg.abc.index.utils.Utils" level="info" additivity="false">
            <AppenderRef ref="BatchProcessingLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Progress Monitoring Logger -->
        <Logger name="com.abcdefg.abc.index.utils.GlobalProcessingReport" level="info" additivity="false">
            <AppenderRef ref="ProgressLogger"/>
            <AppenderRef ref="FileLogger"/>
        </Logger>

        <!-- Memory Monitoring Logger -->
        <Logger name="com.abcdefg.abc.index.utils.MemoryMonitor" level="info" additivity="false">
            <AppenderRef ref="ProgressLogger"/>
            <AppenderRef ref="FileLogger"/>
        </Logger>

        <!-- Main Service Logger -->
        <Logger name="com.abcdefg.abc.index.CCDataIndexPackagingService" level="info" additivity="false">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Root logger -->
        <Root level="info">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Root>
    </Loggers>

</Configuration>

===========================================================================================================================================================================

# ===========================================================
# CCDataIndexAndPackagingUtility - Production Configuration
# ===========================================================
# Last Updated: 2025-10-12
# Optimized for high-volume production processing (2M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base_path=D:/Rameshwar/DoubleDoubleDataMerge/
app.skipHeaderRow=true
app.archive.after.processing=true

# ===== BATCHING CONFIGURATION =====
# Set document count: Maximum documents per set
app.set.doc.count=25

# Force single set per batch
# true = Multiple batches per claim (1 set per batch)
# false = One batch per claim (multiple sets per batch)
app.batch.force.single.set=false

# ===== PERFORMANCE TUNING =====
# Thread Pool Size: Number of parallel CSV file processors
# Recommended: 5-10 for production (multiple CSV files)
# Note: Each thread processes one complete CSV file
app.thread.pool.size=5

# Progress Monitoring: Log progress every N minutes
# Set to 0 to disable progress monitoring
# Recommended: 5 minutes for production visibility
app.progress.log.interval.minutes=5

# ===== DATABASE SETTINGS =====
db.serverName=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.dbName=RAMDB

# Database Connection Pool Size
# IMPORTANT: Should be >= app.thread.pool.size + 5 (buffer for overhead)
# Recommended: 15-20 for production with 5 threads
db.pool.size=15

# Target table for indexing and packaging operations
db.staging.claimcenterdbtable=[RAMDB].[dbo].[Wawa_Doc_Migration_Transit_Data]

# Batch UPDATE Size for Indexing: Number of rows per JDBC batch
# CRITICAL FOR PERFORMANCE: Batch UPDATEs are 100x faster than individual UPDATEs
# Recommended: 1000-2000 rows per batch
# Default: 1000
db.indexing.batch.update.size=1000

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage of max heap)
# Warning logged when memory usage exceeds this threshold
app.memory.warning.threshold=80

# ===== NOTES =====
# Performance Estimates for 80K claims, 2M documents:
# - With batch UPDATEs (1000/batch): ~40-50 minutes total
# - Thread pool of 5: Processes 5 CSV files in parallel
# - Connection pool of 15: Adequate for 5 threads + overhead
#
# JVM Recommended Settings:
# -Xms2G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# Global Processing Report:
# - Written to: {app.base_path}/CCDataIndexPackaging_Summary_Report_{timestamp}.csv
# - Contains: Total CSV files, claims, documents, batches, packaging files
# - Includes: Success/failure counts, processing time, throughput
#
# Packaging Files:
# - Written to: {app.base_path}/Packaging/wawa_migration_docs_{timestamp}.csv
# - Max rows per file: 50,000
# - Format: Pipe-delimited, 33 columns

=====================================================================================================================================================

@echo off
REM ============================================================================
REM CCDataIndexAndPackagingUtility - Batch Execution Script
REM ============================================================================
REM Simplified version with DEV/UAT and PROD profiles
REM ============================================================================

setlocal

REM ============================================================================
REM ENVIRONMENT SELECTION - CHANGE THIS FOR YOUR ENVIRONMENT
REM ============================================================================
REM Options: DEV, UAT, or PROD
REM DEV/UAT: 8 cores, 32GB RAM
REM PROD:    16 cores, 64GB RAM, 2M documents/day

set ENVIRONMENT=PROD

REM ============================================================================
REM BASIC CONFIGURATION
REM ============================================================================

set WORK_DIR=D:\Rameshwar\CCDataIndexAndPackagingUtiltity
cd /d "%WORK_DIR%"

set JAR_FILE=%WORK_DIR%\target\CCDataIndexAndPackagingUtility-0.0.1-SNAPSHOT.jar
set CONFIG_DIR=%WORK_DIR%\config
set PROPERTIES_FILE=%CONFIG_DIR%\CCDataIndexAndPackagingUtiltity.properties
set LOG4J_CONFIG=%CONFIG_DIR%\Log4j2.xml

REM ============================================================================
REM MEMORY SETTINGS BY ENVIRONMENT
REM ============================================================================

if /I "%ENVIRONMENT%"=="PROD" (
    echo Selected Environment: PRODUCTION
    echo   Server: 16 cores, 64GB RAM
    echo   Expected Volume: 2M documents/day
    echo.
    
    REM PROD: 16 cores, 64GB RAM, 2M documents
    set MIN_HEAP=6g
    set MAX_HEAP=12g
    set YOUNG_GEN=4g
    set METASPACE=256m
    set GC_THREADS=4
    set PARALLEL_THREADS=8
    
) else (
    echo Selected Environment: DEV/UAT
    echo   Server: 8 cores, 32GB RAM
    echo   Expected Volume: Test data
    echo.
    
    REM DEV/UAT: 8 cores, 32GB RAM
    set MIN_HEAP=3g
    set MAX_HEAP=6g
    set YOUNG_GEN=2g
    set METASPACE=256m
    set GC_THREADS=2
    set PARALLEL_THREADS=4
)

REM ============================================================================
REM BUILD JVM ARGUMENTS
REM ============================================================================

REM Memory Settings
set JVM_ARGS=-Xms%MIN_HEAP% -Xmx%MAX_HEAP%
set JVM_ARGS=%JVM_ARGS% -XX:NewSize=%YOUNG_GEN% -XX:MaxNewSize=%YOUNG_GEN%
set JVM_ARGS=%JVM_ARGS% -XX:MetaspabcSize=%METASPACE% -XX:MaxMetaspabcSize=%METASPACE%

REM Garbage Collection (G1GC - Best for large datasets)
set JVM_ARGS=%JVM_ARGS% -XX:+UseG1GC
set JVM_ARGS=%JVM_ARGS% -XX:MaxGCPauseMillis=200
set JVM_ARGS=%JVM_ARGS% -XX:ConcGCThreads=%GC_THREADS%
set JVM_ARGS=%JVM_ARGS% -XX:ParallelGCThreads=%PARALLEL_THREADS%

REM Performance Optimizations
set JVM_ARGS=%JVM_ARGS% -XX:+UseStringDeduplication
set JVM_ARGS=%JVM_ARGS% -XX:+UseCompressedOops
set JVM_ARGS=%JVM_ARGS% -Xss1m

REM Error Handling
set JVM_ARGS=%JVM_ARGS% -XX:+HeapDumpOnOutOfMemoryError
set JVM_ARGS=%JVM_ARGS% -XX:HeapDumpPath=%WORK_DIR%\heapdumps
set JVM_ARGS=%JVM_ARGS% -XX:+ExitOnOutOfMemoryError

REM Config Files
set JVM_ARGS=%JVM_ARGS% -Dconfig.properties.path="%PROPERTIES_FILE%"
set JVM_ARGS=%JVM_ARGS% -Dlog4j.configurationFile="%LOG4J_CONFIG%"

REM ============================================================================
REM PRE-EXECUTION CHECKS
REM ============================================================================

echo ============================================================================
echo CCDataIndexAndPackagingUtility - Starting Execution
echo ============================================================================
echo Environment: %ENVIRONMENT%
echo Start Time: %date% %time%
echo.

if not exist "%JAR_FILE%" (
    echo [ERROR] JAR file not found: %JAR_FILE%
    echo Please build the project first: mvn clean package
    pause
    exit /b 1
)

if not exist "%PROPERTIES_FILE%" (
    echo [ERROR] Properties file not found: %PROPERTIES_FILE%
    pause
    exit /b 1
)

if not exist "%LOG4J_CONFIG%" (
    echo [ERROR] Log4j config not found: %LOG4J_CONFIG%
    pause
    exit /b 1
)

echo Memory Configuration:
echo   Min Heap: %MIN_HEAP%
echo   Max Heap: %MAX_HEAP%
echo   Young Gen: %YOUNG_GEN%
echo   GC Threads: %GC_THREADS%
echo   Parallel Threads: %PARALLEL_THREADS%
echo.

echo ============================================================================
echo Executing...
echo ============================================================================
echo.

REM ============================================================================
REM EXECUTE
REM ============================================================================

java %JVM_ARGS% -jar "%JAR_FILE%"

set EXIT_CODE=%ERRORLEVEL%

REM ============================================================================
REM SUMMARY
REM ============================================================================

echo.
echo ============================================================================
echo Execution Summary
echo ============================================================================
echo End Time: %date% %time%
echo Exit Code: %EXIT_CODE%
echo.

if %EXIT_CODE% EQU 0 (
    echo [SUCCESS] CCDataIndexAndPackagingUtility completed successfully!
    echo Check logs: %WORK_DIR%\logs\
    echo Check report: %WORK_DIR%\reports\
    echo Check packaging: %WORK_DIR%\Packaging\Output\
) else (
    echo [ERROR] Failed with exit code: %EXIT_CODE%
    echo Check error log: %WORK_DIR%\logs\Errors.log
)

echo ============================================================================
pause

endlocal
exit /b %EXIT_CODE%
