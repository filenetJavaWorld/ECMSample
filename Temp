package com.wawanesa.ace.constants;

public class Constants {

    public static final String category_claimcenter     = "category_claimcenter";
    public static final String category_dcpfno1         = "category_dcpfno1";
    public static final String category_dcptoclaimcenter = "category_dcptoclaimcenter";

    public static final String metadata  = "metadata";
    public static final String documents = "documents";

    public static final String sourceFolder      = "Source";
    public static final String inprocessFolder   = "Inprocess";
    public static final String transformedFolder = "Transformed";
    public static final String completedFolder   = "Completed";
    public static final String errorFolder       = "Error";
    public static final String archiveFolder     = "Archive";
    public static final String WAWANESA     = "WAWANESA";
    
    public static final String FILENET = "filenet";
    public static final String OPENTEXT = "opentext";
    public static final String FILENET_NUMARIC_VALUE = "10001";
    public static final String OPENTEXT_NUMARIC_VALUE = "10002";
    
}


package com.wawanesa.ace.model;

import java.sql.Date;

public class ClaimDocumentDTO {

    private String amount;
    private String author;
    private String batchDocCount;
    private String batchID;
    private String claimant;
    private long claimID;
    private String claimNumber;
    private String coverage;
    private String customerID;
    private String documentDescription;
    private String documentSubtype;
    private String documentTitle;
    private String documentType;
    private String doNotCreateActivity;
    private String duplicate;
    private String exposureID;
    private String gwDocumentID;
    private String hidden;
    private String inputMethod;
    private String insuredName;
    private String jobID;
    private String mimeType;
    private String OrigDateCreated;
    private String policyNumber;
    private String primaryMembershipNumber;
    private String reviewed;
    private boolean sensitive;
    private String setDocCount;
    private String setID;
    private String status;
    private String setDocIndex;
    
    private String contentFilePath;
    private String contentRetrievalName;
    private String contentType;
    
    private String claimType;  //Open claim /closed Claim
    private String originatingrepo_ext;
    private String doc_uid;
    private String externalID;
    private Date DateProcessed;
    private String comments;
    private String category_type;// Store any 3 types of category type value
    private String file_creation_year;
    private String file_id;
    private String tar_file_name;  // tar file name
    private String csv_file_Name;  // csv file name
    
	public String getAmount() {
		return amount;
	}
	public void setAmount(String amount) {
		this.amount = amount;
	}
	public String getAuthor() {
		return author;
	}
	public void setAuthor(String author) {
		this.author = author;
	}
	public String getBatchDocCount() {
		return batchDocCount;
	}
	public void setBatchDocCount(String batchDocCount) {
		this.batchDocCount = batchDocCount;
	}
	public String getBatchID() {
		return batchID;
	}
	public void setBatchID(String batchID) {
		this.batchID = batchID;
	}
	public String getClaimant() {
		return claimant;
	}
	public void setClaimant(String claimant) {
		this.claimant = claimant;
	}
	public long getClaimID() {
		return claimID;
	}
	public void setClaimID(long claimID) {
		this.claimID = claimID;
	}
	public String getClaimNumber() {
		return claimNumber;
	}
	public void setClaimNumber(String claimNumber) {
		this.claimNumber = claimNumber;
	}
	public String getCoverage() {
		return coverage;
	}
	public void setCoverage(String coverage) {
		this.coverage = coverage;
	}
	public String getCustomerID() {
		return customerID;
	}
	public void setCustomerID(String customerID) {
		this.customerID = customerID;
	}
	public String getDocumentDescription() {
		return documentDescription;
	}
	public void setDocumentDescription(String documentDescription) {
		this.documentDescription = documentDescription;
	}
	public String getDocumentSubtype() {
		return documentSubtype;
	}
	public void setDocumentSubtype(String documentSubtype) {
		this.documentSubtype = documentSubtype;
	}
	public String getDocumentTitle() {
		return documentTitle;
	}
	public void setDocumentTitle(String documentTitle) {
		this.documentTitle = documentTitle;
	}
	public String getDocumentType() {
		return documentType;
	}
	public void setDocumentType(String documentType) {
		this.documentType = documentType;
	}
	public String getDoNotCreateActivity() {
		return doNotCreateActivity;
	}
	public void setDoNotCreateActivity(String doNotCreateActivity) {
		this.doNotCreateActivity = doNotCreateActivity;
	}
	public String getDuplicate() {
		return duplicate;
	}
	public void setDuplicate(String duplicate) {
		this.duplicate = duplicate;
	}
	public String getExposureID() {
		return exposureID;
	}
	public void setExposureID(String exposureID) {
		this.exposureID = exposureID;
	}
	public String getGwDocumentID() {
		return gwDocumentID;
	}
	public void setGwDocumentID(String gwDocumentID) {
		this.gwDocumentID = gwDocumentID;
	}
	public String getHidden() {
		return hidden;
	}
	public void setHidden(String hidden) {
		this.hidden = hidden;
	}
	public String getInputMethod() {
		return inputMethod;
	}
	public void setInputMethod(String inputMethod) {
		this.inputMethod = inputMethod;
	}
	public String getInsuredName() {
		return insuredName;
	}
	public void setInsuredName(String insuredName) {
		this.insuredName = insuredName;
	}
	public String getJobID() {
		return jobID;
	}
	public void setJobID(String jobID) {
		this.jobID = jobID;
	}
	public String getMimeType() {
		return mimeType;
	}
	public void setMimeType(String mimeType) {
		this.mimeType = mimeType;
	}
	public String getOrigDateCreated() {
		return OrigDateCreated;
	}
	public void setOrigDateCreated(String origDateCreated) {
		OrigDateCreated = origDateCreated;
	}
	public String getPolicyNumber() {
		return policyNumber;
	}
	public void setPolicyNumber(String policyNumber) {
		this.policyNumber = policyNumber;
	}
	public String getPrimaryMembershipNumber() {
		return primaryMembershipNumber;
	}
	public void setPrimaryMembershipNumber(String primaryMembershipNumber) {
		this.primaryMembershipNumber = primaryMembershipNumber;
	}
	public String getReviewed() {
		return reviewed;
	}
	public void setReviewed(String reviewed) {
		this.reviewed = reviewed;
	}
	public boolean isSensitive() {
		return sensitive;
	}
	public void setSensitive(boolean sensitive) {
		this.sensitive = sensitive;
	}
	public String getSetDocCount() {
		return setDocCount;
	}
	public void setSetDocCount(String setDocCount) {
		this.setDocCount = setDocCount;
	}
	public String getSetID() {
		return setID;
	}
	public void setSetID(String setID) {
		this.setID = setID;
	}
	public String getStatus() {
		return status;
	}
	public void setStatus(String status) {
		this.status = status;
	}
	public String getSetDocIndex() {
		return setDocIndex;
	}
	public void setSetDocIndex(String setDocIndex) {
		this.setDocIndex = setDocIndex;
	}
	public String getContentFilePath() {
		return contentFilePath;
	}
	public void setContentFilePath(String contentFilePath) {
		this.contentFilePath = contentFilePath;
	}
	public String getContentRetrievalName() {
		return contentRetrievalName;
	}
	public void setContentRetrievalName(String contentRetrievalName) {
		this.contentRetrievalName = contentRetrievalName;
	}
	public String getContentType() {
		return contentType;
	}
	public void setContentType(String contentType) {
		this.contentType = contentType;
	}
	public String getClaimType() {
		return claimType;
	}
	public void setClaimType(String claimType) {
		this.claimType = claimType;
	}
	public String getOriginatingrepo_ext() {
		return originatingrepo_ext;
	}
	public void setOriginatingrepo_ext(String originatingrepo_ext) {
		this.originatingrepo_ext = originatingrepo_ext;
	}
	public String getDoc_uid() {
		return doc_uid;
	}
	public void setDoc_uid(String doc_uid) {
		this.doc_uid = doc_uid;
	}
	public String getExternalID() {
		return externalID;
	}
	public void setExternalID(String externalID) {
		this.externalID = externalID;
	}
	public Date getDateProcessed() {
		return DateProcessed;
	}
	public void setDateProcessed(Date dateProcessed) {
		DateProcessed = dateProcessed;
	}
	public String getComments() {
		return comments;
	}
	public void setComments(String comments) {
		this.comments = comments;
	}
	public String getCategory_type() {
		return category_type;
	}
	public void setCategory_type(String category_type) {
		this.category_type = category_type;
	}
	public String getFile_creation_year() {
		return file_creation_year;
	}
	public void setFile_creation_year(String file_creation_year) {
		this.file_creation_year = file_creation_year;
	}
	public String getFile_id() {
		return file_id;
	}
	public void setFile_id(String file_id) {
		this.file_id = file_id;
	}
	public String getTar_file_name() {
		return tar_file_name;
	}
	public void setTar_file_name(String tar_file_name) {
		this.tar_file_name = tar_file_name;
	}
	public String getCsv_file_Name() {
		return csv_file_Name;
	}
	public void setCsv_file_Name(String csv_file_Name) {
		this.csv_file_Name = csv_file_Name;
	}
    
   
}

package com.wawanesa.ace.utils;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.lang.reflect.Field;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;

import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.json.JSONObject;
import com.wawanesa.ace.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.connection.ConnectionManager;
import com.wawanesa.ace.constants.Constants;
import com.wawanesa.ace.model.ClaimDocumentDTO;

public class Utils {

    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private String base_path;
    private PropertiesConfigLoader config;
    private String insertClaimCenterQuery;

    public Utils(String base_path) {
        this.config = PropertiesConfigLoader.getInstance();
        this.base_path = base_path;
        this.insertClaimCenterQuery = config.getProperty("db.query.insert.claimcenter");
    }

    private boolean isFileExistAndValid(String filePath) {
        try {
            File file = new File(filePath);
            if (file.exists()) {
                return true;
            } else {
                logger.warn("Expected file does not exist: {}", filePath);
                return false;
            }
        } catch (Exception e) {
            logger.error("Error checking file existence: {}", filePath, e);
            return false;
        }
    }

    public static String getFileNameWithoutExtension(String fileName) {
        if (fileName == null || fileName.isEmpty()) {
            return fileName;
        }

        int dotIndex = fileName.lastIndexOf('.');
        if (dotIndex > 0 && dotIndex < fileName.length() - 1) {
            return fileName.substring(0, dotIndex);
        }

        return fileName;
    }

    public ProcessingResult processCategoryFiles(Path metadata_file, String category_type, ConnectionManager connectionManager) {
        String metadata_file_path = null;
        String tar_file_path = null;
        ProcessingResult result = new ProcessingResult(); // Initialize result
        
        try {
            metadata_file_path = metadata_file.getParent() + File.separator + metadata_file.getFileName().toString();
            String changed_tar_file = metadata_file.getFileName().toString().replace(Constants.metadata, Constants.documents);
            tar_file_path = metadata_file.getParent() + File.separator + getFileNameWithoutExtension(changed_tar_file) + ".tar";
            
            logger.info("================================================================================");
            logger.info("Processing File Pair:");
            logger.info("  CSV: {}", metadata_file_path);
            logger.info("  TAR: {}", tar_file_path);
            logger.info("================================================================================");
            
            // Validate both files exist
            if (!isFileExistAndValid(metadata_file_path) || !isFileExistAndValid(tar_file_path)) {
                logger.error("File pair incomplete - skipping processing");
                result.failureCount = 1; // Mark file as failed
                return result;
            }
            
            // STEP 1: Extract TAR file (critical - must succeed before CSV processing)
            logger.info("[STEP 1] Extracting TAR file...");
            String tarExtractionPath = getExtractOfTarFile(tar_file_path, category_type);
            
            if (tarExtractionPath == null) {
                // TAR extraction failed - move both files to Error folder
                logger.error("TAR extraction failed - moving files to Error folder");
                moveFilesToError(tar_file_path, metadata_file_path, category_type, 
                    "TAR extraction failed - cannot process without content files");
                result.failureCount = 1; // Mark file as failed
                return result;
            }
            
            logger.info("[SUCCESS] TAR extracted successfully to: {}", tarExtractionPath);
            
            // STEP 2: Parse and transform CSV data (row-by-row with success/failure tracking)
            logger.info("[STEP 2] Processing CSV data...");
            
            // Extract TAR filename for database tracking
            File tarFile = new File(tar_file_path);
            String tarFileName = tarFile.getName();
            
            try (Connection conn = connectionManager.getConnection()) {
                result = parseAndTransformDataWithTracking(metadata_file, category_type, conn, tarFileName, tarExtractionPath);
                logger.info("[RESULT] Processed: {} rows", result.totalRows);
                logger.info("         Success: {}", result.successCount);
                logger.info("         Failed: {}", result.failureCount);
            }
            
            // STEP 3: Archive or delete source files after successful processing
            if (result.totalRows > 0) {
                logger.info("[STEP 3] Managing source files...");
                handleSourceFilesAfterProcessing(tar_file_path, metadata_file_path, category_type);
            }
            
            logger.info("[COMPLETE] File pair processing finished");
            logger.info("================================================================================");
            return result; // Return actual processing result
            
        } catch (Exception e) {
            logger.error("Unexpected error processing file pair", e);
            
            // On fatal error, try to move files to Error folder
            if (tar_file_path != null && metadata_file_path != null) {
                try {
                    moveFilesToError(tar_file_path, metadata_file_path, category_type, 
                        "Fatal error: " + e.getMessage());
                } catch (Exception moveError) {
                    logger.error("Failed to move files to Error folder: {}", moveError.getMessage(), moveError);
                }
            }
            result.failureCount = 1; // Mark file as failed
            return result; // Return failure result
        }
    }

    /**
     * Extracts TAR file and returns the extraction path
     * @param sourceTarPath Path to the TAR file
     * @param category_type Category type (e.g., category_claimcenter)
     * @return Full path where TAR was extracted, or null if extraction failed
     */
    private String getExtractOfTarFile(String sourceTarPath, String category_type) {
        String tempDirPath = base_path + Constants.inprocessFolder + File.separator + category_type;
        String targetDirPath = base_path + Constants.transformedFolder + File.separator + category_type;

        logger.debug("Temp extraction path: {}", tempDirPath);
        logger.debug("Target path: {}", targetDirPath);
        
        File tarFile = new File(sourceTarPath);
        if (!tarFile.exists()) {
            logger.error("TAR file not found: {}", sourceTarPath);
            return null;
        }

        String tarFileName = tarFile.getName().replaceFirst("[.][^.]+$", "");
        Path tempExtractDir = Paths.get(tempDirPath, tarFileName);
        Path targetDir = Paths.get(targetDirPath, tarFileName);

        try {
            Files.createDirectories(tempExtractDir);
            extractTar(tarFile, tempExtractDir);
            
            // Flatten if TAR contains a single root folder with the same name
            flattenSingleRootFolder(tempExtractDir);
            
            if (Files.exists(targetDir)) {
                deleteDirectory(targetDir);
            }
            Files.move(tempExtractDir, targetDir, StandardCopyOption.REPLACE_EXISTING);
            logger.info("TAR extraction and move completed successfully");
            logger.info("TAR extracted to: {}", targetDir.toString());
            
            // Return the absolute extraction path (with forward slashes for consistency)
            return targetDir.toString().replace('\\', '/');
        } catch (IOException e) {
            logger.error("Error during TAR extraction or move: {}", e.getMessage(), e);
            return null;
        }
    }
    
    /**
     * Flattens extraction if TAR contains a single root folder
     * Example: If extracted/claim_docs/claim_docs/files exists,
     * moves contents up: extracted/claim_docs/files
     */
    private void flattenSingleRootFolder(Path extractedDir) throws IOException {
        File[] contents = extractedDir.toFile().listFiles();
        
        // Check if there's exactly one item and it's a directory
        if (contents != null && contents.length == 1 && contents[0].isDirectory()) {
            File singleFolder = contents[0];
            
            logger.debug("Found single root folder: {}", singleFolder.getName());
            
            // Move all contents of the single folder up one level
            File[] innerContents = singleFolder.listFiles();
            if (innerContents != null) {
                for (File file : innerContents) {
                    Path targetPath = extractedDir.resolve(file.getName());
                    Files.move(file.toPath(), targetPath, StandardCopyOption.REPLACE_EXISTING);
                }
                
                // Delete the now-empty single folder
                if (singleFolder.delete()) {
                    logger.info("Flattened single root folder structure");
                }
            }
        }
    }

    public static void extractTar(File tarFile, Path outputDir) throws IOException {
        try (FileInputStream fis = new FileInputStream(tarFile);
             BufferedInputStream bis = new BufferedInputStream(fis);
             TarArchiveInputStream tarInput = new TarArchiveInputStream(bis)) {

            TarArchiveEntry entry;
            while ((entry = tarInput.getNextTarEntry()) != null) {
                Path outputPath = outputDir.resolve(entry.getName()).normalize();

                // Prevent Zip Slip vulnerability
                if (!outputPath.startsWith(outputDir)) {
                    throw new IOException("Bad entry: " + entry.getName());
                }

                if (entry.isDirectory()) {
                    Files.createDirectories(outputPath);
                } else {
                    Files.createDirectories(outputPath.getParent());
                    try (OutputStream out = Files.newOutputStream(outputPath)) {
                        byte[] buffer = new byte[4096];
                        int len;
                        while ((len = tarInput.read(buffer)) != -1) {
                            out.write(buffer, 0, len);
                        }
                    }
                }
            }
        }
    }

    public static void deleteDirectory(Path path) throws IOException {
        if (Files.exists(path)) {
            Files.walk(path)
                 .sorted(Comparator.reverseOrder())
                 .map(Path::toFile)
                 .forEach(File::delete);
        }
    }


    /** Safer common loader (uses optString; won't throw if a key is missing) */
    private void loadCommonPropertiesSafe(ClaimDocumentDTO dto, JSONObject root, JSONObject meta) {
        dto.setDocumentTitle(meta.optString("document_name", ""));
        dto.setDocumentType(meta.optString("document_type", ""));
        dto.setAuthor(meta.optString("author", ""));
        dto.setOrigDateCreated(meta.optString("create_ts", ""));
        dto.setMimeType(root.optString("mime_type", ""));
        dto.setClaimNumber(meta.optString("claim_num", ""));
        dto.setGwDocumentID(root.optString("external_alias", ""));
    }

    public static Boolean writeDataToCSVFile(Object obj, BufferedWriter writer, boolean writeHeader) throws IOException {
        if (obj == null) return false;

        Class<?> clazz = obj.getClass();
        Field[] fields = clazz.getDeclaredFields();

        try {
            if (writeHeader) {
                List<String> headers = new ArrayList<>();
                for (Field field : fields) {
                    headers.add(field.getName());
                }
                writer.write(String.join(",", headers));
                writer.newLine();
            }

            List<String> values = new ArrayList<>();
            for (Field field : fields) {
                field.setAccessible(true);
                Object value = field.get(obj);
                values.add(value != null ? value.toString() : "");
            }
            writer.write(String.join(",", values));
            writer.newLine();
            return true;
        } catch (IllegalAccessException e) {
            throw new IOException("Failed to access field values", e);
        }
    }
    
    /**
     * New method: Parses CSV and tracks success/failure per row
     * Writes to separate success and failed CSV files
     * @param metadata_file Path to the CSV metadata file
     * @param category_type Category type (e.g., category_claimcenter)
     * @param conn Database connection
     * @param tarFileName Name of the TAR file (for tracking)
     * @param tarExtractionPath Full path where TAR was extracted (for building complete content paths)
     */
    private ProcessingResult parseAndTransformDataWithTracking(Path metadata_file, String category_type, Connection conn, String tarFileName, String tarExtractionPath)
            throws FileNotFoundException, IOException {
        
        ProcessingResult result = new ProcessingResult();
        BufferedReader br = null;
        BufferedWriter successWriter = null;
        BufferedWriter failedWriter = null;
        
        String csvFileName = metadata_file.getFileName().toString();
        String baseFileName = getFileNameWithoutExtension(csvFileName);
        
        // Success and Failed file paths
        String completedDirPath = base_path + Constants.completedFolder + File.separator + category_type;
        String errorDirPath = base_path + Constants.errorFolder + File.separator + category_type;
        String successFilePath = completedDirPath + File.separator + baseFileName + "_success.csv";
        String failedFilePath = errorDirPath + File.separator + baseFileName + "_failed.csv";
        
        // Buffer for failed rows (only create file if there are failures)
        List<String> failedRows = new ArrayList<>();
        String headerLine = null;
        
        try {
            // Create directories
            Files.createDirectories(Paths.get(completedDirPath));
            Files.createDirectories(Paths.get(errorDirPath));
            
            // Open source CSV
            br = new BufferedReader(
                    new InputStreamReader(
                    new FileInputStream(metadata_file.getParent() + File.separator + csvFileName),
                    StandardCharsets.UTF_8));

            // Open success writer upfront (we expect most rows to succeed)
            successWriter = new BufferedWriter(
                    new java.io.OutputStreamWriter(
                    new java.io.FileOutputStream(successFilePath),
                    StandardCharsets.UTF_8));

            // Read header
            headerLine = br.readLine();
            if (headerLine == null) {
                logger.warn("Empty CSV file: {}", metadata_file);
                return result;
            }
            
            // Write header to success file
            successWriter.write(headerLine);
            successWriter.newLine();
            
            java.util.List<String> header = parseCsvRow(headerLine);
            java.util.Map<String, Integer> hidx = indexByName(header);

            // Get column indices
            Integer metaIdx = hidx.get("metadata_json");
            Integer catIdx  = hidx.get("category");
            Integer mimeIdx = hidx.get("mimetype");
            Integer pathIdx = hidx.get("doc_filepath");
            Integer origIdx = hidx.get("orig_rep");
            Integer dUidIdx = hidx.get("doc_uid");
            Integer tsIdx   = hidx.get("file_creation_ts");
            Integer yrIdx   = hidx.get("file_creation_year");
            Integer fileIdIdx = hidx.get("wdr_id");  // Check for wdr_id column (primary)
            if (fileIdIdx == null) fileIdIdx = hidx.get("wcir_id");  // Fallback to wcir_id
            if (fileIdIdx == null) fileIdIdx = hidx.get("file_id");  // Fallback to file_id

            if (metaIdx == null) {
                logger.error("Required column 'metadata_json' not found in CSV: {}", csvFileName);
                return result;
            }

            // Use passed TAR filename instead of searching for it
            logger.debug("Using TAR filename: {}", tarFileName);
            
            // Load batch size from configuration
            String batchSizeStr = config.getProperty("db.batch.insert.size");
            int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
            logger.info("Using batch insert size: {}", batchSize);
            
            // Buffers for batch processing
            List<ClaimDocumentDTO> batchDTOs = new ArrayList<>(batchSize);
            List<String> batchLines = new ArrayList<>(batchSize);

            String line = null;
            String currentLine = null;  // Captured line for error handling
            int rowNum = 1; // header is row 0
            int skippedEmptyLines = 0;

            while ((line = br.readLine()) != null) {
                rowNum++;

                // Silently skip completely empty lines (user-added blank rows at end of file, etc.)
                // These are NOT errors - just ignore them completely
                if (line.trim().length() == 0) {
                    skippedEmptyLines++;
                    continue;  // Don't count, don't log, just skip
                }
                
                // Capture current line for error handling (in case exception corrupts it)
                currentLine = line;
                result.totalRows++;
                
                if (logger.isDebugEnabled()) {
                    logger.debug("Row {} content: {}", rowNum, 
                        line.length() > 100 ? line.substring(0, 100) + "..." : line);
                }
                
                try {
                    // Parse CSV row
                    java.util.List<String> cells = parseCsvRow(currentLine);
                    
                    // Validate we have cells
                    if (cells == null || cells.isEmpty()) {
                        throw new Exception("Row is empty or cannot be parsed");
                    }
                    
                    // Validate metadata_json exists
                    String cell_metadata = getCell(cells, metaIdx);
                    if (cell_metadata == null || cell_metadata.trim().isEmpty()) {
                        throw new Exception("metadata_json column is empty or missing");
                    }
                    
                    // Parse JSON
                    JSONObject jsonObject = parseJsonLenient(cell_metadata);
                if (jsonObject == null) {
                    String unwrapped = unwrapOnce(cell_metadata);
                    jsonObject = parseJsonLenient(unwrapped);
                }
                if (jsonObject == null) {
                        throw new Exception("Invalid JSON in metadata_json - cannot parse");
                }

                    JSONObject metadataJson = jsonObject.optJSONObject("metadata");
                if (metadataJson == null) metadataJson = tryFindMetadataObject(jsonObject);
                if (metadataJson == null) {
                        throw new Exception("No 'metadata' object found in JSON");
                    }
                    
                    // Build DTO
                    ClaimDocumentDTO dto = new ClaimDocumentDTO();
                    loadCommonPropertiesSafe(dto, jsonObject, metadataJson);
                    
                // Try to get file_id from multiple possible locations (CSV column takes priority)
                String fileId = "";
                
                // First, try CSV column (wdr_id, wcir_id, or file_id)
                if (fileIdIdx != null) {
                    fileId = safeTrim(getCell(cells, fileIdIdx));
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from CSV column)", rowNum, fileId);
                    }
                }
                
                // If not in CSV, try JSON locations (note: file_id is numeric in JSON)
                if (fileId.isEmpty()) {
                    long numericId = metadataJson.optLong("file_id", 0L);
                    if (numericId > 0) {
                        fileId = String.valueOf(numericId);
                        logger.debug("Row {}: file_id = {} (from metadata_json.metadata.file_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    long numericId = jsonObject.optLong("file_id", 0L);
                    if (numericId > 0) {
                        fileId = String.valueOf(numericId);
                        logger.debug("Row {}: file_id = {} (from metadata_json.file_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    // Fallback to string-based lookups for wcir_id
                    fileId = metadataJson.optString("wcir_id", "");
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from metadata_json.metadata.wcir_id)", rowNum, fileId);
                    }
                }
                if (fileId.isEmpty()) {
                    fileId = jsonObject.optString("wcir_id", "");
                    if (!fileId.isEmpty()) {
                        logger.debug("Row {}: file_id = {} (from metadata_json.wcir_id)", rowNum, fileId);
                    }
                }
                
                dto.setFile_id(fileId);
                
                if (fileId.isEmpty()) {
                    logger.warn("Row {}: file_id not found in any expected location", rowNum);
                }
                    
                    // Set category type
                    String categoryType = safeTrim(getCell(cells, catIdx));
                    if (catIdx != null) dto.setCategory_type(categoryType);
                    
                    // Handle sensitive field based on category type
                    if (Constants.category_dcptoclaimcenter.equalsIgnoreCase(categoryType)) {
                        // Extract 'sensitive' from metadata JSON for this specific category
                        String sensitiveValue = metadataJson.optString("sensitive", "False");
                        // Normalize to proper case: "True" or "False"
                        if ("true".equalsIgnoreCase(sensitiveValue) || "1".equals(sensitiveValue)) {
                            dto.setSensitive(true);
                        } else {
                            dto.setSensitive(false);
                        }
                    } else {
                        // Default to False for all other category types
                        dto.setSensitive(false);
                    }
                    
                    // doc_filepath -> Build complete path by merging TAR extraction path + CSV relative path
                    String csvDocFilepath = safeTrim(getCell(cells, pathIdx));
                    String normalizedCsvPath = (csvDocFilepath == null) ? "" : csvDocFilepath.replace('\\', '/');
                    
                    // Extract just the filename for ContentRetrivalName
                    int lastSlash = normalizedCsvPath.lastIndexOf('/');
                    String fileName = (lastSlash >= 0) ? normalizedCsvPath.substring(lastSlash + 1) : normalizedCsvPath;
                    dto.setContentRetrievalName(fileName);
                    
                    // Build complete content path: TAR extraction folder + CSV relative path
                    // This allows users to copy-paste the path directly into Explorer to access the file
                    String completeContentPath = "";
                    if (normalizedCsvPath != null && !normalizedCsvPath.isEmpty()) {
                        // Remove leading slash if present (to avoid double slashes)
                        String cleanCsvPath = normalizedCsvPath.startsWith("/") ? normalizedCsvPath.substring(1) : normalizedCsvPath;
                        completeContentPath = tarExtractionPath + "/" + cleanCsvPath;
                    } else {
                        // Fallback: if doc_filepath is empty, use just the filename in extraction folder
                        completeContentPath = tarExtractionPath + "/" + fileName;
                    }
                    dto.setContentFilePath(completeContentPath);
                    
                    if (logger.isDebugEnabled()) {
                        logger.debug("Row {}: CSV doc_filepath='{}' -> Complete path='{}'", 
                                    rowNum, csvDocFilepath, completeContentPath);
                    }

                    if (mimeIdx != null) dto.setContentType(safeTrim(getCell(cells, mimeIdx)));

                    dto.setOrigDateCreated(safeTrim(getCell(cells, tsIdx)));
                    dto.setFile_creation_year(safeTrim(getCell(cells, yrIdx)));
                String origRep = safeTrim(getCell(cells, origIdx));
                String docUid  = safeTrim(getCell(cells, dUidIdx));
                    dto.setOriginatingrepo_ext(origRep);
                    dto.setDoc_uid(docUid);

                String extId = buildExternalID(origRep, docUid);
                    dto.setExternalID(extId);
                    
                    //truncate Claimnumber to 9 char
                    dto.setClaimNumber(truncateClaimNumber(dto.getClaimNumber()));
                    dto.setCsv_file_Name(csvFileName);
                    dto.setTar_file_name(tarFileName);
                    
                    //set default values for below
                    dto.setInputMethod(Constants.WAWANESA);
                    dto.setDoNotCreateActivity("true");
                    
                    // Add to batch buffers (instead of immediate insert)
                    batchDTOs.add(dto);
                    batchLines.add(currentLine);
                    
                    // When batch is full, process it
                    if (batchDTOs.size() >= batchSize) {
                        processBatchWithFallback(batchDTOs, batchLines, conn, batchSize,
                                                 successWriter, failedRows, result);
                        
                        // Clear buffers for next batch
                        batchDTOs.clear();
                        batchLines.clear();
                        
                        if (logger.isDebugEnabled()) {
                            logger.debug("Processed batch of {} rows, total success: {}, total failed: {}", 
                                        batchSize, result.successCount, result.failureCount);
                        }
                    }
                    
                } catch (Exception e) {
                    // FAILURE: Buffer failed row (will write to file at end if any failures)
                    String errorMessage = compact(e.getMessage());
                    
                    // Use currentLine (captured at start) to ensure we have the original row
                    // Even if 'line' variable gets modified during processing
                    String safeCurrentLine = (currentLine != null) ? currentLine : "";
                    
                    if (safeCurrentLine.isEmpty()) {
                        logger.error("Row {}: currentLine is empty! Original line was: '{}'", rowNum, line);
                    }
                    
                    // Store: original row + comma + escaped error message
                    String failedRow = safeCurrentLine + ",\"" + errorMessage.replace("\"", "\"\"") + "\"";
                    failedRows.add(failedRow);
                    
                    result.failureCount++;
                    logger.warn("Row {} failed: {}", rowNum, errorMessage);
                    
                    // Continue to next row (exception is caught, loop continues)
                }
            }
            
            // Process remaining batch (if any rows left)
            if (!batchDTOs.isEmpty()) {
                logger.info("Processing final batch of {} rows", batchDTOs.size());
                processBatchWithFallback(batchDTOs, batchLines, conn, batchSize,
                                         successWriter, failedRows, result);
            }
            
            // Write failed rows to file ONLY if there are failures
            if (!failedRows.isEmpty()) {
                logger.info("Creating failed file with {} rows", failedRows.size());
                failedWriter = new BufferedWriter(
                    new java.io.OutputStreamWriter(
                        new java.io.FileOutputStream(failedFilePath),
                        StandardCharsets.UTF_8));
                
                // Write header with error_message column
                failedWriter.write(headerLine + ",error_message");
                failedWriter.newLine();
                
                // Write all failed rows
                for (String failedRow : failedRows) {
                    failedWriter.write(failedRow);
                    failedWriter.newLine();
                }
                
                logger.info("Failed file created: {}", failedFilePath);
            } else {
                logger.info("All rows processed successfully - no failed file created");
            }
            
            logger.info("Success file created: {}", successFilePath);
            
            // Report skipped empty lines if any (informational only)
            if (skippedEmptyLines > 0) {
                logger.info("Skipped {} empty line(s) in CSV (blank rows - not errors)", skippedEmptyLines);
                result.skippedEmptyLines = skippedEmptyLines; // Store in result
            }
            
        } catch (Exception e) {
            logger.error("Error processing CSV: {}", csvFileName, e);
        } finally {
            try {
                if (successWriter != null) successWriter.close();
                if (failedWriter != null) failedWriter.close();
                if (br != null) br.close();
            } catch (Exception e2) {
                e2.printStackTrace();
            }
        }
        
        return result;
    }
    
 private String truncateClaimNumber(String claimNumber) {
		if(claimNumber != null && claimNumber.trim().length()>9) {
			claimNumber = claimNumber.trim();
			claimNumber = claimNumber.substring(claimNumber.length()-9);
		}
		return claimNumber;
	}

	// Inserts only the columns approved for ClaimCenter Phase-1 + DateProcessed
    private void insertClaimCenterRow(ClaimDocumentDTO dto, Connection conn) throws java.sql.SQLException {
        try (java.sql.PreparedStatement ps = conn.prepareStatement(insertClaimCenterQuery)) {

            conn.setAutoCommit(true); // single-row inserts

            int i = 1;
            ps.setString(i++, nz(dto.getOriginatingrepo_ext()));   // originatingrepo_ext
            ps.setString(i++, nz(dto.getDoc_uid()));               // doc_uid
            ps.setString(i++, nz(dto.getExternalID()));            // externalID
            ps.setString(i++, nz(dto.getCategory_type()));         // category_type

            ps.setString(i++, nz(dto.getContentFilePath()));       // contentFilePath
            ps.setString(i++, nz(dto.getContentRetrievalName()));   // contentRetrievalName (DB has the 'e')
            ps.setString(i++, nz(dto.getContentType()));           // contentType

            ps.setString(i++, nz(dto.getOrigDateCreated()));       // OrigDateCreated
            ps.setString(i++, nz(dto.getFile_creation_year()));    // file_creation_year

            ps.setString(i++, nz(dto.getFile_id()));               // file_id
            ps.setString(i++, nz(dto.getClaimNumber()));           // claimNumber
            ps.setString(i++, nz(dto.getDocumentTitle()));         // documentTitle
            ps.setString(i++, nz(dto.getMimeType()));              // mimeType (from JSON)

            ps.setString(i++, nz(dto.getDoNotCreateActivity()));         // doNotCreateActivity
            ps.setString(i++, nz(dto.getInputMethod()));              // inputMethod

            ps.setString(i++, nz(dto.getCsv_file_Name()));         // csv_file_Name
            ps.setString(i++, nz(dto.getTar_file_name()));         // tar_file_name
            
            ps.setBoolean(i++, dto.isSensitive());                     // sensitive (BIT column)

            ps.executeUpdate();
        }
    }
    
    /**
     * Insert multiple rows using JDBC batch insert for high performance
     * This method provides ~100x speedup compared to single-row inserts
     * 
     * @param dtoList List of DTOs to insert
     * @param conn Database connection
     * @param batchSize Number of rows per batch (e.g., 1000)
     * @return Number of rows successfully inserted
     * @throws SQLException if batch insert fails
     */
    private int insertClaimCenterRowsBatch(List<ClaimDocumentDTO> dtoList, Connection conn, int batchSize) 
            throws SQLException {
        
        if (dtoList == null || dtoList.isEmpty()) {
            return 0;
        }
        
        conn.setAutoCommit(false); // Enable batch mode
        int totalInserted = 0;
        
        try (PreparedStatement ps = conn.prepareStatement(insertClaimCenterQuery)) {
            int currentBatchSize = 0;
            
            for (ClaimDocumentDTO dto : dtoList) {
                // Set all 18 parameters
                int i = 1;
                ps.setString(i++, nz(dto.getOriginatingrepo_ext()));   // 1
                ps.setString(i++, nz(dto.getDoc_uid()));               // 2
                ps.setString(i++, nz(dto.getExternalID()));            // 3
                ps.setString(i++, nz(dto.getCategory_type()));         // 4
                ps.setString(i++, nz(dto.getContentFilePath()));       // 5
                ps.setString(i++, nz(dto.getContentRetrievalName()));   // 6
                ps.setString(i++, nz(dto.getContentType()));           // 7
                ps.setString(i++, nz(dto.getOrigDateCreated()));       // 8
                ps.setString(i++, nz(dto.getFile_creation_year()));    // 9
                ps.setString(i++, nz(dto.getFile_id()));               // 10
                ps.setString(i++, nz(dto.getClaimNumber()));           // 11
                ps.setString(i++, nz(dto.getDocumentTitle()));         // 12
                ps.setString(i++, nz(dto.getMimeType()));              // 13
                ps.setString(i++, nz(dto.getDoNotCreateActivity()));   // 14
                ps.setString(i++, nz(dto.getInputMethod()));           // 15
                ps.setString(i++, nz(dto.getCsv_file_Name()));         // 16
                ps.setString(i++, nz(dto.getTar_file_name()));         // 17
                
                ps.setBoolean(i++, dto.isSensitive());                     // 18 - sensitive (BIT column)
                
                ps.addBatch(); // Add to batch
                currentBatchSize++;
                
                // Execute batch when it reaches configured size
                if (currentBatchSize >= batchSize) {
                    int[] results = ps.executeBatch();
                    totalInserted += results.length;
                    conn.commit();
                    
                    if (logger.isDebugEnabled()) {
                        logger.debug("Batch committed: {} rows", results.length);
                    }
                    
                    currentBatchSize = 0;
                }
            }
            
            // Execute remaining batch (if any)
            if (currentBatchSize > 0) {
                int[] results = ps.executeBatch();
                totalInserted += results.length;
                conn.commit();
                
                if (logger.isDebugEnabled()) {
                    logger.debug("Final batch committed: {} rows", results.length);
                }
            }
            
            logger.info("Batch insert completed: {} rows inserted", totalInserted);
            return totalInserted;
            
        } catch (SQLException e) {
            logger.error("Batch insert failed, rolling back", e);
            conn.rollback();
            throw e;
        }
    }
    
    /**
     * Process a batch of DTOs with individual fallback on batch failure
     * This ensures we can identify which specific rows failed
     * 
     * @param dtoList List of DTOs to process
     * @param linesList Corresponding CSV lines (for success tracking)
     * @param conn Database connection
     * @param batchSize Batch size for inserts
     * @param successWriter Writer for successful rows
     * @param failedRows List to accumulate failed rows
     * @param result Processing result object to update
     * @throws IOException if writing success rows fails
     */
    private void processBatchWithFallback(List<ClaimDocumentDTO> dtoList, 
                                          List<String> linesList, 
                                          Connection conn, 
                                          int batchSize,
                                          BufferedWriter successWriter,
                                          List<String> failedRows,
                                          ProcessingResult result) throws IOException {
        try {
            // Try batch insert first (fast path)
            int inserted = insertClaimCenterRowsBatch(dtoList, conn, batchSize);
            
            // All succeeded - write to success file
            for (String line : linesList) {
                successWriter.write(line);
                successWriter.newLine();
            }
            result.successCount += inserted;
            
        } catch (SQLException batchException) {
            // Batch failed - fall back to individual inserts to identify failed rows
            logger.warn("Batch insert failed, falling back to individual inserts: {}", 
                       batchException.getMessage());
            
            for (int i = 0; i < dtoList.size(); i++) {
                ClaimDocumentDTO dto = dtoList.get(i);
                String line = linesList.get(i);
                
                try {
                    insertClaimCenterRow(dto, conn); // Use existing single-row method
                    successWriter.write(line);
                    successWriter.newLine();
                    result.successCount++;
                    
                } catch (SQLException rowException) {
                    String errorMessage = compact(rowException.getMessage());
                    String failedRow = line + ",\"" + errorMessage.replace("\"", "\"\"") + "\"";
                    failedRows.add(failedRow);
                    result.failureCount++;
                    logger.warn("Row failed: {}", errorMessage);
                }
            }
        }
    }
 
    
 // RFC-ish CSV row splitter (handles quotes/commas)
    private java.util.List<String> parseCsvRow(String line) {
        java.util.List<String> out = new java.util.ArrayList<>();
        if (line == null) return out;
        StringBuilder sb = new StringBuilder();
        boolean inQuotes = false;
        for (int i = 0; i < line.length(); i++) {
            char c = line.charAt(i);
            if (c == '"') {
                if (inQuotes && i + 1 < line.length() && line.charAt(i + 1) == '"') {
                    sb.append('"'); i++; // escaped quote
                } else {
                    inQuotes = !inQuotes;
                }
            } else if (c == ',' && !inQuotes) {
                out.add(sb.toString());
                sb.setLength(0);
            } else {
                sb.append(c);
            }
        }
        out.add(sb.toString());
        return out;
    }
    private java.util.Map<String, Integer> indexByName(java.util.List<String> header) {
        java.util.Map<String, Integer> m = new java.util.HashMap<>();
        for (int i = 0; i < header.size(); i++) m.put(header.get(i), i);
        return m;
    }
    private String getCell(java.util.List<String> row, Integer idx) {
        if (idx == null || idx < 0 || idx >= row.size()) return null;
        return row.get(idx);
    }
    private String safeTrim(String s) { return s == null ? "" : s.trim(); }
    private String nz(String s) { return s == null ? "" : s; }

    private org.json.JSONObject parseJsonLenient(String s) {
        if (s == null) return null;
        String t = s.trim();
        if (t.isEmpty()) return null;
        try {
            return new org.json.JSONObject(t);
        } catch (org.json.JSONException e1) {
            try { return new org.json.JSONObject(t.replace('\'', '"')); }
            catch (org.json.JSONException e2) { return null; }
        }
    }
    private String unwrapOnce(String s) {
        if (s == null) return null;
        String t = s.trim();
        if (t.length() >= 2 &&
            ((t.startsWith("\"") && t.endsWith("\"")) || (t.startsWith("'") && t.endsWith("'")))) {
            return t.substring(1, t.length()-1);
        }
        return s;
    }
    private org.json.JSONObject tryFindMetadataObject(org.json.JSONObject root) {
        for (String k : root.keySet()) {
            try {
                Object v = root.get(k);
                if (v instanceof org.json.JSONObject) {
                    org.json.JSONObject jo = (org.json.JSONObject) v;
                    if ("metadata".equalsIgnoreCase(k)) return jo;
                    org.json.JSONObject deeper = tryFindMetadataObject(jo);
                    if (deeper != null) return deeper;
                }
            } catch (Exception ignore) {}
        }
        return null;
    }
    private String compact(String msg) {
        if (msg == null) return "";
        return msg.replaceAll("\\s+", " ").trim();
    }

    /** externalID rule: opentext => doc_uid as-is; filenet => "filenet:" + doc_uid */
    private String buildExternalID(String origRep, String docUid) {
        String or = safeTrim(origRep);
        String du = safeTrim(docUid);
        if (or.equalsIgnoreCase(Constants.OPENTEXT)) {
            du = du.replaceAll("OT", Constants.OPENTEXT_NUMARIC_VALUE);
            return du;
        } else if (or.equalsIgnoreCase(Constants.FILENET)) {
            return Constants.FILENET_NUMARIC_VALUE + ":" + du;
        } else {
            return (or.isEmpty() ? "unknown" : or) + ":" + du;
        }
    }



    // ========================================================================
    // FILE LIFECYCLE MANAGEMENT METHODS
    // ========================================================================
    
    /**
     * Inner class to track processing results per CSV file
     * Made public so it can be accessed from DataTransformService
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
    }
    
    /**
     * Moves both TAR and CSV files to Error folder when processing fails
     */
    private void moveFilesToError(String tarPath, String csvPath, String category, String reason) {
        try {
            // Create error info file
            String errorDirPath = base_path + Constants.errorFolder + File.separator + category;
            Files.createDirectories(Paths.get(errorDirPath));
            
            File tarFile = new File(tarPath);
            File csvFile = new File(csvPath);
            
            // Move TAR file
            if (tarFile.exists()) {
                Path targetTar = Paths.get(errorDirPath, tarFile.getName());
                Files.move(tarFile.toPath(), targetTar, StandardCopyOption.REPLACE_EXISTING);
                logger.info("Moved TAR to Error folder: {}", targetTar);
            }
            
            // Move CSV file
            if (csvFile.exists()) {
                Path targetCsv = Paths.get(errorDirPath, csvFile.getName());
                Files.move(csvFile.toPath(), targetCsv, StandardCopyOption.REPLACE_EXISTING);
                logger.info("Moved CSV to Error folder: {}", targetCsv);
            }
            
            // Create error log file
            String errorLogPath = errorDirPath + File.separator + 
                getFileNameWithoutExtension(csvFile.getName()) + "_error.log";
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogPath))) {
                writer.write("Error Timestamp: " + new java.util.Date());
                writer.newLine();
                writer.write("Reason: " + reason);
                writer.newLine();
                writer.write("TAR File: " + tarFile.getName());
                writer.newLine();
                writer.write("CSV File: " + csvFile.getName());
                writer.newLine();
            }
            logger.info("Created error log: {}", errorLogPath);
            
        } catch (Exception e) {
            logger.error("Failed to move files to Error folder: {}", e.getMessage(), e);
        }
    }
    
    /**
     * Archives or deletes source files after successful processing
     * Controlled by property: app.archive.after.processing (true/false)
     */
    private void handleSourceFilesAfterProcessing(String tarPath, String csvPath, String category) {
        try {
            String archiveAfterProcessing = config.getProperty("app.archive.after.processing");
            boolean shouldArchive = archiveAfterProcessing != null && 
                                   archiveAfterProcessing.trim().equalsIgnoreCase("true");
            
            File tarFile = new File(tarPath);
            File csvFile = new File(csvPath);
            
            if (shouldArchive) {
                // Archive mode: Move to Archive folder
                String archiveDirPath = base_path + Constants.archiveFolder + File.separator + category;
                Files.createDirectories(Paths.get(archiveDirPath));
                
                if (tarFile.exists()) {
                    Path targetTar = Paths.get(archiveDirPath, tarFile.getName());
                    Files.move(tarFile.toPath(), targetTar, StandardCopyOption.REPLACE_EXISTING);
                    logger.info("Archived TAR: {}", targetTar);
                }
                
                if (csvFile.exists()) {
                    Path targetCsv = Paths.get(archiveDirPath, csvFile.getName());
                    Files.move(csvFile.toPath(), targetCsv, StandardCopyOption.REPLACE_EXISTING);
                    logger.info("Archived CSV: {}", targetCsv);
                }
            } else {
                // Delete mode: Remove source files
                if (tarFile.exists() && tarFile.delete()) {
                    logger.info("Deleted TAR: {}", tarPath);
                }
                if (csvFile.exists() && csvFile.delete()) {
                    logger.info("Deleted CSV: {}", csvPath);
                }
            }
        } catch (Exception e) {
            logger.error("Failed to handle source files: {}", e.getMessage(), e);
        }
    }

}

# ===========================================================
# DataTransformUtility - Production Configuration
# Updated: October 11, 2025 (Phase 1 Implementation)
# ===========================================================

# ------------------------------------------------------------
# APPLICATION PATHS
# ------------------------------------------------------------
app.base_path=D:/Rameshwar/wawanesa/

# Comma-separated list of category types to process
# Must match folder names under Source/ directory
#category_claimcenter,category_dcpfnol,category_dcptoclaimcenter
app.category_types=category_claimcenter,category_dcpfnol,category_dcptoclaimcenter

# ------------------------------------------------------------
# CSV PROCESSING OPTIONS
# ------------------------------------------------------------

# Skip the header row when processing CSV files
# true = CSV has header row (skip first line)
# false = CSV has no header (process all lines as data)
app.skipHeaderRow=true

# ------------------------------------------------------------
# FILE LIFECYCLE MANAGEMENT
# ------------------------------------------------------------

# Archive or delete source files after successful processing
# true  = Move processed TAR and CSV files to Archive/{category}/ folder
# false = Delete processed TAR and CSV files from Source/{category}/ folder
app.archive.after.processing=true

# Clean up extracted files after processing to save disk space
# true  = Delete extracted document files from Transformed/{category}/ after archiving
# false = Keep extracted files in Transformed/{category}/ folder
# Recommended: true for production (saves disk space)
app.cleanup.extracted.files.after.processing=false

# ------------------------------------------------------------
# PERFORMANCE & THREADING (Phase 1 - NEW)
# ------------------------------------------------------------

# Thread pool size for parallel file processing
# Higher values = more concurrent file processing = faster completion
# Recommendation: 2x CPU cores for I/O bound tasks
# Examples:
#   - 8 CPU cores  → thread.pool.size=10-15
#   - 16 CPU cores → thread.pool.size=20-30
#   - 32 CPU cores → thread.pool.size=30-50
# For 25M documents: Recommended 20
# Default: 5
app.thread.pool.size=20

# Progress logging interval (minutes)
# How often to log overall processing progress (files, rows, ETA)
# Set to 0 to disable periodic progress logging
# For 25M documents: Recommended 3-5 minutes
# Default: 5
app.progress.log.interval.minutes=5

# ------------------------------------------------------------
# DATABASE CONNECTION (SQL Server)
# ------------------------------------------------------------

# SQL Server hostname or IP address
# Use double backslash (\\) for named instances
db.serverName=RAMESHWAR\\SQLEXPRESS

# SQL Server port
db.port=1433

# Database name
db.dbName=Wawa_DMS_Conversion_UAT

# Connection pool size (number of concurrent database connections)
# IMPORTANT: Should be >= app.thread.pool.size for optimal performance
# Recommendation: Set to app.thread.pool.size + 10 for buffer
# Example: If thread.pool.size=20, set pool.size=30
# For 25M documents: Recommended 30
# Default: 10
db.pool.size=30

# Database batch insert size (Phase 1 - NEW - CRITICAL FOR PERFORMANCE)
# Number of rows to insert in a single batch operation
# Higher values = faster inserts but more memory per thread
# Performance Impact: 100x faster than single-row inserts!
# Recommendations:
#   - Testing/Debug: 100-500   (easier to debug failures)
#   - Production:    1000-2000 (optimal performance)
#   - High-end:      5000      (only if you have 64GB+ RAM)
# For 25M documents: Recommended 1000
# Default: 1000
db.batch.insert.size=1000

# ------------------------------------------------------------
# PERFORMANCE EXPECTATIONS (With Phase 1 Implementation)
# ------------------------------------------------------------
# 
# For 25M documents with recommended settings above:
#   - Processing Time: 12-15 minutes (vs 14 hours without batch inserts)
#   - Throughput:      ~30,000-40,000 rows/sec average
#   - Memory Usage:    8-12GB peak (set JVM -Xmx16G)
#   - Thread Pool:     20 concurrent threads
#   - Success Rate:    99%+ expected
# 
# Progress reports will be logged every 5 minutes showing:
#   - Files processed / remaining
#   - Rows success / failed / skipped
#   - Success rate percentage
#   - Estimated time to completion (ETA)
#   - Thread pool utilization
#   - Memory usage
# 
# Final summary report will be generated at:
#   D:/Rameshwar/wawanesa/DataTransformUtility_Summary_Report_[timestamp].csv
#
# ------------------------------------------------------------

# ------------------------------------------------------------
# JVM SETTINGS RECOMMENDATION
# ------------------------------------------------------------
# For 25M documents, use these JVM settings:
# 
# java -Xms4G \
#      -Xmx16G \
#      -XX:+UseG1GC \
#      -XX:MaxGCPauseMillis=200 \
#      -XX:+HeapDumpOnOutOfMemoryError \
#      -XX:HeapDumpPath=D:\Rameshwar\DataTransformUtility\dumps \
#      -Djava.library.path="C:\Program Files\Microsoft JDBC Driver 12.10 for SQL Server\auth\x64" \
#      -Dconfig.file=D:\Rameshwar\DataTransformUtility\config\DataTransformUtility.properties \
#      -Dlog4j.configurationFile=D:\Rameshwar\DataTransformUtility\config\log4j2.xml \
#      -jar target\DataTransformUtility-1.0-SNAPSHOT.jar
#
# ------------------------------------------------------------

# ===========================================================
# NOTES
# ===========================================================
# 
# 1. Database Batch Inserts (NEW):
#    - Provides 100x performance improvement
#    - If batch fails, automatically falls back to individual inserts
#    - Failed rows are identified and written to *_failed.csv
# 
# 2. Thread Pool Sizing:
#    - More threads = faster processing (up to a point)
#    - Too many threads can cause contention
#    - Monitor "Thread Pool" stats in progress reports
# 
# 3. Global Reporting (NEW):
#    - Summary report generated automatically at end
#    - Contains: total files, rows, success/failure rates, timing
#    - Location: {app.base_path}/DataTransformUtility_Summary_Report_[timestamp].csv
# 
# 4. Progress Monitoring (NEW):
#    - Logs progress every N minutes (configurable)
#    - Shows ETA, success rate, thread pool stats, memory usage
#    - Helps track long-running jobs
# 
# 5. Memory Management:
#    - Monitor logs for "HIGH MEMORY USAGE" warnings
#    - If memory exceeds 85%, consider:
#      * Increasing JVM heap size (-Xmx)
#      * Reducing batch size (db.batch.insert.size)
#      * Reducing thread pool size (app.thread.pool.size)
# 
# ===========================================================

# ------------------------------------------------------------
# DATABASE QUERIES
# ------------------------------------------------------------

# Insert query for ClaimCenter documents
db.query.insert.claimcenter=INSERT INTO dbo.Wawa_Doc_Migration_Transit_Data ( \
  originatingrepo_ext, doc_uid, externalID, category_type, \
  contentFilePath, contentRetrievalName, contentType, \
  OrigDateCreated, file_creation_year, \
  file_id, claimNumber, documentTitle, mimeType, doNotCreateActivity, inputMethod, \
  csv_file_Name, tar_file_name, sensitive \
) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)








=================================================================================================================================================================================








package com.wawanesa.ace.merge;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.merge.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.merge.connection.ConnectionManager;
import com.wawanesa.ace.merge.constants.Constants;
import com.wawanesa.ace.merge.utils.GlobalProcessingReport;
import com.wawanesa.ace.merge.utils.Utils;


public class ClaimCenterDataMergeService {

    private static final Logger Logger = LogManager.getLogger(ClaimCenterDataMergeService.class);

    public static void main(String[] args) {

        Logger.info("Application started");
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        final ConnectionManager[] connManagerRef = new ConnectionManager[1];
        
        // Initialize global processing report
        GlobalProcessingReport report = new GlobalProcessingReport();
        AtomicInteger totalFilesQueued = new AtomicInteger(0);

        try {
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService; // Assign to non-final variable for shutdown hook
            
            Logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        report.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            Logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            report.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        Logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                Logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                Logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination (e.g., Ctrl+C)
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                Logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        Logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                Logger.info("Shutdown hook completed");
            }));

            Logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            Logger.info("Config File: {}", System.getProperty("config.file"));
            Logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String base_path = config.getProperty("app.base_path");
            Logger.info("Base Path: {}", base_path);

            // Validate all required directories exist
            if (isValidDirectory(base_path, Constants.INPUT_FOLDER)
                    && isValidDirectory(base_path, Constants.INPROGRESS_FOLDER)
                    && isValidDirectory(base_path, Constants.COMPLETED_FOLDER)
                    && isValidDirectory(base_path, Constants.FAILED_FOLDER)
                    && isValidDirectory(base_path, Constants.ARCHIVE_FOLDER)) {

                Logger.info("All required directories validated successfully under base path: {}", base_path);

                // Initialize connection manager
                connManagerRef[0] = new ConnectionManager(config);
                Logger.info("Database connection pool initialized");
                
                // Initialize utility class
                Utils utils = new Utils(config, connManagerRef[0]);
                
                // Process all CSV files in Input folder
                Path inputDir = Paths.get(base_path, Constants.INPUT_FOLDER);
                
                // First pass: Count total files to queue
                try (DirectoryStream<Path> stream = Files.newDirectoryStream(inputDir, "conv_claims_extract_*.csv")) {
                    for (Path csvFile : stream) {
                        if (Files.isRegularFile(csvFile)) {
                            totalFilesQueued.incrementAndGet();
                            report.incrementFilesQueued();
                        }
                    }
                    Logger.info("Found {} CSV file(s) to process", totalFilesQueued.get());
                } catch (IOException e) {
                    Logger.error("Error counting CSV files in Input directory", e);
                }
                
                // Second pass: Process files
                processCSVFiles(inputDir, utils, executor, report);

            } else {
                Logger.error("Required directories are missing under the base path: {}", base_path);
                Logger.error("Application terminated due to missing directories");
            }

        } catch (Exception e) {
            Logger.error("Error during processing", e);
            e.printStackTrace();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    Logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    Logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    Logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService gracefully
            if (executor != null) {
                try {
                    Logger.info("Shutting down executor service...");
                    executor.shutdown(); // Disable new tasks from being submitted
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        Logger.warn("Executor did not terminate in time, forcing shutdown...");
                        executor.shutdownNow(); // Cancel currently executing tasks
                        
                        // Wait a while for tasks to respond to being cancelled
                        if (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                            Logger.error("Executor did not terminate after forced shutdown");
                        }
                    }
                    Logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    Logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManagerRef[0] != null) {
                try {
                    Logger.info("Closing database connection pool...");
                    connManagerRef[0].close();
                    Logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    Logger.error("Error closing connection manager", e);
                    e.printStackTrace();
                }
            }
            
            // Write final summary report
            try {
                Logger.info("===== FINAL SUMMARY =====");
                Logger.info(report.getSummary());
                
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                
                // Validate/Create SummaryReports folder
                File summaryReportsDir = new File(basePath, "SummaryReports");
                if (!summaryReportsDir.exists()) {
                    if (summaryReportsDir.mkdirs()) {
                        Logger.info("Created SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                    } else {
                        Logger.error("Failed to create SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                        throw new IOException("Cannot create SummaryReports folder");
                    }
                } else {
                    Logger.debug("SummaryReports folder already exists: {}", summaryReportsDir.getAbsolutePath());
                }
                
                report.writeReportToFile(basePath);
                Logger.info("Summary report written to: {}", summaryReportsDir.getAbsolutePath());
            } catch (Exception e) {
                Logger.error("Failed to write summary report", e);
            }
            
            Logger.info("Application completed");
        }
    }

    /**
     * Process all CSV files in the Input directory using multi-threading
     */
    private static void processCSVFiles(Path inputDir, Utils utils, ExecutorService executor, GlobalProcessingReport report) {
        Logger.info("Scanning Input directory for CSV files: {}", inputDir);
        
        int fileCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(inputDir, "conv_claims_extract_*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    fileCount++;
                    String fileName = csvFile.getFileName().toString();
                    Logger.info("Submitting CSV file for processing: {}", fileName);
                    
                    // Submit each CSV file for parallel processing
                    executor.submit(() -> {
                        try {
                            Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                            
                            // Record success in global report
                            report.recordFileSuccess(fileName, 
                                new GlobalProcessingReport.ProcessingResult(
                                    result.totalRows, 
                                    result.successCount, 
                                    result.failureCount, 
                                    result.skippedEmptyLines
                                )
                            );
                            
                        } catch (Exception e) {
                            Logger.error("Error processing CSV file: {}", fileName, e);
                            report.recordFileFailure(fileName, e.getMessage());
                        }
                    });
                }
            }
            
            if (fileCount == 0) {
                Logger.warn("No CSV files found in Input directory: {}", inputDir);
            } else {
                Logger.info("Total {} CSV file(s) submitted for processing", fileCount);
            }
            
        } catch (IOException e) {
            Logger.error("Error reading Input directory: {}", inputDir, e);
        }
    }
    
    /**
     * Validate that a directory exists under the base path
     */
    private static boolean isValidDirectory(String base_path, String folderName) {
        boolean isExist = false;
        try {
            File dir = new File(base_path + folderName);
            if (dir.exists() && dir.isDirectory()) {
                isExist = true;
                Logger.debug("Directory validated: {}", dir.getAbsolutePath());
            } else {
                Logger.error("Expected directory does not exist: {}", 
                           base_path + File.separator + folderName);
            }
        } catch (Exception e) {
            Logger.error("Error validating directory: {}", folderName, e);
        }
        return isExist;
    }
}



package com.wawanesa.ace.merge.model;

/**
 * Data Transfer Object for ClaimCenter Document CSV data
 * Maps CSV columns to database columns for UPDATE operations
 */
public class ClaimCenterDocumentDTO {
    
    private String documentTitle;
    private String documentType;
    private String documentSubtype;
    private String claimID;
    private String claimNumber;
    private String policyNumber;
    private String author;
    private String documentDescription;
    private String claimType;
    private String externalID;
    private String gwDocumentID;
    private String csvFileName;
    private boolean isDataMerged;
    
	public String getDocumentTitle() {
		return documentTitle;
	}
	public void setDocumentTitle(String documentTitle) {
		this.documentTitle = documentTitle;
	}
	public String getDocumentType() {
		return documentType;
	}
	public void setDocumentType(String documentType) {
		this.documentType = documentType;
	}
	public String getDocumentSubtype() {
		return documentSubtype;
	}
	public void setDocumentSubtype(String documentSubtype) {
		this.documentSubtype = documentSubtype;
	}
	public String getClaimID() {
		return claimID;
	}
	public void setClaimID(String claimID) {
		this.claimID = claimID;
	}
	public String getClaimNumber() {
		return claimNumber;
	}
	public void setClaimNumber(String claimNumber) {
		this.claimNumber = claimNumber;
	}
	public String getPolicyNumber() {
		return policyNumber;
	}
	public void setPolicyNumber(String policyNumber) {
		this.policyNumber = policyNumber;
	}
	public String getAuthor() {
		return author;
	}
	public void setAuthor(String author) {
		this.author = author;
	}
	public String getDocumentDescription() {
		return documentDescription;
	}
	public void setDocumentDescription(String documentDescription) {
		this.documentDescription = documentDescription;
	}
	public String getClaimType() {
		return claimType;
	}
	public void setClaimType(String claimType) {
		this.claimType = claimType;
	}
	public String getExternalID() {
		return externalID;
	}
	public void setExternalID(String externalID) {
		this.externalID = externalID;
	}
	public String getGwDocumentID() {
		return gwDocumentID;
	}
	public void setGwDocumentID(String gwDocumentID) {
		this.gwDocumentID = gwDocumentID;
	}
	public String getCsvFileName() {
		return csvFileName;
	}
	public void setCsvFileName(String csvFileName) {
		this.csvFileName = csvFileName;
	}
	public boolean isDataMerged() {
		return isDataMerged;
	}
	public void setDataMerged(boolean isDataMerged) {
		this.isDataMerged = isDataMerged;
	}
    
    
   
}



package com.wawanesa.ace.merge.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // File-level counters
    private final AtomicInteger totalFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalFilesFailed = new AtomicInteger(0);
    
    // Row-level counters
    private final AtomicLong totalRowsProcessed = new AtomicLong(0);
    private final AtomicLong totalRowsUpdated = new AtomicLong(0);
    private final AtomicLong totalRowsFailed = new AtomicLong(0);
    private final AtomicLong totalRowsSkipped = new AtomicLong(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a file has been queued for processing
     */
    public void incrementFilesQueued() {
        totalFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful file processing with statistics
     */
    public void recordFileSuccess(String fileName, ProcessingResult result) {
        totalFilesProcessed.incrementAndGet();
        totalFilesSuccess.incrementAndGet();
        
        totalRowsProcessed.addAndGet(result.totalRows);
        totalRowsUpdated.addAndGet(result.successCount);
        totalRowsFailed.addAndGet(result.failureCount);
        totalRowsSkipped.addAndGet(result.skippedEmptyLines);
        
        logger.info("File completed: {} | Rows: Total={}, Updated={}, Failed={}, Skipped={}", 
                   fileName, result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
    }
    
    /**
     * Record failed file processing
     */
    public void recordFileFailure(String fileName, String errorMessage) {
        totalFilesProcessed.incrementAndGet();
        totalFilesFailed.incrementAndGet();
        
        logger.error("File failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalFilesQueued.get(), totalFilesProcessed.get(), totalFilesQueued.get(),
                   totalFilesSuccess.get(), totalFilesFailed.get());
        logger.info("Rows: Processed={}, Updated={}, Failed={}, Skipped={}", 
                   totalRowsProcessed.get(), totalRowsUpdated.get(), totalRowsFailed.get(), totalRowsSkipped.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- File Statistics ---\n");
        sb.append(String.format("Total Files Queued: %d\n", totalFilesQueued.get()));
        sb.append(String.format("Total Files Processed: %d\n", totalFilesProcessed.get()));
        sb.append(String.format("Files Success: %d\n", totalFilesSuccess.get()));
        sb.append(String.format("Files Failed: %d\n", totalFilesFailed.get()));
        sb.append("\n--- Row Statistics ---\n");
        sb.append(String.format("Total Rows Processed: %d\n", totalRowsProcessed.get()));
        sb.append(String.format("Total Rows Updated: %d\n", totalRowsUpdated.get()));
        sb.append(String.format("Total Rows Failed: %d\n", totalRowsFailed.get()));
        sb.append(String.format("Total Rows Skipped: %d\n", totalRowsSkipped.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long rowsPerSecond = totalRowsProcessed.get() / totalDuration.getSeconds();
            long rowsPerMinute = totalRowsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d rows/sec, %d rows/min\n", rowsPerSecond, rowsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataMerge_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, "SummaryReports", fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write file statistics
            writer.newLine();
            writer.write("--- File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Files Queued,%d", totalFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total Files Processed,%d", totalFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("Files Success,%d", totalFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("Files Failed,%d", totalFilesFailed.get()));
            writer.newLine();
            
            // Write row statistics
            writer.newLine();
            writer.write("--- Row Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Rows Processed,%d", totalRowsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Updated,%d", totalRowsUpdated.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Failed,%d", totalRowsFailed.get()));
            writer.newLine();
            writer.write(String.format("Total Rows Skipped,%d", totalRowsSkipped.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long rowsPerSecond = totalRowsProcessed.get() / totalDuration.getSeconds();
                long rowsPerMinute = totalRowsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Rows Per Second,%d", rowsPerSecond));
                writer.newLine();
                writer.write(String.format("Rows Per Minute,%d", rowsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
    /**
     * Inner class for tracking individual file processing results
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
        
        public ProcessingResult() {}
        
        public ProcessingResult(int totalRows, int successCount, int failureCount, int skippedEmptyLines) {
            this.totalRows = totalRows;
            this.successCount = successCount;
            this.failureCount = failureCount;
            this.skippedEmptyLines = skippedEmptyLines;
        }
    }
}



package com.wawanesa.ace.merge.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.temporal.ChronoUnit;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.merge.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.merge.connection.ConnectionManager;
import com.wawanesa.ace.merge.constants.Constants;
import com.wawanesa.ace.merge.model.ClaimCenterDocumentDTO;

/**
 * Utility class for processing ClaimCenter CSV extracts and updating database records
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private String updateMergeQuery;
    
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load SQL query from properties and replace table name placeholder
        String queryTemplate = config.getProperty("db.query.update.merge");
        this.updateMergeQuery = queryTemplate.replace("%TABLE_NAME%", this.dbTableName);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalRows = 0;
        public int successCount = 0;
        public int failureCount = 0;
        public int skippedEmptyLines = 0;
    }
    
    /**
     * Process a single CSV file from Input folder
     * Returns ProcessingResult for global reporting
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== Starting processing of CSV file: {} =====", csvFileName);
        
        Path inprogressDir = Paths.get(basePath, Constants.INPROGRESS_FOLDER);
        Path inprogressFile = inprogressDir.resolve(csvFileName);
        
        try {
            // Step 1: Move file to Inprogress folder
            logger.info("Moving {} to Inprogress folder", csvFileName);
            Files.move(csvFilePath, inprogressFile, StandardCopyOption.REPLACE_EXISTING);
            logger.info("File moved to: {}", inprogressFile);
            
            // Step 2: Process the CSV file
            ProcessingResult result = processCSVWithTracking(inprogressFile);
            
            // Step 3: Handle file lifecycle based on results
            handleFileLifecycle(inprogressFile, result);
            
            logger.info("===== Completed processing of CSV file: {} =====", csvFileName);
            logger.info("Total Rows: {}, Success: {}, Failed: {}, Empty Lines Skipped: {}", 
                       result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to Failed folder on critical error
            try {
                Path failedDir = Paths.get(basePath, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(inprogressFile, failedFile, StandardCopyOption.REPLACE_EXISTING);
                
                // Create error log file
                createErrorLogFile(failedDir, csvFileName, "Critical processing error: " + e.getMessage());
                
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder: {}", csvFileName, ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    /**
     * Process CSV file with row-level tracking
     */
    private ProcessingResult processCSVWithTracking(Path csvFilePath) throws IOException {
        ProcessingResult result = new ProcessingResult();
        String csvFileName = csvFilePath.getFileName().toString();
        
        // Prepare output files for success and failure tracking
        Path completedDir = Paths.get(basePath, Constants.COMPLETED_FOLDER);
        Path failedDir = Paths.get(basePath, Constants.FAILED_FOLDER);
        
        String baseFileName = csvFileName.replace(".csv", "");
        Path successFile = completedDir.resolve(baseFileName + "_SUCCESS.csv");
        Path failedFile = failedDir.resolve(baseFileName + "_FAILED.csv");
        
        List<String> successRows = new ArrayList<>();
        List<String> failedRows = new ArrayList<>();
        
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = skipHeaderProp == null ? true : Boolean.parseBoolean(skipHeaderProp);
        String headerLine = null;
        
        // Get batch size from configuration
        String batchSizeStr = config.getProperty("db.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        logger.info("Processing CSV file: {}", csvFilePath);
        logger.info("Delimiter: pipe (|), Skip header: {}, Batch size: {}", skipHeaderRow, batchSize);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()));
             Connection conn = connectionManager.getConnection()) {
            
            String line;
            int rowNum = 0;
            
            // Read and process header
            if (skipHeaderRow && (line = reader.readLine()) != null) {
                headerLine = line.trim();
                logger.debug("Header: {}", headerLine);
            }
            
            // Build header index map
            Map<String, Integer> headerIndex = buildHeaderIndex(headerLine);
            
            // Validate required columns exist
            if (!validateRequiredColumns(headerIndex, csvFileName)) {
                result.failureCount = 1;
                return result;
            }
            
            // Batch processing buffers
            List<ClaimCenterDocumentDTO> batchDTOs = new ArrayList<>();
            List<String> batchLines = new ArrayList<>();
            
            // Process each row
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().length() == 0) {
                    result.skippedEmptyLines++;
                    continue;
                }
                
                result.totalRows++;
                String currentLine = line;
                
                try {
                    // Parse CSV row
                    String[] cells = parseCSVLine(line);
                    
                    // Build DTO from CSV row
                    ClaimCenterDocumentDTO dto = buildDTOFromCSV(cells, headerIndex, csvFileName);
                    
                    dto.setDataMerged(true);
                    
                    // Validate DTO
                    validateDTO(dto, rowNum);
                    
                    // Add to batch
                    batchDTOs.add(dto);
                    batchLines.add(currentLine);
                    
                    // Process batch when it reaches batch size
                    if (batchDTOs.size() >= batchSize) {
                        processBatchWithFallback(batchDTOs, batchLines, conn, batchSize, 
                                               successRows, failedRows, result);
                        batchDTOs.clear();
                        batchLines.clear();
                        
                        logger.debug("Processed batch at row {}", rowNum);
                    }
                    
                } catch (Exception e) {
                    result.failureCount++;
                    String errorMsg = e.getMessage().replace("\"", "'").replace("|", ";");
                    failedRows.add(currentLine + "|" + errorMsg);
                    logger.error("Row {}: Failed to process - {}", rowNum, e.getMessage());
                }
            }
            
            // Process remaining batch
            if (!batchDTOs.isEmpty()) {
                processBatchWithFallback(batchDTOs, batchLines, conn, batchSize, 
                                       successRows, failedRows, result);
                logger.debug("Processed final batch of {} rows", batchDTOs.size());
            }
            
            logger.info("CSV processing completed: Total={}, Success={}, Failed={}, Skipped Empty={}", 
                       result.totalRows, result.successCount, result.failureCount, result.skippedEmptyLines);
            
        } catch (SQLException e) {
            logger.error("Database connection error while processing CSV: {}", csvFileName, e);
            throw new IOException("Database error: " + e.getMessage(), e);
        }
        
        // Write success file
        if (!successRows.isEmpty()) {
            writeSuccessFile(successFile, headerLine, successRows);
            logger.info("Created success file with {} rows: {}", successRows.size(), successFile);
        }
        
        // Write failed file (only if there are failures)
        if (!failedRows.isEmpty()) {
            writeFailedFile(failedFile, headerLine, failedRows);
            logger.info("Created failed file with {} rows: {}", failedRows.size(), failedFile);
        }
        
        return result;
    }
    
    /**
     * Build header index map for CSV columns
     */
    private Map<String, Integer> buildHeaderIndex(String headerLine) {
        Map<String, Integer> index = new HashMap<>();
        if (headerLine == null || headerLine.trim().isEmpty()) {
            return index;
        }
        
        String[] headers = headerLine.split("\\|", -1);
        for (int i = 0; i < headers.length; i++) {
            String header = headers[i].trim().replace("\"", "");
            index.put(header, i);
        }
        
        logger.debug("Header index built with {} columns", index.size());
        return index;
    }
    
    /**
     * Validate that all required columns exist in CSV
     */
    private boolean validateRequiredColumns(Map<String, Integer> headerIndex, String csvFileName) {
    	// NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
    	String[] requiredColumns = {
    	    "NAME", "DOCUMENT_TYPE", "CLAIM_NUMBER", "POLICY_NUMBER", "DOCUMENT_SUBTYPE",
    	    "AUTHOR", "DOCUMENT_DESCRIPTION", "STATUS", "EXTERNALID", "ID", "CLAIMID"
    	};
        
        List<String> missingColumns = new ArrayList<>();
        for (String col : requiredColumns) {
            if (!headerIndex.containsKey(col)) {
                missingColumns.add(col);
            }
        }
        
        if (!missingColumns.isEmpty()) {
            logger.error("CSV file {} is missing required columns: {}", csvFileName, missingColumns);
            return false;
        }
        
        return true;
    }
    
    /**
     * Parse CSV line handling pipe delimiter
     */
    private String[] parseCSVLine(String line) {
        // Simple split by pipe - handles basic CSV
        return line.split("\\|", -1);
    }
    
    /**
     * Build DTO from CSV row
     */
    private ClaimCenterDocumentDTO buildDTOFromCSV(String[] cells, Map<String, Integer> headerIndex, String csvFileName) {
        ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();

        // NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
        dto.setDocumentTitle(safeTrim(getCell(cells, headerIndex.get("NAME"))));
        dto.setDocumentType(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_TYPE"))));
        dto.setClaimNumber(safeTrim(getCell(cells, headerIndex.get("CLAIM_NUMBER"))));
        dto.setPolicyNumber(safeTrim(getCell(cells, headerIndex.get("POLICY_NUMBER"))));
        dto.setDocumentSubtype(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_SUBTYPE"))));
        dto.setAuthor(safeTrim(getCell(cells, headerIndex.get("AUTHOR"))));
        dto.setDocumentDescription(safeTrim(getCell(cells, headerIndex.get("DOCUMENT_DESCRIPTION"))));
        dto.setClaimType(safeTrim(getCell(cells, headerIndex.get("STATUS"))));
        dto.setExternalID(safeTrim(getCell(cells, headerIndex.get("EXTERNALID"))));
        dto.setGwDocumentID(safeTrim(getCell(cells, headerIndex.get("ID"))));
        dto.setClaimID(safeTrim(getCell(cells, headerIndex.get("CLAIMID"))));
        dto.setCsvFileName(csvFileName);

        return dto;
    }

    
    /**
     * Validate DTO has required fields
     */
    private void validateDTO(ClaimCenterDocumentDTO dto, int rowNum) throws Exception {
        if (dto.getExternalID() == null || dto.getExternalID().trim().isEmpty()) {
            throw new Exception("Missing required field: externalID");
        }
    }
    
    /**
     * Update database record matching externalID
     */
    private int updateDatabaseRecord(ClaimCenterDocumentDTO dto, Connection conn) throws SQLException {
        // NAME|DOCUMENT_TYPE|CLAIM_NUMBER|POLICY_NUMBER|DOCUMENT_SUBTYPE|AUTHOR|DOCUMENT_DESCRIPTION|STATUS|EXTERNALID|ID|CLAIMID
        try (PreparedStatement pstmt = conn.prepareStatement(updateMergeQuery)) {
        	 pstmt.setString(1, dto.getDocumentTitle());
             pstmt.setString(2, dto.getDocumentType());
             pstmt.setString(3, dto.getDocumentSubtype());
             //pstmt.setLong(4, Long.parseLong(dto.getClaimID()));
             pstmt.setString(4, dto.getClaimNumber());
             pstmt.setString(5, dto.getPolicyNumber());
             pstmt.setString(6, dto.getAuthor());
             pstmt.setString(7, dto.getDocumentDescription());
             pstmt.setString(8, dto.getClaimType());
             pstmt.setString(9, dto.getGwDocumentID());
             pstmt.setBoolean(10, true);
             pstmt.setString(11, dto.getCsvFileName());
             pstmt.setTimestamp(12, Timestamp.valueOf(LocalDateTime.now().truncatedTo(ChronoUnit.MILLIS)));
             pstmt.setString(13, dto.getExternalID());

            System.out.println("UPDATE SQL executed: rows affected for externalID=" + dto.getExternalID());
            int rowsUpdated = pstmt.executeUpdate();

            if (logger.isDebugEnabled()) {
                logger.debug("UPDATE SQL executed: {} rows affected for externalID={}", rowsUpdated, dto.getExternalID());
            }

            return rowsUpdated;
        }
    }
    
    /**
     * Update multiple database records using JDBC batch processing
     * Significantly faster than individual UPDATEs for large datasets
     * 
     * @param dtoList List of DTOs to update
     * @param conn Database connection
     * @param batchSize Maximum batch size
     * @return Number of rows successfully updated
     */
    private int updateDatabaseRecordsBatch(List<ClaimCenterDocumentDTO> dtoList, Connection conn, int batchSize) 
            throws SQLException {
        
        if (dtoList == null || dtoList.isEmpty()) {
            return 0;
        }
        
        int totalUpdated = 0;
        
        try (PreparedStatement pstmt = conn.prepareStatement(updateMergeQuery)) {
            int batchCount = 0;
            
            for (ClaimCenterDocumentDTO dto : dtoList) {
                pstmt.setString(1, dto.getDocumentTitle());
                pstmt.setString(2, dto.getDocumentType());
                pstmt.setString(3, dto.getDocumentSubtype());
                //pstmt.setLong(4, Long.parseLong(dto.getClaimID()));
                pstmt.setString(4, dto.getClaimNumber());
                pstmt.setString(5, dto.getPolicyNumber());
                pstmt.setString(6, dto.getAuthor());
                pstmt.setString(7, dto.getDocumentDescription());
                pstmt.setString(8, dto.getClaimType());
                pstmt.setString(9, dto.getGwDocumentID());
                pstmt.setBoolean(10, true);
                pstmt.setString(11, dto.getCsvFileName());
                pstmt.setTimestamp(12, Timestamp.valueOf(LocalDateTime.now().truncatedTo(ChronoUnit.MILLIS)));
                pstmt.setString(13, dto.getExternalID());
                
                pstmt.addBatch();
                batchCount++;
                
                // Execute batch when reaching batch size
                if (batchCount >= batchSize) {
                    int[] updateCounts = pstmt.executeBatch();
                    totalUpdated += countSuccessfulUpdates(updateCounts);
                    batchCount = 0;
                    
                    logger.debug("Executed batch of {} updates", batchSize);
                }
            }
            
            // Execute remaining batch
            if (batchCount > 0) {
                int[] updateCounts = pstmt.executeBatch();
                totalUpdated += countSuccessfulUpdates(updateCounts);
                
                logger.debug("Executed final batch of {} updates", batchCount);
            }
            
            logger.info("Batch UPDATE completed: {} records updated from {} DTOs", totalUpdated, dtoList.size());
            
            return totalUpdated;
        }
    }
    
    /**
     * Count successful updates from batch execution results
     */
    private int countSuccessfulUpdates(int[] updateCounts) {
        int count = 0;
        for (int updateCount : updateCounts) {
            if (updateCount > 0) {
                count++;
            }
        }
        return count;
    }
    
    /**
     * Process a batch of DTOs with fallback to individual processing on failure
     */
    private void processBatchWithFallback(List<ClaimCenterDocumentDTO> dtoList,
                                          List<String> linesList,
                                          Connection conn,
                                          int batchSize,
                                          List<String> successRows,
                                          List<String> failedRows,
                                          ProcessingResult result) throws SQLException {
        
        if (dtoList.isEmpty()) {
            return;
        }
        
        try {
            // Attempt batch UPDATE
            int successCount = updateDatabaseRecordsBatch(dtoList, conn, batchSize);
            
            // If all records updated successfully, add to success rows
            if (successCount == dtoList.size()) {
                successRows.addAll(linesList);
                result.successCount += successCount;
                logger.debug("Batch UPDATE successful: {}/{} records", successCount, dtoList.size());
            } else {
                // Partial success - fall back to individual processing to identify failures
                logger.warn("Batch UPDATE partial success: {}/{} - falling back to individual processing", 
                           successCount, dtoList.size());
                processBatchIndividually(dtoList, linesList, conn, successRows, failedRows, result);
            }
            
        } catch (SQLException e) {
            // Batch failed - fall back to individual processing
            logger.warn("Batch UPDATE failed, falling back to individual processing: {}", e.getMessage());
            processBatchIndividually(dtoList, linesList, conn, successRows, failedRows, result);
        }
    }
    
    /**
     * Process DTOs individually (fallback method)
     */
    private void processBatchIndividually(List<ClaimCenterDocumentDTO> dtoList,
                                         List<String> linesList,
                                         Connection conn,
                                         List<String> successRows,
                                         List<String> failedRows,
                                         ProcessingResult result) {
        
        for (int i = 0; i < dtoList.size(); i++) {
            ClaimCenterDocumentDTO dto = dtoList.get(i);
            String line = linesList.get(i);
            
            try {
                int rowsUpdated = updateDatabaseRecord(dto, conn);
                
                if (rowsUpdated > 0) {
                    result.successCount++;
                    successRows.add(line);
                } else {
                    result.failureCount++;
                    String errorMsg = "No record found with externalID=" + dto.getExternalID();
                    failedRows.add(line + "|" + errorMsg);
                    logger.warn("No record found for externalID={}", dto.getExternalID());
                }
                
            } catch (Exception e) {
                result.failureCount++;
                String errorMsg = e.getMessage().replace("\"", "'").replace("|", ";");
                failedRows.add(line + "|" + errorMsg);
                logger.error("Failed to update externalID={}: {}", dto.getExternalID(), e.getMessage());
            }
        }
    }

    
    /**
     * Write success rows to file
     */
    private void writeSuccessFile(Path successFile, String headerLine, List<String> successRows) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
            // Write header
            if (headerLine != null) {
                writer.write(headerLine);
                writer.newLine();
            }
            
            // Write success rows
            for (String row : successRows) {
                writer.write(row);
                writer.newLine();
            }
        }
    }
    
    /**
     * Write failed rows to file with error messages
     */
    private void writeFailedFile(Path failedFile, String headerLine, List<String> failedRows) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
            // Write header with error_message column
            if (headerLine != null) {
                writer.write(headerLine + "|error_message");
                writer.newLine();
            }
            
            // Write failed rows (already have error message appended)
            for (String row : failedRows) {
                writer.write(row);
                writer.newLine();
            }
        }
    }
    
    /**
     * Handle file lifecycle after processing
     */
    private void handleFileLifecycle(Path inprogressFile, ProcessingResult result) throws IOException {
        String csvFileName = inprogressFile.getFileName().toString();
        String archiveProp = config.getProperty("app.archive.after.processing");
        boolean archiveAfterProcessing = archiveProp == null ? true : Boolean.parseBoolean(archiveProp);
        
        if (archiveAfterProcessing) {
            // Move to Archive folder
            Path archiveDir = Paths.get(basePath, Constants.ARCHIVE_FOLDER);
            Path archiveFile = archiveDir.resolve(csvFileName);
            
            Files.move(inprogressFile, archiveFile, StandardCopyOption.REPLACE_EXISTING);
            logger.info("Moved processed file to Archive: {}", archiveFile);
        } else {
            // Delete the file
            Files.delete(inprogressFile);
            logger.info("Deleted processed file from Inprogress: {}", csvFileName);
        }
    }
    
    /**
     * Create error log file for critical failures
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replace(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + java.time.LocalDateTime.now());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file for {}", csvFileName, e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        // Handle "null" string as empty
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
    
    /**
     * Safe get cell from array
     */
    private String getCell(String[] cells, Integer index) {
        if (index == null || index < 0 || index >= cells.length) {
            return "";
        }
        return cells[index];
    }
}



# ===========================================================
# CCDataMergeUtility - Production Configuration
# ===========================================================
# Last Updated: 2025-10-12
# Optimized for high-volume production processing (2M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base_path=D:/Rameshwar/ClaimCenterDataMerge/
app.skipHeaderRow=true
app.archive.after.processing=true

# ===== PERFORMANCE TUNING =====
# Thread Pool Size: Number of parallel CSV file processors
# Recommended: 20-30 for production (2M documents/day)
# Default: 5 for development/testing
app.thread.pool.size=20

# Progress Monitoring: Log progress every N minutes
# Set to 0 to disable progress monitoring
# Recommended: 5 minutes for production visibility
app.progress.log.interval.minutes=5

# ===== DATABASE SETTINGS =====
db.serverName=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.dbName=Wawa_DMS_Conversion_UAT

# Database Connection Pool Size
# IMPORTANT: Should be >= app.thread.pool.size + 10 (buffer for overhead)
# Recommended: 30-40 for production with 20 threads
db.pool.size=30

# Target table for UPDATE operations
db.staging.claimcenterdbtable=Wawa_DMS_Conversion_UAT.[dbo].[Wawa_Doc_Migration_Transit_Data]

# Batch UPDATE Size: Number of rows per JDBC batch
# CRITICAL FOR PERFORMANCE: Batch UPDATEs are 50-100x faster than individual UPDATEs
# Recommended: 1000-2000 rows per batch
# Default: 1000
db.batch.update.size=1000

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage of max heap)
# Warning logged when memory usage exceeds this threshold
app.memory.warning.threshold=80

# ===== NOTES =====
# Performance Estimates for 2M documents/day:
# - With batch UPDATEs (1000/batch): ~15-20 minutes total
# - Thread pool of 20: Processes ~200 files in parallel
# - Connection pool of 30: Adequate for 20 threads + overhead
#
# JVM Recommended Settings:
# -Xms4G -Xmx8G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# Global Processing Report:
# - Written to: {app.base_path}/CCDataMerge_Summary_Report_{timestamp}.csv
# - Contains: Total files, rows, success/failure counts, processing time, throughput

# ===== DATABASE QUERIES =====
# Update query for merging ClaimCenter document metadata
db.query.update.merge=UPDATE %TABLE_NAME% SET \
documentTitle = ?, \
documentType = ?, \
documentSubtype = ?, \
claimNumber = ?, \
policyNumber = ?, \
author = ?, \
documentDescription = ?, \
claimType = ?, \
gwDocumentID = ?, \
isDataMerged = ?, \
CC_Extract_file_Name = ?, \
CC_Extract_file_loaded_date = ? \
WHERE externalID = ?


========================================================================================================================================================================



package com.wawanesa.ace.index;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.utils.GlobalProcessingReport;
import com.wawanesa.ace.index.utils.Utils;

/**
 * CCDataIndexAndPackagingUtility - Main Service Class
 * 
 * Purpose:
 * - Process CSV files from CCDataMergeUtility output
 * - Apply batching/indexing logic to documents
 * - Update database with batch/set metadata
 * - Generate packaging CSV files for migration
 * 
 * Processing Flow:
 * Step 1: Validate/Create folder structure
 * Step 2: Move CSV files from Completed → Indexing\Data
 * Step 3: Extract claim numbers → Query DB → Apply batching logic
 * Step 4: Update database with indexing metadata
 * Step 5a: Write success/failed tracking files → Archive CSV
 * Step 5b: Generate packaging CSV files (pipe-delimited, max 50K rows)
 */
public class CCDataIndexPackagingService {

    private static final Logger logger = LogManager.getLogger(CCDataIndexPackagingService.class);

    public static void main(String[] args) {

        logger.info("===== CCDataIndexAndPackagingUtility STARTED =====");
        ConnectionManager connManager = null;
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        
        // Initialize global processing report
        GlobalProcessingReport globalReport = new GlobalProcessingReport();

        try {
            // Load configuration
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            logger.info("=== CONFIGURATION ===");
            logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            logger.info("Config File: {}", System.getProperty("config.file"));
            logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String basePath = config.getProperty("app.base_path");
            logger.info("Base Path: {}", basePath);
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService;
            
            logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            final ConnectionManager[] connManagerRef = new ConnectionManager[1];
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        globalReport.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            globalReport.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                logger.info("Shutdown hook completed");
            }));
            
            // STEP 1: Validate and create required folder structure
            logger.info("=== STEP 1: Validating Folder Structure ===");
            if (!validateAndCreateFolderStructure(basePath)) {
                logger.error("Failed to validate/create folder structure. Exiting...");
                return;
            }
            logger.info("Folder structure validated successfully");
            
            // Initialize connection manager
            connManager = new ConnectionManager(config);
            connManagerRef[0] = connManager;
            logger.info("Database connection pool initialized");
            
            // Initialize utility class
            Utils utils = new Utils(config, connManager);
            
            // STEP 2: Move CSV files from Completed → Indexing\Data
            logger.info("=== STEP 2: Moving CSV Files ===");
            int movedFileCount = moveCSVFilesToIndexingData(basePath);
            logger.info("Moved {} CSV file(s) to Indexing\\Data folder", movedFileCount);
            
            if (movedFileCount == 0) {
                logger.warn("No CSV files found to process. Exiting...");
                return;
            }
            
            // STEP 3-5: Process all CSV files in Indexing\Data folder (MULTI-THREADED)
            logger.info("=== STEP 3-5: Processing CSV Files (Multi-Threaded) ===");
            Path indexingDataDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
            int processedFileCount = processCSVFiles(indexingDataDir, utils, executor, globalReport);
            
            logger.info("=== PROCESSING COMPLETE ===");
            logger.info("Total CSV files processed: {}", processedFileCount);
            logger.info("===== CCDataIndexAndPackagingUtility COMPLETED SUCCESSFULLY =====");

        } catch (Exception e) {
            logger.error("Critical error in CCDataIndexPackagingService", e);
            e.printStackTrace();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService gracefully
            if (executor != null) {
                try {
                    logger.info("Shutting down executor service...");
                    executor.shutdown();
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        logger.warn("Executor did not terminate in time, forcing shutdown...");
                        executor.shutdownNow();
                        
                        if (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                            logger.error("Executor did not terminate after forced shutdown");
                        }
                    }
                    logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManager != null) {
                try {
                    logger.info("Closing database connection pool...");
                    connManager.close();
                    logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    logger.error("Error closing connection manager", e);
                }
            }
            
            // Write final summary report
            try {
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                
                logger.info("===== FINAL SUMMARY =====");
                logger.info(globalReport.getSummary());
                
                // Validate/Create SummaryReports folder
                File summaryReportsDir = new File(basePath, "SummaryReports");
                if (!summaryReportsDir.exists()) {
                    if (summaryReportsDir.mkdirs()) {
                        logger.info("Created SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                    } else {
                        logger.error("Failed to create SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                        throw new IOException("Cannot create SummaryReports folder");
                    }
                } else {
                    logger.debug("SummaryReports folder already exists: {}", summaryReportsDir.getAbsolutePath());
                }
                
                globalReport.writeReportToFile(basePath);
                logger.info("Summary report written to: {}", summaryReportsDir.getAbsolutePath());
            } catch (Exception e) {
                logger.error("Failed to write summary report", e);
            }
            
            logger.info("Application terminated");
        }
    }

    // ==================== STEP 1: VALIDATE AND CREATE FOLDERS ====================
    
    /**
     * Step 1: Validate and create required folder structure
     * 
     * Required folders:
     * - D:\Rameshwar\ClaimCenterDataMerge\Completed (source)
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Archive
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Completed
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Failed
     * - D:\Rameshwar\ClaimCenterDataMerge\Packaging
     */
    private static boolean validateAndCreateFolderStructure(String basePath) {
        boolean success = true;
        
        // Define required folders
        String[] requiredFolders = {
            Constants.COMPLETED_SOURCE_FOLDER,
            Constants.INDEXING_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.DATA_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.ARCHIVE_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.COMPLETED_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.FAILED_FOLDER,
            Constants.PACKAGING_FOLDER
        };
        
        for (String folderName : requiredFolders) {
            File folder = new File(basePath, folderName);
            
            if (folder.exists() && folder.isDirectory()) {
                logger.info("✓ Folder exists: {}", folder.getAbsolutePath());
            } else {
                // Create folder
                if (folder.mkdirs()) {
                    logger.info("✓ Folder created: {}", folder.getAbsolutePath());
                } else {
                    logger.error("✗ Failed to create folder: {}", folder.getAbsolutePath());
                    success = false;
                }
            }
        }
        
        return success;
    }
    
    // ==================== STEP 2: MOVE CSV FILES ====================
    
    /**
     * Step 2: Move all CSV files from Completed → Indexing\Data
     * 
     * Source: D:\Rameshwar\ClaimCenterDataMerge\Completed
     * Target: D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     */
    private static int moveCSVFilesToIndexingData(String basePath) {
        Path sourceDir = Paths.get(basePath, Constants.COMPLETED_SOURCE_FOLDER);
        Path targetDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
        
        logger.info("Source Directory: {}", sourceDir);
        logger.info("Target Directory: {}", targetDir);
        
        int movedCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    try {
                        String fileName = csvFile.getFileName().toString();
                        Path targetFile = targetDir.resolve(fileName);
                        
                        Files.move(csvFile, targetFile, StandardCopyOption.REPLACE_EXISTING);
                        logger.info("Moved: {} → {}", fileName, targetDir);
                        movedCount++;
                        
                    } catch (IOException e) {
                        logger.error("Failed to move file: {}", csvFile.getFileName(), e);
                    }
                }
            }
        } catch (IOException e) {
            logger.error("Error reading source directory: {}", sourceDir, e);
        }
        
        return movedCount;
    }
    
    // ==================== STEP 3-5: PROCESS CSV FILES (MULTI-THREADED) ====================
    
    /**
     * Step 3-5: Process all CSV files in Indexing\Data folder using multi-threading
     * 
     * For each CSV file (in parallel):
     * - Extract claim numbers
     * - Query database for documents
     * - Apply batching/indexing logic
     * - Update database (BATCH UPDATEs)
     * - Write tracking files
     * - Generate packaging CSV files
     * - Archive processed CSV
     */
    private static int processCSVFiles(Path indexingDataDir, Utils utils, 
                                      ExecutorService executor, GlobalProcessingReport globalReport) {
        logger.info("Scanning Indexing\\Data directory for CSV files: {}", indexingDataDir);
        
        // Collect all CSV files
        List<Path> csvFiles = new ArrayList<>();
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexingDataDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    csvFiles.add(csvFile);
                    globalReport.incrementCSVFilesQueued();
                }
            }
        } catch (IOException e) {
            logger.error("Error reading Indexing\\Data directory: {}", indexingDataDir, e);
            return 0;
        }
        
        if (csvFiles.isEmpty()) {
            logger.warn("No CSV files found in Indexing\\Data directory: {}", indexingDataDir);
            return 0;
        }
        
        logger.info("Found {} CSV file(s) to process with multi-threading", csvFiles.size());
        
        // Process each CSV file in parallel
        for (Path csvFile : csvFiles) {
            final String fileName = csvFile.getFileName().toString();
            
            executor.submit(() -> {
                try {
                    logger.info("Thread-{}: Processing CSV file: {}", 
                               Thread.currentThread().getId(), fileName);
                    
                    // Process the CSV file (Steps 3-5)
                    Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                    
                    // Record success in global report
                    globalReport.recordCSVFileSuccess(fileName, result);
                    
                    logger.info("Thread-{}: Completed CSV file: {}", 
                               Thread.currentThread().getId(), fileName);
                    
                } catch (Exception e) {
                    logger.error("Thread-{}: Error processing CSV file: {}", 
                                Thread.currentThread().getId(), fileName, e);
                    globalReport.recordCSVFileFailure(fileName, e.getMessage());
                }
            });
        }
        
        // Wait for all tasks to complete
        executor.shutdown();
        try {
            logger.info("Waiting for all CSV files to complete processing...");
            executor.awaitTermination(24, TimeUnit.HOURS); // Max 24 hours
            logger.info("All CSV files processing completed");
        } catch (InterruptedException e) {
            logger.error("Processing interrupted", e);
            executor.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        return csvFiles.size();
    }
}


package com.wawanesa.ace.index.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // CSV file-level counters
    private final AtomicInteger totalCSVFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesFailed = new AtomicInteger(0);
    
    // Claim-level counters
    private final AtomicInteger totalClaimsProcessed = new AtomicInteger(0);
    
    // Document-level counters
    private final AtomicLong totalDocumentsProcessed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexFailed = new AtomicLong(0);
    private final AtomicLong totalDocumentsPackaged = new AtomicLong(0);
    
    // Batch counters
    private final AtomicInteger totalBatchesCreated = new AtomicInteger(0);
    private final AtomicInteger totalPackagingFilesCreated = new AtomicInteger(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a CSV file has been queued for processing
     */
    public void incrementCSVFilesQueued() {
        totalCSVFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful CSV file processing
     */
    public void recordCSVFileSuccess(String fileName, Utils.ProcessingResult result) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesSuccess.incrementAndGet();
        
        totalClaimsProcessed.addAndGet(result.totalClaimNumbers);
        totalDocumentsProcessed.addAndGet(result.totalDocuments);
        totalDocumentsIndexed.addAndGet(result.indexedSuccessCount);
        totalDocumentsIndexFailed.addAndGet(result.indexedFailureCount);
        totalBatchesCreated.addAndGet(result.batchIDsCreated.size());
        totalPackagingFilesCreated.addAndGet(result.packagingFilesCreated);
        
        logger.info("CSV completed: {} | Claims={}, Docs={}, Indexed={}, Failed={}, Batches={}, PkgFiles={}", 
                   fileName, result.totalClaimNumbers, result.totalDocuments, 
                   result.indexedSuccessCount, result.indexedFailureCount,
                   result.batchIDsCreated.size(), result.packagingFilesCreated);
    }
    
    /**
     * Record failed CSV file processing
     */
    public void recordCSVFileFailure(String fileName, String errorMessage) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesFailed.incrementAndGet();
        
        logger.error("CSV failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("CSV Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalCSVFilesQueued.get(), totalCSVFilesProcessed.get(), totalCSVFilesQueued.get(),
                   totalCSVFilesSuccess.get(), totalCSVFilesFailed.get());
        logger.info("Claims: Processed={}", totalClaimsProcessed.get());
        logger.info("Documents: Processed={}, Indexed={}, Failed={}", 
                   totalDocumentsProcessed.get(), totalDocumentsIndexed.get(), totalDocumentsIndexFailed.get());
        logger.info("Batches: Created={}, Packaging Files={}", 
                   totalBatchesCreated.get(), totalPackagingFilesCreated.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- CSV File Statistics ---\n");
        sb.append(String.format("Total CSV Files Queued: %d\n", totalCSVFilesQueued.get()));
        sb.append(String.format("Total CSV Files Processed: %d\n", totalCSVFilesProcessed.get()));
        sb.append(String.format("CSV Files Success: %d\n", totalCSVFilesSuccess.get()));
        sb.append(String.format("CSV Files Failed: %d\n", totalCSVFilesFailed.get()));
        sb.append("\n--- Claim Statistics ---\n");
        sb.append(String.format("Total Claims Processed: %d\n", totalClaimsProcessed.get()));
        sb.append("\n--- Document Statistics ---\n");
        sb.append(String.format("Total Documents Processed: %d\n", totalDocumentsProcessed.get()));
        sb.append(String.format("Total Documents Indexed: %d\n", totalDocumentsIndexed.get()));
        sb.append(String.format("Total Documents Index Failed: %d\n", totalDocumentsIndexFailed.get()));
        sb.append("\n--- Batch & Packaging Statistics ---\n");
        sb.append(String.format("Total Batches Created: %d\n", totalBatchesCreated.get()));
        sb.append(String.format("Total Packaging Files Created: %d\n", totalPackagingFilesCreated.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
            long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d docs/sec, %d docs/min\n", docsPerSecond, docsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataIndexPackaging_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, "SummaryReports", fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write CSV file statistics
            writer.newLine();
            writer.write("--- CSV File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total CSV Files Queued,%d", totalCSVFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Processed,%d", totalCSVFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Success,%d", totalCSVFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Failed,%d", totalCSVFilesFailed.get()));
            writer.newLine();
            
            // Write claim statistics
            writer.newLine();
            writer.write("--- Claim Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Claims Processed,%d", totalClaimsProcessed.get()));
            writer.newLine();
            
            // Write document statistics
            writer.newLine();
            writer.write("--- Document Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Documents Processed,%d", totalDocumentsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Indexed,%d", totalDocumentsIndexed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Index Failed,%d", totalDocumentsIndexFailed.get()));
            writer.newLine();
            
            // Write batch statistics
            writer.newLine();
            writer.write("--- Batch & Packaging Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Batches Created,%d", totalBatchesCreated.get()));
            writer.newLine();
            writer.write(String.format("Total Packaging Files Created,%d", totalPackagingFilesCreated.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
                long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Documents Per Second,%d", docsPerSecond));
                writer.newLine();
                writer.write(String.format("Documents Per Minute,%d", docsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
}



package com.wawanesa.ace.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Set;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.model.ClaimCenterDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int setDocCount;
    private boolean forceSingleSet;
    
    // SQL Queries loaded from properties
    private String selectDocumentsByClaimQuery;
    private String updateIndexingQuery;
    private String selectPackagingByBatchIdsQueryTemplate;
    private String updateProcessedQuery;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load set configuration (with default)
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        // Load force single set configuration (with default)
        String forceSingleSetStr = config.getProperty("app.batch.force.single.set");
        this.forceSingleSet = (forceSingleSetStr != null) ? 
            Boolean.parseBoolean(forceSingleSetStr) : Constants.DEFAULT_FORCE_SINGLE_SET;
        
        // Load SQL queries from properties and replace table name placeholder
        this.selectDocumentsByClaimQuery = config.getProperty("db.query.select.documents.by.claim")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateIndexingQuery = config.getProperty("db.query.update.indexing")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectPackagingByBatchIdsQueryTemplate = config.getProperty("db.query.select.packaging.by.batchids")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateProcessedQuery = config.getProperty("db.query.update.processed")
            .replace("%TABLE_NAME%", this.dbTableName);
        
        logger.info("Utils initialized with setDocCount={}, forceSingleSet={}", setDocCount, forceSingleSet);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
        
        /**
         * Merge another result into this one (for multi-threaded aggregation)
         */
        public void merge(ProcessingResult other) {
            this.totalClaimNumbers += other.totalClaimNumbers;
            this.totalDocuments += other.totalDocuments;
            this.indexedSuccessCount += other.indexedSuccessCount;
            this.indexedFailureCount += other.indexedFailureCount;
            this.packagingFilesCreated += other.packagingFilesCreated;
            this.batchIDsCreated.addAll(other.batchIDsCreated);
        }
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Process each claim individually:
     *   * Query DB for documents of one claim
     *   * Apply batching/indexing logic
     *   * Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} =====", csvFileName);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return result;
            }
            
            // STEP 3: Process each claim individually
            int claimIndex = 0;
            for (String claimNumber : claimNumbers) {
                claimIndex++;
                logger.info("Processing claim {}/{}: {}", claimIndex, claimNumbers.size(), claimNumber);
                
                // Query DB for documents of this specific claim
                List<ClaimCenterDocumentDTO> documents = fetchDocumentsForOneClaim(claimNumber);
                
                if (documents.isEmpty()) {
                    logger.warn("No documents found for claim: {} (isDataMerged=1)", claimNumber);
                    continue;
                }
                
                logger.info("Retrieved {} documents for claim: {}", documents.size(), claimNumber);
                result.totalDocuments += documents.size();
                
                // Apply batching and indexing logic (one batch per claim or multiple based on config)
                applyBatchingLogicForClaim(documents, result, claimNumber);
                
                // Update database with indexing metadata
                updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                
                logger.info("Completed claim: {} - Batches created: {}", claimNumber, 
                           result.batchIDsCreated.size() - (claimIndex - 1));
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLACE_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive, supports both camelCase and UNDERSCORE formats)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replace("\"", "");
                    String normalizedHeader = header.toUpperCase().replace("_", "");
                    
                    // Support both "claimNumber" and "CLAIM_NUMBER" formats
                    if (normalizedHeader.equals("CLAIMNUMBER")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' or 'CLAIM_NUMBER' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    logger.error("Supported formats: claimNumber, ClaimNumber, CLAIM_NUMBER, claim_number");
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents of a single claim with isDataMerged = 1
     * This method is called per claim to support one-batch-per-claim logic
     */
    private List<ClaimCenterDocumentDTO> fetchDocumentsForOneClaim(String claimNumber) throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectDocumentsByClaimQuery)) {
            
            pstmt.setString(1, claimNumber);
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    
                    // Packaging fields
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    documents.add(dto);
                }
            }
        }
        
        logger.debug("Fetched {} documents for claimNumber: {}", documents.size(), claimNumber);
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents for a single claim
     * 
     * Case 1: forceSingleSet = true
     *   - Create multiple batches, each with 1 set (max setDocCount docs per batch)
     *   - Example: 27 docs → Batch1(25 docs, 1 set) + Batch2(2 docs, 1 set)
     * 
     * Case 2: forceSingleSet = false
     *   - Create one batch per claim with multiple sets (max setDocCount docs per set)
     *   - Example: 27 docs → Batch1(27 docs, Set1: 25 docs + Set2: 2 docs)
     * 
     * @param documents List of documents for this claim
     * @param result ProcessingResult to track batch IDs
     * @param claimNumber Current claim number being processed
     */
    private void applyBatchingLogicForClaim(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        logger.debug("Applying batching logic for claim: {} with {} documents (forceSingleSet={})", 
                    claimNumber, totalDocs, forceSingleSet);
        
        if (forceSingleSet) {
            // CASE 1: Create multiple batches, each with 1 set (max setDocCount docs per batch)
            applyForceSingleSetLogic(documents, result, claimNumber);
        } else {
            // CASE 2: Create one batch per claim with multiple sets
            applyMultiSetPerBatchLogic(documents, result, claimNumber);
        }
    }
    
    /**
     * Case 1: Force single set per batch
     * - Create multiple batches for the claim
     * - Each batch contains exactly 1 set (max setDocCount docs)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 25 docs (setID=1, setDocCount=25, batchDocCount=25)
     *   Batch 2: 2 docs (setID=1, setDocCount=2, batchDocCount=2)
     */
    private void applyForceSingleSetLogic(List<ClaimCenterDocumentDTO> documents, 
                                          ProcessingResult result, 
                                          String claimNumber) {
        
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        int batchNumber = 1;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size (max setDocCount)
            int currentBatchSize = Math.min(setDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID();
            result.batchIDsCreated.add(batchID);
            
            logger.info("Claim: {} - Batch {}: batchID={}, docs={}, sets=1", 
                       claimNumber, batchNumber, batchID, currentBatchSize);
            
            // Process documents in current batch (single set)
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata (always setID = 1 for this case)
                doc.setSetID(1);
                doc.setSetDocCount(currentBatchSize);
                doc.setSetDocIndex(setDocIndex);
                
                setDocIndex++;
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
            batchNumber++;
        }
        
        logger.info("Claim: {} - Created {} batches (forceSingleSet=true)", claimNumber, batchNumber - 1);
    }
    
    /**
     * Case 2: Multiple sets per batch
     * - Create one batch for the claim
     * - Batch contains multiple sets (max setDocCount docs per set)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 27 docs (batchDocCount=27)
     *     - Set 1: setID=1, setDocCount=25, setDocIndex=1-25
     *     - Set 2: setID=2, setDocCount=2, setDocIndex=1-2
     */
    private void applyMultiSetPerBatchLogic(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        
        // Generate unique batch ID and job ID (one per claim)
        String batchID = generateBatchID();
        String jobID = generateJobID();
        result.batchIDsCreated.add(batchID);
        
        // Calculate number of sets needed
        int numberOfSets = (int) Math.ceil((double) totalDocs / setDocCount);
        
        logger.info("Claim: {} - Batch: batchID={}, docs={}, sets={}", 
                   claimNumber, batchID, totalDocs, numberOfSets);
        
        // Process all documents in this single batch
        int setID = 1;
        int setDocIndex = 1;
        int docsInCurrentSet = 0;
        
        for (ClaimCenterDocumentDTO doc : documents) {
            docsInCurrentSet++;
            
            // Calculate set doc count for current set
            int remainingDocs = totalDocs - ((setID - 1) * setDocCount);
            int currentSetDocCount = Math.min(setDocCount, remainingDocs);
            
            // Set batch-level metadata
            doc.setBatchID(batchID);
            doc.setJobID(jobID);
            doc.setBatchDocCount(totalDocs);
            
            // Set set-level metadata
            doc.setSetID(setID);
            doc.setSetDocCount(currentSetDocCount);
            doc.setSetDocIndex(setDocIndex);
            
            // Increment set document index
            setDocIndex++;
            
            // Move to next set if current set is full
            if (setDocIndex > setDocCount) {
                logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
                setID++;
                setDocIndex = 1;
                docsInCurrentSet = 0;
            }
        }
        
        // Log last set if it has documents
        if (docsInCurrentSet > 0) {
            logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
        }
        
        logger.info("Claim: {} - Created 1 batch with {} sets (forceSingleSet=false)", claimNumber, numberOfSets);
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID() {
        return Constants.JOB_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata using JDBC batch processing
     * Falls back to individual UPDATEs if batch fails
     */
    private void updateDatabaseWithIndexing(List<ClaimCenterDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        // Get batch size from config (default 1000)
        String batchSizeStr = config.getProperty("db.indexing.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        try {
            // Try batch UPDATE first (fast path)
            updateDatabaseWithIndexingBatch(documents, successLines, failedLines, result, batchSize);
            
        } catch (SQLException e) {
            logger.warn("Batch UPDATE failed, falling back to individual UPDATEs: {}", e.getMessage());
            // Fallback to individual UPDATEs
            updateDatabaseWithIndexingIndividual(documents, successLines, failedLines, result);
        }
    }
    
    /**
     * Batch UPDATE implementation (100x faster than individual UPDATEs)
     */
    private void updateDatabaseWithIndexingBatch(List<ClaimCenterDocumentDTO> documents,
                                                 List<String> successLines,
                                                 List<String> failedLines,
                                                 ProcessingResult result,
                                                 int batchSize) throws SQLException {
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            conn.setAutoCommit(false);
            int count = 0;
            int batchStartIndex = 0;
            
            for (int i = 0; i < documents.size(); i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                pstmt.setInt(1, doc.getBatchDocCount());
                pstmt.setString(2, doc.getBatchID());
                pstmt.setString(3, doc.getJobID());
                pstmt.setInt(4, doc.getSetDocCount());
                pstmt.setInt(5, doc.getSetID());
                pstmt.setInt(6, doc.getSetDocIndex());
                pstmt.setString(7, doc.getExternalID());
                pstmt.addBatch();
                count++;
                
                // Execute batch when size reached
                if (count % batchSize == 0) {
                    int[] results = pstmt.executeBatch();
                    conn.commit();
                    
                    // Track results
                    for (int j = 0; j < results.length; j++) {
                        ClaimCenterDocumentDTO batchDoc = documents.get(batchStartIndex + j);
                        if (results[j] > 0) {
                            result.indexedSuccessCount++;
                            successLines.add(batchDoc.getExternalID());
                        } else {
                            result.indexedFailureCount++;
                            failedLines.add(batchDoc.getExternalID() + "|No record found");
                        }
                    }
                    
                    logger.debug("Executed batch of {} indexing UPDATEs", count);
                    batchStartIndex = i + 1;
                }
            }
            
            // Execute remaining batch
            if (count % batchSize != 0) {
                int[] results = pstmt.executeBatch();
                conn.commit();
                
                // Track results for remaining batch
                int remainingStart = (count / batchSize) * batchSize;
                for (int j = 0; j < results.length; j++) {
                    ClaimCenterDocumentDTO batchDoc = documents.get(remainingStart + j);
                    if (results[j] > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(batchDoc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(batchDoc.getExternalID() + "|No record found");
                    }
                }
                
                logger.debug("Executed final batch of {} indexing UPDATEs", count % batchSize);
            }
            
            logger.info("Batch indexing UPDATE completed: {} documents indexed, {} failed", 
                       result.indexedSuccessCount, result.indexedFailureCount);
        }
    }
    
    /**
     * Individual UPDATE fallback (for when batch fails)
     */
    private void updateDatabaseWithIndexingIndividual(List<ClaimCenterDocumentDTO> documents,
                                                      List<String> successLines,
                                                      List<String> failedLines,
                                                      ProcessingResult result) {
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            conn.setAutoCommit(false);
            
            for (ClaimCenterDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Individual indexing UPDATE completed (fallback)");
            
        } catch (SQLException e) {
            logger.error("Individual UPDATE fallback also failed", e);
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replace(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        // Replace the %IN_CLAUSE% placeholder with the generated IN clause
        String selectSQL = selectPackagingByBatchIdsQueryTemplate.replace("%IN_CLAUSE%", inClause.toString());
        
        List<ClaimCenterDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<ClaimCenterDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (ClaimCenterDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(ClaimCenterDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<ClaimCenterDocumentDTO> documents) throws SQLException {
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateProcessedQuery)) {
            
            conn.setAutoCommit(false);
            Date currentDate = new Date();
            
            for (ClaimCenterDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLACE_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replace(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
}


package com.wawanesa.ace.index.model;

import java.util.Date;

/**
 * Data Transfer Object for ClaimCenter Document
 * Used for indexing, batching, and packaging document metadata
 */
public class ClaimCenterDocumentDTO {
    
    // ==================== CORE FIELDS ====================
    private String externalID;          // Unique identifier (Primary key for updates)
    private String claimNumber;         // Claim number (used for grouping)
    private String claimID;
    private String gwDocumentID;
    
    // ==================== INDEXING FIELDS (Updated during Step 3-4) ====================
    private Integer batchDocCount;      // Max documents per batch (e.g., 500)
    private String batchID;             // Unique batch ID: wawa_<timestamp>
    private String jobID;               // Job ID: wawa_migrate_<timestamp>
    private Integer setDocCount;        // Max documents per set (e.g., 25)
    private Integer setID;              // Set ID within batch (1, 2, 3...)
    private Integer setDocIndex;        // Document index within set (1-25)
    private Boolean isIndexed;          // Flag: indexing completed
    private Boolean isDataMerged;       // Flag: data merge completed (query filter)
    private Boolean isProcessed;        // Flag: packaging completed
    private Date dateProcessed;         // Timestamp of packaging
    
    // ==================== PACKAGING OUTPUT FIELDS (Step 5) ====================
    private String amount;
    private String author;
    private String claimant;
    private String coverage;
    private String customerID;
    private String documentDescription;
    private String documentSubtype;
    private String documentTitle;
    private String documentType;
    private String doNotCreateActivity;
    private String duplicate;
    private String exposureID;
    private String hidden;
    private String inputMethod;
    private String insuredName;
    private String mimeType;
    private String origDateCreated;
    private String policyNumber;
    private String primaryMembershipNumber;
    private String reviewed;
    private String sensitive;
    private String contentFilePath;
    private String contentRetrievalName;
    
    // Metadata
    private String csvFileName;
    
    // ==================== CONSTRUCTORS ====================
    public ClaimCenterDocumentDTO() {
    }
    
    // ==================== GETTERS AND SETTERS ====================
    
    public String getExternalID() {
        return externalID;
    }
    
    public void setExternalID(String externalID) {
        this.externalID = externalID;
    }
    
    public String getClaimNumber() {
        return claimNumber;
    }
    
    public void setClaimNumber(String claimNumber) {
        this.claimNumber = claimNumber;
    }
    
    public String getClaimID() {
        return claimID;
    }
    
    public void setClaimID(String claimID) {
        this.claimID = claimID;
    }
    
    public String getGwDocumentID() {
        return gwDocumentID;
    }
    
    public void setGwDocumentID(String gwDocumentID) {
        this.gwDocumentID = gwDocumentID;
    }
    
    public Integer getBatchDocCount() {
        return batchDocCount;
    }
    
    public void setBatchDocCount(Integer batchDocCount) {
        this.batchDocCount = batchDocCount;
    }
    
    public String getBatchID() {
        return batchID;
    }
    
    public void setBatchID(String batchID) {
        this.batchID = batchID;
    }
    
    public String getJobID() {
        return jobID;
    }
    
    public void setJobID(String jobID) {
        this.jobID = jobID;
    }
    
    public Integer getSetDocCount() {
        return setDocCount;
    }
    
    public void setSetDocCount(Integer setDocCount) {
        this.setDocCount = setDocCount;
    }
    
    public Integer getSetID() {
        return setID;
    }
    
    public void setSetID(Integer setID) {
        this.setID = setID;
    }
    
    public Integer getSetDocIndex() {
        return setDocIndex;
    }
    
    public void setSetDocIndex(Integer setDocIndex) {
        this.setDocIndex = setDocIndex;
    }
    
    public Boolean getIsIndexed() {
        return isIndexed;
    }
    
    public void setIsIndexed(Boolean isIndexed) {
        this.isIndexed = isIndexed;
    }
    
    public Boolean getIsDataMerged() {
        return isDataMerged;
    }
    
    public void setIsDataMerged(Boolean isDataMerged) {
        this.isDataMerged = isDataMerged;
    }
    
    public Boolean getIsProcessed() {
        return isProcessed;
    }
    
    public void setIsProcessed(Boolean isProcessed) {
        this.isProcessed = isProcessed;
    }
    
    public Date getDateProcessed() {
        return dateProcessed;
    }
    
    public void setDateProcessed(Date dateProcessed) {
        this.dateProcessed = dateProcessed;
    }
    
    public String getAmount() {
        return amount;
    }
    
    public void setAmount(String amount) {
        this.amount = amount;
    }
    
    public String getAuthor() {
        return author;
    }
    
    public void setAuthor(String author) {
        this.author = author;
    }
    
    public String getClaimant() {
        return claimant;
    }
    
    public void setClaimant(String claimant) {
        this.claimant = claimant;
    }
    
    public String getCoverage() {
        return coverage;
    }
    
    public void setCoverage(String coverage) {
        this.coverage = coverage;
    }
    
    public String getCustomerID() {
        return customerID;
    }
    
    public void setCustomerID(String customerID) {
        this.customerID = customerID;
    }
    
    public String getDocumentDescription() {
        return documentDescription;
    }
    
    public void setDocumentDescription(String documentDescription) {
        this.documentDescription = documentDescription;
    }
    
    public String getDocumentSubtype() {
        return documentSubtype;
    }
    
    public void setDocumentSubtype(String documentSubtype) {
        this.documentSubtype = documentSubtype;
    }
    
    public String getDocumentTitle() {
        return documentTitle;
    }
    
    public void setDocumentTitle(String documentTitle) {
        this.documentTitle = documentTitle;
    }
    
    public String getDocumentType() {
        return documentType;
    }
    
    public void setDocumentType(String documentType) {
        this.documentType = documentType;
    }
    
    public String getDoNotCreateActivity() {
        return doNotCreateActivity;
    }
    
    public void setDoNotCreateActivity(String doNotCreateActivity) {
        this.doNotCreateActivity = doNotCreateActivity;
    }
    
    public String getDuplicate() {
        return duplicate;
    }
    
    public void setDuplicate(String duplicate) {
        this.duplicate = duplicate;
    }
    
    public String getExposureID() {
        return exposureID;
    }
    
    public void setExposureID(String exposureID) {
        this.exposureID = exposureID;
    }
    
    public String getHidden() {
        return hidden;
    }
    
    public void setHidden(String hidden) {
        this.hidden = hidden;
    }
    
    public String getInputMethod() {
        return inputMethod;
    }
    
    public void setInputMethod(String inputMethod) {
        this.inputMethod = inputMethod;
    }
    
    public String getInsuredName() {
        return insuredName;
    }
    
    public void setInsuredName(String insuredName) {
        this.insuredName = insuredName;
    }
    
    public String getMimeType() {
        return mimeType;
    }
    
    public void setMimeType(String mimeType) {
        this.mimeType = mimeType;
    }
    
    public String getOrigDateCreated() {
        return origDateCreated;
    }
    
    public void setOrigDateCreated(String origDateCreated) {
        this.origDateCreated = origDateCreated;
    }
    
    public String getPolicyNumber() {
        return policyNumber;
    }
    
    public void setPolicyNumber(String policyNumber) {
        this.policyNumber = policyNumber;
    }
    
    public String getPrimaryMembershipNumber() {
        return primaryMembershipNumber;
    }
    
    public void setPrimaryMembershipNumber(String primaryMembershipNumber) {
        this.primaryMembershipNumber = primaryMembershipNumber;
    }
    
    public String getReviewed() {
        return reviewed;
    }
    
    public void setReviewed(String reviewed) {
        this.reviewed = reviewed;
    }
    
    public String getSensitive() {
        return sensitive;
    }
    
    public void setSensitive(String sensitive) {
        this.sensitive = sensitive;
    }
    
    public String getContentFilePath() {
        return contentFilePath;
    }
    
    public void setContentFilePath(String contentFilePath) {
        this.contentFilePath = contentFilePath;
    }
    
    public String getContentRetrievalName() {
        return contentRetrievalName;
    }
    
    public void setContentRetrievalName(String contentRetrievalName) {
        this.contentRetrievalName = contentRetrievalName;
    }
    
    public String getCsvFileName() {
        return csvFileName;
    }
    
    public void setCsvFileName(String csvFileName) {
        this.csvFileName = csvFileName;
    }
    
    /**
     * Generates pipe-delimited packaging output line in specified column order
     * Order: amount|author|batchDocCount|batchID|claimant|claimID|claimNumber|coverage|
     *        customerID|documentDescription|documentSubtype|documentTitle|documentType|
     *        doNotCreateActivity|duplicate|exposureID|gwDocumentID|hidden|inputMethod|
     *        insuredName|jobID|mimeType|OrigDateCreated|policyNumber|primaryMembershipNumber|
     *        reviewed|sensitive|setDocCount|setID|SetDocIndex|ClbSecurityController|
     *        contentFilePath|contentRetrievalName
     */
    public String toPackagingCSVLine() {
        return String.join("|",
            nullSafe(amount),
            nullSafe(author),
            nullSafe(batchDocCount),
            nullSafe(batchID),
            nullSafe(claimant),
            nullSafe(claimID),
            nullSafe(claimNumber),
            nullSafe(coverage),
            nullSafe(customerID),
            nullSafe(documentDescription),
            nullSafe(documentSubtype),
            nullSafe(documentTitle),
            nullSafe(documentType),
            nullSafe(doNotCreateActivity),
            nullSafe(duplicate),
            nullSafe(exposureID),
            nullSafe(gwDocumentID),
            nullSafe(hidden),
            nullSafe(inputMethod),
            nullSafe(insuredName),
            nullSafe(jobID),
            nullSafe(mimeType),
            nullSafe(origDateCreated),
            nullSafe(policyNumber),
            nullSafe(primaryMembershipNumber),
            nullSafe(reviewed),
            nullSafe(sensitive),
            nullSafe(setDocCount),
            nullSafe(setID),
            nullSafe(setDocIndex),
            nullSafe(contentFilePath),
            nullSafe(contentRetrievalName)
        );
    }
    
    /**
     * Returns packaging CSV header in correct column order
     */
    public static String getPackagingCSVHeader() {
        return "amount|author|batchDocCount|batchID|claimant|claimID|claimNumber|coverage|" +
               "customerID|documentDescription|documentSubtype|documentTitle|documentType|" +
               "doNotCreateActivity|duplicate|exposureID|gwDocumentID|hidden|inputMethod|" +
               "insuredName|jobID|mimeType|OrigDateCreated|policyNumber|primaryMembershipNumber|" +
               "reviewed|sensitive|setDocCount|setID|SetDocIndex|" +
               "contentFilePath|contentRetrievalName";
    }
    
    private String nullSafe(Object value) {
        return value == null ? "" : value.toString();
    }
    
    @Override
    public String toString() {
        return "ClaimCenterDocumentDTO{" +
                "externalID='" + externalID + '\'' +
                ", claimNumber='" + claimNumber + '\'' +
                ", batchID='" + batchID + '\'' +
                ", setID=" + setID +
                ", setDocIndex=" + setDocIndex +
                '}';
    }
}



package com.wawanesa.ace.index.constants;

/**
 * Constants for CCDataIndexAndPackagingUtility
 * Defines folder structure and batch/set configuration
 */
public class Constants {

    // ==================== FOLDER STRUCTURE ====================
    // Root folders under base path (D:\Rameshwar\ClaimCenterDataMerge)
    public static final String COMPLETED_SOURCE_FOLDER = "Completed";      // Source folder from CCDataMergeUtility
    public static final String INDEXING_FOLDER         = "Indexing";       // Main indexing folder
    public static final String PACKAGING_FOLDER        = "Packaging";      // Output folder for packaged CSV files
    
    // Sub-folders under Indexing folder
    public static final String DATA_FOLDER             = "Data";           // CSV files being indexed
    public static final String ARCHIVE_FOLDER          = "Archive";        // Archived CSV files after processing
    public static final String COMPLETED_FOLDER        = "Completed";      // Success tracking files
    public static final String FAILED_FOLDER           = "Failed";         // Failed tracking files
    
    // ==================== BATCH & SET CONFIGURATION ====================
    // Maximum documents per set within a batch (configurable)
    public static final int DEFAULT_SET_DOC_COUNT = 25;
    
    // Force single set per batch (configurable)
    // true = Create multiple batches, each with 1 set (max 25 docs per batch)
    // false = Create one batch per claim with multiple sets (max 25 docs per set)
    public static final boolean DEFAULT_FORCE_SINGLE_SET = false;
    
    // Maximum rows per packaging CSV file
    public static final int MAX_PACKAGING_FILE_ROWS = 50000;
    
    // ==================== BATCH ID GENERATION ====================
    // BatchID format: wawa_<yyyyMMddHHmmssSSSSS>
    public static final String BATCH_ID_PREFIX = "wawa_";
    
    // JobID format: wawa_migrate_<yyyyMMddHHmmssSSSSS>
    public static final String JOB_ID_PREFIX = "wawa_migrate_";
    
    // Packaging file name format: wawa_migration_docs_<yyyyMMddHHmmssSSSSS>.csv
    public static final String PACKAGING_FILE_PREFIX = "wawa_migration_docs_";
    public static final String PACKAGING_FILE_EXTENSION = ".csv";
    
    // ==================== FILE NAMING CONVENTIONS ====================
    public static final String INDEX_SUCCESS_SUFFIX = "_INDEX_SUCCESS.csv";
    public static final String INDEX_FAILED_SUFFIX  = "_INDEX_FAILED.csv";
    
    // ==================== CSV DELIMITERS ====================
    public static final String PIPE_DELIMITER = "|";
    
}


# ===========================================================
# CCDataIndexAndPackagingUtility - Production Configuration
# ===========================================================
# Last Updated: 2025-10-12
# Optimized for high-volume production processing (2M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base_path=D:/Rameshwar/ClaimCenterDataMerge/
app.skipHeaderRow=true
app.archive.after.processing=true

# ===== BATCHING CONFIGURATION =====
# Set document count: Maximum documents per set
app.set.doc.count=25

# Force single set per batch
# true = Multiple batches per claim (1 set per batch)
# false = One batch per claim (multiple sets per batch)
app.batch.force.single.set=false

# ===== PERFORMANCE TUNING =====
# Thread Pool Size: Number of parallel CSV file processors
# Recommended: 5-10 for production (multiple CSV files)
# Note: Each thread processes one complete CSV file
app.thread.pool.size=5

# Progress Monitoring: Log progress every N minutes
# Set to 0 to disable progress monitoring
# Recommended: 5 minutes for production visibility
app.progress.log.interval.minutes=5

# ===== DATABASE SETTINGS =====
db.serverName=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.dbName=Wawa_DMS_Conversion_UAT

# Database Connection Pool Size
# IMPORTANT: Should be >= app.thread.pool.size + 5 (buffer for overhead)
# Recommended: 15-20 for production with 5 threads
db.pool.size=15

# Target table for indexing and packaging operations
db.staging.claimcenterdbtable=Wawa_DMS_Conversion_UAT.[dbo].[Wawa_Doc_Migration_Transit_Data]

# Batch UPDATE Size for Indexing: Number of rows per JDBC batch
# CRITICAL FOR PERFORMANCE: Batch UPDATEs are 100x faster than individual UPDATEs
# Recommended: 1000-2000 rows per batch
# Default: 1000
db.indexing.batch.update.size=1000

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage of max heap)
# Warning logged when memory usage exceeds this threshold
app.memory.warning.threshold=80

# ===== NOTES =====
# Performance Estimates for 80K claims, 2M documents:
# - With batch UPDATEs (1000/batch): ~40-50 minutes total
# - Thread pool of 5: Processes 5 CSV files in parallel
# - Connection pool of 15: Adequate for 5 threads + overhead
#
# JVM Recommended Settings:
# -Xms2G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# Global Processing Report:
# - Written to: {app.base_path}/CCDataIndexPackaging_Summary_Report_{timestamp}.csv
# - Contains: Total CSV files, claims, documents, batches, packaging files
# - Includes: Success/failure counts, processing time, throughput
#
# Packaging Files:
# - Written to: {app.base_path}/Packaging/wawa_migration_docs_{timestamp}.csv
# - Max rows per file: 50,000
# - Format: Pipe-delimited, 33 columns

# ===== DATABASE QUERIES =====

# SELECT query for fetching documents for one claim (where isDataMerged = 1)
db.query.select.documents.by.claim=SELECT externalID, claimNumber, claimID, gwDocumentID, \
amount, author, claimant, coverage, customerID, \
documentDescription, documentSubtype, documentTitle, documentType, \
doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, \
insuredName, mimeType, OrigDateCreated, policyNumber, \
primaryMembershipNumber, reviewed, sensitive, \
contentFilePath, contentRetrievalName \
FROM %TABLE_NAME% \
WHERE claimNumber = ? AND isDataMerged = 1 AND claimID IS NOT NULL \
ORDER BY externalID

# UPDATE query for batch indexing (setting batch/set metadata)
db.query.update.indexing=UPDATE %TABLE_NAME% SET \
batchDocCount = ?, batchID = ?, jobID = ?, \
setDocCount = ?, setID = ?, SetDocIndex = ?, isIndexed = 1 \
WHERE externalID = ?

# SELECT query for fetching packaging documents by batchID list
db.query.select.packaging.by.batchids=SELECT externalID, claimNumber, claimID, gwDocumentID, \
amount, author, batchDocCount, batchID, claimant, coverage, customerID, \
documentDescription, documentSubtype, documentTitle, documentType, \
doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, \
insuredName, jobID, mimeType, OrigDateCreated, policyNumber, \
primaryMembershipNumber, reviewed, sensitive, setDocCount, setID, SetDocIndex, \
contentFilePath, contentRetrievalName \
FROM %TABLE_NAME% \
WHERE batchID IN (%IN_CLAUSE%) \
AND isDataMerged = 1 AND isIndexed = 1 \
ORDER BY batchID, setID, SetDocIndex

# UPDATE query for marking documents as processed
db.query.update.processed=UPDATE %TABLE_NAME% SET \
isProcessed = 1, DateProcessed = ? \
WHERE externalID = ?






========================================================================================================================


============================================================================
QUICK START GUIDE - ClaimID Sync System
============================================================================
Database: Wawa_DMS_Conversion_UAT
Location: D:\Rameshwar\NewScripts\

============================================================================
FIRST TIME SETUP (Run Once)
============================================================================

Step 1: Create Tables & Indexes
    Run in SQL Server Management Studio (in order):
    
    1. 01_Create_Table_GW_Conv_ClaimID.sql
    2. 02_Create_Table_Wawa_Doc_Migration_Transit_Data.sql
    3. 03_SP_Sync_ClaimID_From_GW_To_Transit.sql
    
    Time: 5-10 minutes
    
Step 2: Verify Setup
    SELECT name FROM sys.tables 
    WHERE name IN ('GW_Conv_ClaimID', 'Wawa_Doc_Migration_Transit_Data');
    
    Expected: 2 tables
    
    SELECT name FROM sys.procedures 
    WHERE name = 'SP_Sync_ClaimID_From_GW_To_Transit';
    
    Expected: 1 procedure

============================================================================
DAILY OPERATIONS
============================================================================

Step 1: Load Data (Java Utility)
    - Load mappings → GW_Conv_ClaimID table (~60K records)
    - Load documents → Wawa_Doc_Migration_Transit_Data table (~1M records)

Step 2: Run Sync (SQL)
    Execute: 04_Execute_Daily_Sync.sql
    
    Expected: 
    - Shows mapping data loaded today
    - Shows Transit records needing sync
    - Runs sync procedure
    - Shows completion statistics
    
    Time: 3-5 minutes for typical daily volume

Step 3: Verify (Quick Check)
    SELECT 
        COUNT(*) AS Total,
        SUM(CASE WHEN claimID IS NOT NULL THEN 1 ELSE 0 END) AS WithClaimID,
        SUM(CASE WHEN claimID IS NULL THEN 1 ELSE 0 END) AS WithoutClaimID
    FROM Wawa_Doc_Migration_Transit_Data;
    
    Goal: 99%+ should have claimID

============================================================================
TROUBLESHOOTING
============================================================================

Issue: No records synced
Solution:
    -- Check if GW data loaded today
    SELECT COUNT(*) FROM GW_Conv_ClaimID 
    WHERE CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE);
    
    -- If 0, run Java utility first

Issue: Sync is slow
Solution:
    -- Rebuild key index
    ALTER INDEX IX_Transit_ClaimNumber_NullClaimID 
    ON Wawa_Doc_Migration_Transit_Data REBUILD;

Issue: Some records stay NULL
Solution:
    -- Find records without mapping
    SELECT TOP 10 t.claimNumber, t.externalID
    FROM Wawa_Doc_Migration_Transit_Data t
    WHERE t.claimID IS NULL
      AND NOT EXISTS (SELECT 1 FROM GW_Conv_ClaimID g 
                      WHERE g.claimNumber = t.claimNumber);
    
    -- These need their mapping loaded into GW_Conv_ClaimID

============================================================================
QUICK REFERENCE
============================================================================

Table 1: GW_Conv_ClaimID
    Purpose: Claim mapping (claimID ↔ claimNumber)
    Primary Key: (claimNumber, claimID)
    Daily Volume: ~60K new mappings
    
Table 2: Wawa_Doc_Migration_Transit_Data
    Purpose: Document metadata
    Primary Key: (claimNumber, externalID)
    Current Volume: 25M+ records
    Daily Volume: ~1M new documents

Procedure: SP_Sync_ClaimID_From_GW_To_Transit
    Purpose: Update claimID in Transit from GW mapping
    Speed: ~8,000-10,000 records/second
    Usage: EXEC SP_Sync_ClaimID_From_GW_To_Transit;

============================================================================
PERFORMANCE EXPECTATIONS
============================================================================

Sync Times:
    100K records:  10-15 seconds
    1M records:    2-3 minutes
    10M records:   15-20 minutes
    25M records:   40-50 minutes

Success Rate:
    Excellent:     99%+ with claimID
    Good:          95-99% with claimID
    Acceptable:    90-95% with claimID
    Needs Review:  <90% with claimID

============================================================================
FILES
============================================================================

Setup (Run Once):
    01_Create_Table_GW_Conv_ClaimID.sql
    02_Create_Table_Wawa_Doc_Migration_Transit_Data.sql
    03_SP_Sync_ClaimID_From_GW_To_Transit.sql

Daily (Run Every Day):
    04_Execute_Daily_Sync.sql

Documentation:
    README.md         - Complete guide
    QUICK_START.txt   - This file

============================================================================
CONTACT
============================================================================

For detailed information: See README.md
For quick help: See this file (QUICK_START.txt)

============================================================================

*******************************************************************************************************************************************************
# ClaimID Sync System - Complete Setup Guide

**Database:** `Wawa_DMS_Conversion_UAT`  
**Purpose:** Sync ClaimID from GW mapping table to Transit data table  
**Daily Volume:** 60K mappings, 1M+ documents

---

## 📋 **System Overview**

### **Data Flow:**
```
Java Utility
    ↓ (loads daily data)
GW_Conv_ClaimID (60K mappings)
    ↓ (sync procedure)
Wawa_Doc_Migration_Transit_Data (25M+ documents)
    ↓ (updates claimID)
Ready for Processing
```

### **Tables:**
1. **GW_Conv_ClaimID** - Claim mapping table (claimID ↔ claimNumber)
2. **Wawa_Doc_Migration_Transit_Data** - Document transit table (needs claimID)

### **Process:**
- Java loads data into both tables
- SQL procedure syncs claimID between them
- Documents get associated with correct claimID

---

## 🚀 **Quick Start - First Time Setup**

### **Step 1: Create Tables & Indexes**
Run these scripts in order:

```sql
-- Script 1: Create GW mapping table
D:\Rameshwar\NewScripts\01_Create_Table_GW_Conv_ClaimID.sql

-- Script 2: Create/verify Transit table
D:\Rameshwar\NewScripts\02_Create_Table_Wawa_Doc_Migration_Transit_Data.sql

-- Script 3: Create sync procedure
D:\Rameshwar\NewScripts\03_SP_Sync_ClaimID_From_GW_To_Transit.sql
```

**Estimated Time:** 5-10 minutes

---

### **Step 2: Verify Setup**
```sql
-- Check tables exist
SELECT name FROM sys.tables 
WHERE name IN ('GW_Conv_ClaimID', 'Wawa_Doc_Migration_Transit_Data')
ORDER BY name;
-- Expected: 2 tables

-- Check procedure exists
SELECT name FROM sys.procedures 
WHERE name = 'SP_Sync_ClaimID_From_GW_To_Transit';
-- Expected: 1 procedure

-- Check indexes
SELECT 
    t.name AS TableName,
    COUNT(i.name) AS IndexCount
FROM sys.tables t
LEFT JOIN sys.indexes i ON t.object_id = i.object_id AND i.name IS NOT NULL
WHERE t.name IN ('GW_Conv_ClaimID', 'Wawa_Doc_Migration_Transit_Data')
GROUP BY t.name;
-- Expected: GW_Conv_ClaimID (4 indexes), Wawa_Doc_Migration_Transit_Data (7 indexes)
```

---

## 📅 **Daily Operations**

### **Daily Workflow:**

1. **Java:** Load mapping data → `GW_Conv_ClaimID`
2. **Java:** Load document data → `Wawa_Doc_Migration_Transit_Data`
3. **SQL:** Run sync script

```sql
-- Execute daily sync
D:\Rameshwar\NewScripts\04_Execute_Daily_Sync.sql
```

### **Expected Output:**
```
============================================================================
DAILY SYNC WORKFLOW
============================================================================
Date: 2025-10-21
Start Time: 2025-10-21 09:00:00.000
============================================================================

--- Step 1: Verify GW Mapping Data ---
GW_Conv_ClaimID Table Statistics:
  Total Mappings: 125000
  Loaded Today: 60000
  ✓ Fresh mapping data available

--- Step 2: Verify Transit Data ---
Wawa_Doc_Migration_Transit_Data Table Statistics:
  Total Records: 25500000
  With ClaimID: 24000000 (94.12%)
  Without ClaimID: 1500000 (5.88%)
  Can be matched today: 1450000

--- Step 3: Execute ClaimID Sync ---
============================================================================
SYNC CLAIMID: GW_Conv_ClaimID → Wawa_Doc_Migration_Transit_Data
============================================================================
Step 1: Analyzing records that need ClaimID...
  Records with NULL claimID: 1400000
  Records with mismatched claimID: 50000
  Total records needing update: 1450000

Step 2: Updating ClaimID in batches...
  Batch    1: Updated   100000 records | Total:   100000 (6.90%)
  Batch    2: Updated   100000 records | Total:   200000 (13.79%)
  ...
  Batch   15: Updated    50000 records | Total:  1450000 (100.00%)

✓ All batches completed

Step 3: Verification & Statistics
Transit Table Statistics (After Sync):
  Total Records: 25500000
  Records WITH ClaimID: 25450000 (99.80%)
  Records WITHOUT ClaimID: 50000 (0.20%)

Sync Summary:
  Records Updated: 1450000
  Batches Processed: 15
  Duration: 180 seconds
  Performance: ~8055 records/second

============================================================================
✓ SYNC COMPLETED SUCCESSFULLY
============================================================================

============================================================================
✓ DAILY WORKFLOW COMPLETED SUCCESSFULLY
============================================================================
Total Duration: 185 seconds
```

---

## 📊 **Table Structures**

### **1. GW_Conv_ClaimID**

```sql
CREATE TABLE GW_Conv_ClaimID (
    claimID bigint NOT NULL,
    claimNumber varchar(40) NOT NULL,
    claimMigratedOn datetime2(3) NULL,
    dataLoadedOn datetime2(7) NOT NULL DEFAULT SYSDATETIME(),
    
    PRIMARY KEY (claimNumber, claimID)
);
```

**Columns:**
- `claimID` - GW claim identifier (bigint)
- `claimNumber` - Business claim number (varchar(40))
- `claimMigratedOn` - When claim migrated to GW
- `dataLoadedOn` - Auto-populated load timestamp

**Primary Key:** `(claimNumber, claimID)` - Composite unique  
**Indexes:** 4 total (1 PK + 3 supporting)  
**Daily Volume:** ~60K new mappings

---

### **2. Wawa_Doc_Migration_Transit_Data**

```sql
CREATE TABLE Wawa_Doc_Migration_Transit_Data (
    claimNumber varchar(20) NOT NULL,
    externalID varchar(100) NOT NULL,
    claimID bigint NULL,  -- Synced from GW_Conv_ClaimID
    documentTitle varchar(500) NULL,
    documentType varchar(50) NULL,
    -- ... (50+ other columns)
    
    PRIMARY KEY (claimNumber, externalID)
);
```

**Key Columns:**
- `claimNumber` - Links to GW_Conv_ClaimID.claimNumber
- `externalID` - Unique document identifier
- `claimID` - **Synced via procedure** from GW_Conv_ClaimID

**Primary Key:** `(claimNumber, externalID)` - Composite unique  
**Indexes:** 7 total (1 PK + 6 supporting)  
**Compression:** PAGE (for 25M+ records)  
**Current Volume:** 25M+ records  
**Daily Volume:** ~1M new documents

---

## 🔍 **Stored Procedure Details**

### **SP_Sync_ClaimID_From_GW_To_Transit**

**Purpose:** Updates `claimID` in Transit table from GW mapping table

**Logic:**
```sql
UPDATE Wawa_Doc_Migration_Transit_Data t
SET t.claimID = g.claimID
FROM Wawa_Doc_Migration_Transit_Data t
INNER JOIN GW_Conv_ClaimID g ON t.claimNumber = g.claimNumber
WHERE t.claimID IS NULL OR t.claimID != g.claimID;
```

**Performance:**
- Batch Size: 100K records per batch
- Speed: ~8,000-10,000 records/second
- Estimates:
  - 1M records: ~2-3 minutes
  - 10M records: ~15-20 minutes
  - 25M records (full table): ~40-50 minutes

**Features:**
- ✅ Batch processing (avoids lock escalation)
- ✅ Progress reporting (shows % complete)
- ✅ Error recovery (each batch is a transaction)
- ✅ Statistics & verification
- ✅ Data quality checks

**Usage:**
```sql
EXEC SP_Sync_ClaimID_From_GW_To_Transit;
```

---

## 🔧 **Index Reference**

### **GW_Conv_ClaimID Indexes:**

| Index Name | Type | Columns | Purpose |
|------------|------|---------|---------|
| PK_GW_Conv_ClaimID | CLUSTERED PK | claimNumber, claimID | Primary lookup |
| IX_GW_Conv_ClaimID_ClaimID | NON-CLUSTERED | claimID | Reverse lookup |
| IX_GW_Conv_ClaimID_DataLoadedOn | NON-CLUSTERED | dataLoadedOn DESC | Daily queries |
| IX_GW_Conv_ClaimID_ClaimMigratedOn | NON-CLUSTERED (Filtered) | claimMigratedOn DESC | Reporting |

### **Wawa_Doc_Migration_Transit_Data Indexes:**

| Index Name | Type | Columns | Purpose |
|------------|------|---------|---------|
| PK_Transit_ClaimNumber_ExternalID | CLUSTERED PK | claimNumber, externalID | Primary lookup |
| IX_Transit_ExternalID | NON-CLUSTERED | externalID | Document lookup |
| IX_Transit_ClaimID | NON-CLUSTERED (Filtered) | claimID | Reverse lookup |
| IX_Transit_ClaimNumber_NullClaimID | NON-CLUSTERED (Filtered) | claimNumber | **Sync optimization** |
| IX_Transit_BatchID | NON-CLUSTERED (Filtered) | batchID | Batch processing |
| IX_Transit_DateProcessed | NON-CLUSTERED (Filtered) | DateProcessed DESC | Reporting |
| IX_Transit_CC_Extract_LoadedDate | NON-CLUSTERED (Filtered) | CC_Extract_file_loaded_date | Daily tracking |

**Note:** Filtered indexes only index rows where the filter condition is true, improving performance and reducing index size.

---

## 📈 **Monitoring & Verification**

### **Check Daily Sync Status:**
```sql
-- Quick daily summary
SELECT 
    'Total Transit Records' AS Metric,
    COUNT(*) AS Value
FROM Wawa_Doc_Migration_Transit_Data
UNION ALL
SELECT 
    'Records WITH ClaimID',
    COUNT(*)
FROM Wawa_Doc_Migration_Transit_Data
WHERE claimID IS NOT NULL
UNION ALL
SELECT 
    'Records WITHOUT ClaimID',
    COUNT(*)
FROM Wawa_Doc_Migration_Transit_Data
WHERE claimID IS NULL
UNION ALL
SELECT 
    'GW Mappings Loaded Today',
    COUNT(*)
FROM GW_Conv_ClaimID
WHERE CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE);
```

### **Find Records Missing ClaimID:**
```sql
-- Records that could be matched but aren't
SELECT TOP 100
    t.claimNumber,
    t.externalID,
    t.documentTitle,
    g.claimID AS AvailableClaimID,
    g.dataLoadedOn AS MappingLoadDate
FROM Wawa_Doc_Migration_Transit_Data t
INNER JOIN GW_Conv_ClaimID g ON t.claimNumber = g.claimNumber
WHERE t.claimID IS NULL
ORDER BY g.dataLoadedOn DESC;
```

### **Check Sync Performance:**
```sql
-- Records synced by date (based on GW mapping load date)
SELECT 
    CAST(g.dataLoadedOn AS DATE) AS LoadDate,
    COUNT(*) AS RecordsSynced
FROM Wawa_Doc_Migration_Transit_Data t
INNER JOIN GW_Conv_ClaimID g ON t.claimNumber = g.claimNumber
WHERE t.claimID = g.claimID
GROUP BY CAST(g.dataLoadedOn AS DATE)
ORDER BY LoadDate DESC;
```

---

## 🚨 **Troubleshooting**

### **Issue 1: No records updated during sync**
**Symptoms:** Procedure runs but updates 0 records

**Causes:**
1. All records already have claimID
2. No matching claimNumbers between tables
3. Data not loaded yet

**Solutions:**
```sql
-- Check 1: Do records need claimID?
SELECT COUNT(*) AS RecordsNeedingClaimID
FROM Wawa_Doc_Migration_Transit_Data
WHERE claimID IS NULL;

-- Check 2: Are there matching claimNumbers?
SELECT COUNT(*) AS MatchableRecords
FROM Wawa_Doc_Migration_Transit_Data t
INNER JOIN GW_Conv_ClaimID g ON t.claimNumber = g.claimNumber
WHERE t.claimID IS NULL;

-- Check 3: Was GW data loaded today?
SELECT COUNT(*) AS GWRecordsToday
FROM GW_Conv_ClaimID
WHERE CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE);
```

---

### **Issue 2: Sync is slow**
**Symptoms:** Procedure takes longer than expected

**Causes:**
1. Missing/fragmented indexes
2. Lock contention
3. Large batch size

**Solutions:**
```sql
-- Check 1: Verify indexes exist
SELECT 
    t.name AS TableName,
    i.name AS IndexName,
    i.type_desc AS IndexType,
    ps.avg_fragmentation_in_percent AS Fragmentation
FROM sys.indexes i
INNER JOIN sys.tables t ON i.object_id = t.object_id
INNER JOIN sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'LIMITED') ps
    ON i.object_id = ps.object_id AND i.index_id = ps.index_id
WHERE t.name IN ('GW_Conv_ClaimID', 'Wawa_Doc_Migration_Transit_Data')
ORDER BY t.name, i.name;

-- Check 2: Rebuild fragmented indexes (if >30% fragmented)
ALTER INDEX IX_Transit_ClaimNumber_NullClaimID 
ON Wawa_Doc_Migration_Transit_Data 
REBUILD WITH (ONLINE = OFF);

-- Check 3: Update statistics
UPDATE STATISTICS GW_Conv_ClaimID WITH FULLSCAN;
UPDATE STATISTICS Wawa_Doc_Migration_Transit_Data WITH FULLSCAN;
```

---

### **Issue 3: ClaimID datatype mismatch**
**Symptoms:** Some records don't sync despite matching claimNumber

**Cause:** claimID is `bigint` in both tables - should not mismatch

**Verification:**
```sql
-- Verify data types match
SELECT 
    t.name AS TableName,
    c.name AS ColumnName,
    ty.name AS DataType,
    c.max_length,
    c.precision
FROM sys.columns c
INNER JOIN sys.tables t ON c.object_id = t.object_id
INNER JOIN sys.types ty ON c.user_type_id = ty.user_type_id
WHERE t.name IN ('GW_Conv_ClaimID', 'Wawa_Doc_Migration_Transit_Data')
  AND c.name IN ('claimID', 'claimNumber')
ORDER BY t.name, c.name;
```

---

### **Issue 4: Records still NULL after sync**
**Symptoms:** Some Transit records keep NULL claimID even after sync

**Cause:** No matching claimNumber in GW_Conv_ClaimID

**Investigation:**
```sql
-- Find Transit records without GW mapping
SELECT TOP 100
    t.claimNumber,
    t.externalID,
    t.documentTitle,
    t.CC_Extract_file_loaded_date
FROM Wawa_Doc_Migration_Transit_Data t
WHERE t.claimID IS NULL
  AND NOT EXISTS (
      SELECT 1 FROM GW_Conv_ClaimID g 
      WHERE g.claimNumber = t.claimNumber
  )
ORDER BY t.claimNumber;

-- Action: These records need their mapping data loaded into GW_Conv_ClaimID
```

---

## 📁 **File Reference**

```
D:\Rameshwar\NewScripts\
├── README.md                                          ← THIS FILE
│
├── 01_Create_Table_GW_Conv_ClaimID.sql               ← Run ONCE (setup)
│   Creates: GW_Conv_ClaimID table with 4 indexes
│
├── 02_Create_Table_Wawa_Doc_Migration_Transit_Data.sql  ← Run ONCE (setup)
│   Creates: Wawa_Doc_Migration_Transit_Data with 7 indexes
│
├── 03_SP_Sync_ClaimID_From_GW_To_Transit.sql        ← Run ONCE (setup)
│   Creates: Sync stored procedure
│
└── 04_Execute_Daily_Sync.sql                         ← Run DAILY
    Executes: Complete daily workflow
```

---

## ✅ **Deployment Checklist**

### **Initial Setup (One Time):**
- [ ] Run `01_Create_Table_GW_Conv_ClaimID.sql`
- [ ] Run `02_Create_Table_Wawa_Doc_Migration_Transit_Data.sql`
- [ ] Run `03_SP_Sync_ClaimID_From_GW_To_Transit.sql`
- [ ] Verify all indexes created successfully
- [ ] Test with sample data
- [ ] Document Java utility configuration

### **Daily Operations:**
- [ ] Java: Load GW mapping data
- [ ] Java: Load Transit document data
- [ ] SQL: Run `04_Execute_Daily_Sync.sql`
- [ ] Verify sync completion rate (>99%)
- [ ] Check for errors in output
- [ ] Monitor performance metrics

### **Weekly Maintenance:**
- [ ] Check index fragmentation
- [ ] Review records still missing claimID
- [ ] Verify data quality metrics
- [ ] Review error logs (if any)

---

## 🎯 **Success Criteria**

After setup and first sync:
- ✅ Both tables exist with all indexes
- ✅ Stored procedure created and executable
- ✅ Sync completes without errors
- ✅ 95%+ of Transit records have claimID
- ✅ Performance within expected range (~8K-10K records/sec)

---

## 📞 **Support Information**

### **Performance Expectations:**
- **Sync Speed:** 8,000-10,000 records/second
- **1M records:** 2-3 minutes
- **10M records:** 15-20 minutes
- **Full 25M table:** 40-50 minutes

### **Data Quality Targets:**
- **Excellent:** 99%+ records with claimID
- **Good:** 95-99% records with claimID
- **Acceptable:** 90-95% records with claimID
- **Needs Attention:** <90% records with claimID

---

**Document Version:** 1.0  
**Last Updated:** October 21, 2025  
**Database:** Wawa_DMS_Conversion_UAT  
**Status:** ✅ Production Ready



*******************************************************************************************************************************************************

USE [Wawa_DMS_Conversion_UAT];
GO

/*
============================================================================
Script: Create Table - GW_Conv_ClaimID
============================================================================
Purpose: 
    Create claim mapping table with optimized indexes for high-volume lookups
    Daily volume: ~60K new mappings

Table Structure:
    - claimID (bigint): Unique claim identifier
    - claimNumber (varchar(40)): Business claim number
    - claimMigratedOn (datetime2(3)): When claim was migrated
    - dataLoadedOn (datetime2(7)): When record was loaded (auto-populated)

Primary Key: 
    Composite key on (claimID, claimNumber)

Constraints:
    - dataLoadedOn defaults to current timestamp

Author: System Generated
Date: 2025-10-21
============================================================================
*/

PRINT '============================================================================';
PRINT 'CREATING TABLE: GW_Conv_ClaimID';
PRINT 'Database: Wawa_DMS_Conversion_UAT';
PRINT 'Start Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
PRINT '============================================================================';
PRINT '';

-- ============================================================================
-- Drop table if exists (for clean deployment)
-- ============================================================================
IF OBJECT_ID('dbo.GW_Conv_ClaimID', 'U') IS NOT NULL
BEGIN
    PRINT 'Dropping existing table...';
    DROP TABLE [dbo].[GW_Conv_ClaimID];
    PRINT '[OK] Existing table dropped';
    PRINT '';
END

-- ============================================================================
-- Create table with primary key and default constraint
-- ============================================================================
PRINT 'Creating table GW_Conv_ClaimID...';

CREATE TABLE [dbo].[GW_Conv_ClaimID] (
    [claimID] [bigint] NOT NULL,
    [claimNumber] [varchar](40) NOT NULL,
    [claimMigratedOn] [datetime2](3) NULL,
    [dataLoadedOn] [datetime2](7) NOT NULL DEFAULT SYSDATETIME(),
    
    -- Composite Primary Key (clustered for optimal JOIN performance)
    CONSTRAINT [PK_GW_Conv_ClaimID] PRIMARY KEY CLUSTERED 
    (
        [claimNumber] ASC,  -- Most common lookup is by claimNumber
        [claimID] ASC
    )
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        IGNORE_DUP_KEY = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 90  -- 10% free space for daily inserts (60K/day)
    )
);

PRINT '[OK] Table created';
PRINT '';

-- ============================================================================
-- Create performance indexes
-- ============================================================================
PRINT 'Creating performance indexes...';
PRINT '';

-- Index 1: Non-clustered index on claimID for reverse lookups
PRINT '  Creating index on claimID...';
CREATE NONCLUSTERED INDEX [IX_GW_Conv_ClaimID_ClaimID]
ON [dbo].[GW_Conv_ClaimID] ([claimID] ASC)
INCLUDE ([claimNumber], [claimMigratedOn])
WITH (
    PAD_INDEX = OFF,
    STATISTICS_NORECOMPUTE = OFF,
    SORT_IN_TEMPDB = OFF,
    DROP_EXISTING = OFF,
    ONLINE = OFF,
    ALLOW_ROW_LOCKS = ON,
    ALLOW_PAGE_LOCKS = ON,
    FILLFACTOR = 90
);
PRINT '  [OK] Index created: IX_GW_Conv_ClaimID_ClaimID';

-- Index 2: Non-clustered index on dataLoadedOn for daily processing queries
PRINT '  Creating index on dataLoadedOn...';
CREATE NONCLUSTERED INDEX [IX_GW_Conv_ClaimID_DataLoadedOn]
ON [dbo].[GW_Conv_ClaimID] ([dataLoadedOn] DESC)
INCLUDE ([claimID], [claimNumber], [claimMigratedOn])
WITH (
    PAD_INDEX = OFF,
    STATISTICS_NORECOMPUTE = OFF,
    SORT_IN_TEMPDB = OFF,
    DROP_EXISTING = OFF,
    ONLINE = OFF,
    ALLOW_ROW_LOCKS = ON,
    ALLOW_PAGE_LOCKS = ON,
    FILLFACTOR = 90
);
PRINT '  [OK] Index created: IX_GW_Conv_ClaimID_DataLoadedOn';

-- Index 3: Non-clustered index on claimMigratedOn for reporting
PRINT '  Creating index on claimMigratedOn...';
CREATE NONCLUSTERED INDEX [IX_GW_Conv_ClaimID_ClaimMigratedOn]
ON [dbo].[GW_Conv_ClaimID] ([claimMigratedOn] DESC)
INCLUDE ([claimID], [claimNumber])
WHERE [claimMigratedOn] IS NOT NULL
WITH (
    PAD_INDEX = OFF,
    STATISTICS_NORECOMPUTE = OFF,
    SORT_IN_TEMPDB = OFF,
    DROP_EXISTING = OFF,
    ONLINE = OFF,
    ALLOW_ROW_LOCKS = ON,
    ALLOW_PAGE_LOCKS = ON,
    FILLFACTOR = 90
);
PRINT '  [OK] Index created: IX_GW_Conv_ClaimID_ClaimMigratedOn (filtered)';

PRINT '';
PRINT '[OK] All indexes created';
PRINT '';

-- ============================================================================
-- Add extended properties (documentation)
-- ============================================================================
PRINT 'Adding table documentation...';

EXEC sys.sp_addextendedproperty 
    @name=N'MS_Description', 
    @value=N'Claim mapping table - maps claimID to claimNumber for GW integration' , 
    @level0type=N'SCHEMA',
    @level0name=N'dbo', 
    @level1type=N'TABLE',
    @level1name=N'GW_Conv_ClaimID';

EXEC sys.sp_addextendedproperty 
    @name=N'MS_Description', 
    @value=N'Unique claim identifier from GW system' , 
    @level0type=N'SCHEMA',
    @level0name=N'dbo', 
    @level1type=N'TABLE',
    @level1name=N'GW_Conv_ClaimID', 
    @level2type=N'COLUMN',
    @level2name=N'claimID';

EXEC sys.sp_addextendedproperty 
    @name=N'MS_Description', 
    @value=N'Business claim number (primary lookup key)' , 
    @level0type=N'SCHEMA',
    @level0name=N'dbo', 
    @level1type=N'TABLE',
    @level1name=N'GW_Conv_ClaimID', 
    @level2type=N'COLUMN',
    @level2name=N'claimNumber';

EXEC sys.sp_addextendedproperty 
    @name=N'MS_Description', 
    @value=N'Timestamp when claim was migrated to GW' , 
    @level0type=N'SCHEMA',
    @level0name=N'dbo', 
    @level1type=N'TABLE',
    @level1name=N'GW_Conv_ClaimID', 
    @level2type=N'COLUMN',
    @level2name=N'claimMigratedOn';

EXEC sys.sp_addextendedproperty 
    @name=N'MS_Description', 
    @value=N'Timestamp when record was loaded (auto-populated)' , 
    @level0type=N'SCHEMA',
    @level0name=N'dbo', 
    @level1type=N'TABLE',
    @level1name=N'GW_Conv_ClaimID', 
    @level2type=N'COLUMN',
    @level2name=N'dataLoadedOn';

PRINT '[OK] Documentation added';
PRINT '';

-- ============================================================================
-- Verification
-- ============================================================================
PRINT 'Verification:';
PRINT '';

-- Check table exists
IF OBJECT_ID('dbo.GW_Conv_ClaimID', 'U') IS NOT NULL
    PRINT '  [OK] Table created: GW_Conv_ClaimID';
ELSE
    PRINT '  ✗ ERROR: Table not created';

-- Check primary key
IF EXISTS (SELECT * FROM sys.indexes WHERE object_id = OBJECT_ID('dbo.GW_Conv_ClaimID') AND is_primary_key = 1)
    PRINT '  [OK] Primary key created: PK_GW_Conv_ClaimID';
ELSE
    PRINT '  ✗ ERROR: Primary key not created';

-- Check default constraint
IF EXISTS (SELECT * FROM sys.default_constraints WHERE parent_object_id = OBJECT_ID('dbo.GW_Conv_ClaimID') AND parent_column_id = COLUMNPROPERTY(OBJECT_ID('dbo.GW_Conv_ClaimID'), 'dataLoadedOn', 'ColumnId'))
    PRINT '  [OK] Default constraint created: dataLoadedOn';
ELSE
    PRINT '  ✗ ERROR: Default constraint not created';

-- Check indexes
DECLARE @IndexCount INT;
SELECT @IndexCount = COUNT(*)
FROM sys.indexes 
WHERE object_id = OBJECT_ID('dbo.GW_Conv_ClaimID') 
  AND name IS NOT NULL;

PRINT '  [OK] Indexes created: ' + CAST(@IndexCount AS VARCHAR) + ' indexes';

PRINT '';
PRINT '============================================================================';
PRINT '[OK] TABLE CREATION COMPLETED SUCCESSFULLY';
PRINT '============================================================================';
PRINT 'Table: GW_Conv_ClaimID';
PRINT 'Primary Key: (claimNumber, claimID)';
PRINT 'Indexes: 4 (1 clustered PK + 3 non-clustered)';
PRINT 'Default Constraint: dataLoadedOn = SYSDATETIME()';
PRINT '';
PRINT 'Completion Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
PRINT '============================================================================';
GO

-- ============================================================================
-- Sample test data (optional - comment out for production)
-- ============================================================================
/*
PRINT '';
PRINT 'Inserting sample test data...';

INSERT INTO [dbo].[GW_Conv_ClaimID] (claimID, claimNumber, claimMigratedOn)
VALUES 
    (1001, 'CLM2025001', '2025-01-15 10:30:00'),
    (1002, 'CLM2025002', '2025-01-15 11:45:00'),
    (1003, 'CLM2025003', '2025-01-16 09:15:00'),
    (1004, 'CLM2025004', NULL),  -- Not yet migrated
    (1005, 'CLM2025005', NULL);

PRINT '[OK] Sample data inserted: 5 records';
PRINT '';

-- Verify
SELECT * FROM [dbo].[GW_Conv_ClaimID];
*/
GO




*******************************************************************************************************************************************************

USE [Wawa_DMS_Conversion_UAT];
GO

/*
============================================================================
Script: Create Table - Wawa_Doc_Migration_Transit_Data
============================================================================
Purpose: 
    Create transit data table with optimized indexes for high-volume operations
    Existing volume: 25M+ records
    Daily volume: ~1M new documents

Table Structure:
    Main document metadata table for FileNet migration

Primary Key: 
    Composite key on (claimNumber, externalID)

Performance:
    - Page compression enabled for large table
    - Optimized indexes for sync operations
    - Batch processing friendly

Author: System Generated
Date: 2025-10-21
============================================================================
*/

PRINT '============================================================================';
PRINT 'CREATING TABLE: Wawa_Doc_Migration_Transit_Data';
PRINT 'Database: Wawa_DMS_Conversion_UAT';
PRINT 'Start Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
PRINT '============================================================================';
PRINT '';

-- ============================================================================
-- Drop table if exists (for clean deployment)
-- ============================================================================
IF OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data', 'U') IS NOT NULL
BEGIN
    PRINT '⚠ WARNING: Table already exists with data!';
    PRINT 'Skipping table creation to preserve existing data.';
    PRINT 'If you want to recreate, manually drop the table first.';
    PRINT '';
    PRINT 'Proceeding to verify/add missing indexes...';
    PRINT '';
    GOTO CreateIndexes;
END

-- ============================================================================
-- Create table with primary key and compression
-- ============================================================================
PRINT 'Creating table Wawa_Doc_Migration_Transit_Data...';

CREATE TABLE [dbo].[Wawa_Doc_Migration_Transit_Data](
    [amount] [varchar](50) NULL,
    [author] [varchar](50) NULL,
    [claimant] [varchar](100) NULL,
    [claimID] [bigint] NULL,
    [claimNumber] [varchar](20) NOT NULL,
    [coverage] [varchar](50) NULL,
    [customerID] [varchar](50) NULL,
    [documentDescription] [varchar](500) NULL,
    [documentSubtype] [varchar](50) NULL,
    [documentTitle] [varchar](500) NULL,
    [documentType] [varchar](50) NULL,
    [doNotCreateActivity] [bit] NOT NULL DEFAULT 1,
    [duplicate] [varchar](10) NULL,
    [exposureID] [varchar](50) NULL,
    [gwDocumentID] [varchar](50) NULL,
    [hidden] [varchar](10) NULL,
    [inputMethod] [varchar](50) NULL,
    [insuredName] [varchar](100) NULL,
    [mimeType] [varchar](100) NULL,
    [OrigDateCreated] [varchar](100) NULL,
    [policyNumber] [varchar](50) NULL,
    [primaryMembershipNumber] [varchar](50) NULL,
    [reviewed] [varchar](10) NULL,
    [sensitive] [bit] NOT NULL DEFAULT 0,
    [claimType] [varchar](50) NULL,
    [batchDocCount] [int] NULL,
    [batchID] [varchar](50) NULL,
    [jobID] [varchar](50) NULL,
    [setDocCount] [int] NULL,
    [setID] [int] NULL,
    [SetDocIndex] [int] NULL,
    [contentFilePath] [varchar](500) NULL,
    [contentRetrievalName] [varchar](500) NULL,
    [contentType] [varchar](100) NULL,
    [originatingrepo_ext] [varchar](50) NULL,
    [doc_uid] [varchar](100) NULL,
    [externalID] [varchar](100) NOT NULL,
    [isDataMerged] [bit] NOT NULL DEFAULT 0,
    [isIndexed] [bit] NOT NULL DEFAULT 0,
    [isProcessed] [bit] NOT NULL DEFAULT 0,
    [DateProcessed] [datetime2](3) NULL,
    [isGWNotified] [bit] NOT NULL DEFAULT 0,
    [FN_Doc_GUID] [varchar](50) NULL,
    [comments] [varchar](500) NULL,
    [category_type] [varchar](50) NOT NULL,
    [file_creation_year] [varchar](4) NULL,
    [file_id] [varchar](50) NULL,
    [tar_file_name] [varchar](200) NULL,
    [csv_file_Name] [varchar](200) NULL,
    [CC_Extract_file_Name] [varchar](200) NULL,
    [CC_Extract_file_loaded_date] [datetime2](3) NULL,
    
    -- Composite Primary Key (clustered)
    CONSTRAINT [PK_Transit_ClaimNumber_ExternalID] PRIMARY KEY CLUSTERED 
    (
        [claimNumber] ASC,
        [externalID] ASC
    )
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        IGNORE_DUP_KEY = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 95,  -- High fill factor for mostly read operations (25M+ records)
        DATA_COMPRESSION = PAGE  -- Page compression for large table
    )
);

PRINT '[OK] Table created with page compression';
PRINT '';

CreateIndexes:

-- ============================================================================
-- Create performance indexes
-- ============================================================================
PRINT 'Creating/verifying performance indexes...';
PRINT '';

-- Index 1: Non-clustered index on externalID for lookups
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_ExternalID' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index on externalID...';
    CREATE NONCLUSTERED INDEX [IX_Transit_ExternalID]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([externalID] ASC)
    INCLUDE ([claimNumber], [claimID], [documentTitle], [documentType])
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 95,
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_ExternalID';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_ExternalID';

-- Index 2: Non-clustered index on claimID for reverse lookups
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_ClaimID' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index on claimID...';
    CREATE NONCLUSTERED INDEX [IX_Transit_ClaimID]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([claimID] ASC)
    INCLUDE ([claimNumber], [externalID], [documentTitle], [isProcessed])
    WHERE [claimID] IS NOT NULL
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 95,
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_ClaimID (filtered)';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_ClaimID';

-- Index 3: Non-clustered index for ClaimID sync operations (records needing claimID)
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_ClaimNumber_NullClaimID' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index for sync operations (NULL claimID)...';
    CREATE NONCLUSTERED INDEX [IX_Transit_ClaimNumber_NullClaimID]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([claimNumber] ASC)
    INCLUDE ([externalID], [claimID])
    WHERE [claimID] IS NULL
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 85,  -- Lower for frequent updates
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_ClaimNumber_NullClaimID (filtered)';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_ClaimNumber_NullClaimID';

-- Index 4: Non-clustered index on batchID for batch processing
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_BatchID' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index on batchID...';
    CREATE NONCLUSTERED INDEX [IX_Transit_BatchID]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([batchID] ASC)
    INCLUDE ([claimNumber], [externalID], [claimID], [isProcessed])
    WHERE [batchID] IS NOT NULL
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 95,
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_BatchID (filtered)';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_BatchID';

-- Index 5: Non-clustered index on DateProcessed for reporting
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_DateProcessed' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index on DateProcessed...';
    CREATE NONCLUSTERED INDEX [IX_Transit_DateProcessed]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([DateProcessed] DESC)
    INCLUDE ([claimNumber], [externalID], [claimID], [isProcessed])
    WHERE [DateProcessed] IS NOT NULL
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 95,
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_DateProcessed (filtered)';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_DateProcessed';

-- Index 6: Non-clustered index on CC_Extract_file_loaded_date for daily processing
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_Transit_CC_Extract_LoadedDate' AND object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data'))
BEGIN
    PRINT '  Creating index on CC_Extract_file_loaded_date...';
    CREATE NONCLUSTERED INDEX [IX_Transit_CC_Extract_LoadedDate]
    ON [dbo].[Wawa_Doc_Migration_Transit_Data] ([CC_Extract_file_loaded_date] DESC)
    INCLUDE ([claimNumber], [externalID], [claimID])
    WHERE [CC_Extract_file_loaded_date] IS NOT NULL
    WITH (
        PAD_INDEX = OFF,
        STATISTICS_NORECOMPUTE = OFF,
        SORT_IN_TEMPDB = OFF,
        DROP_EXISTING = OFF,
        ONLINE = OFF,
        ALLOW_ROW_LOCKS = ON,
        ALLOW_PAGE_LOCKS = ON,
        FILLFACTOR = 90,
        DATA_COMPRESSION = PAGE
    );
    PRINT '  [OK] Index created: IX_Transit_CC_Extract_LoadedDate (filtered)';
END
ELSE
    PRINT '  [OK] Index already exists: IX_Transit_CC_Extract_LoadedDate';

PRINT '';
PRINT '[OK] All indexes created/verified';
PRINT '';

-- ============================================================================
-- Verification
-- ============================================================================
PRINT 'Verification:';
PRINT '';

-- Check table exists
IF OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data', 'U') IS NOT NULL
    PRINT '  [OK] Table exists: Wawa_Doc_Migration_Transit_Data';
ELSE
    PRINT '  [Error] ERROR: Table not created';

-- Check primary key
IF EXISTS (SELECT * FROM sys.indexes WHERE object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data') AND is_primary_key = 1)
    PRINT '  [OK] Primary key exists: PK_Transit_ClaimNumber_ExternalID';
ELSE
    PRINT '  [Error] ERROR: Primary key not found';

-- Check compression
IF EXISTS (
    SELECT * FROM sys.partitions 
    WHERE object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data')
    AND data_compression_desc = 'PAGE'
)
    PRINT '  [OK] Page compression enabled';
ELSE
    PRINT '  [Warn] Page compression not enabled (may need manual rebuild)';

-- Check indexes
DECLARE @IndexCount INT;
SELECT @IndexCount = COUNT(*)
FROM sys.indexes 
WHERE object_id = OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data') 
  AND name IS NOT NULL;

PRINT '  [OK] Indexes created: ' + CAST(@IndexCount AS VARCHAR) + ' indexes';

-- Show record count (if table has data)
DECLARE @RecordCount BIGINT;
IF OBJECT_ID('dbo.Wawa_Doc_Migration_Transit_Data', 'U') IS NOT NULL
BEGIN
    SELECT @RecordCount = COUNT(*) FROM [dbo].[Wawa_Doc_Migration_Transit_Data];
    PRINT '  [INFO] Current record count: ' + CAST(@RecordCount AS VARCHAR);
END

PRINT '';
PRINT '============================================================================';
PRINT '[OK] TABLE SETUP COMPLETED SUCCESSFULLY';
PRINT '============================================================================';
PRINT 'Table: Wawa_Doc_Migration_Transit_Data';
PRINT 'Primary Key: (claimNumber, externalID)';
PRINT 'Indexes: 7 (1 clustered PK + 6 non-clustered)';
PRINT 'Compression: PAGE (for 25M+ records)';
PRINT '';
PRINT 'Completion Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
PRINT '============================================================================';
GO



*******************************************************************************************************************************************************

USE [Wawa_DMS_Conversion_UAT];
GO

/*
============================================================================
Stored Procedure: SP_Sync_ClaimID_From_GW_To_Transit
============================================================================
Purpose: 
    Sync claimID from GW_Conv_ClaimID to Wawa_Doc_Migration_Transit_Data
    based on matching claimNumber

Business Logic:
    - Updates records in Transit table where claimID is NULL or needs update
    - Matches on claimNumber
    - Updates claimID from GW mapping table

Performance:
    - Optimized for 25M+ records in Transit table
    - Uses batch processing (100K per batch)
    - Utilizes filtered indexes for optimal performance
    - Uses TABLOCK for improved batch update performance

Usage:
    EXEC [dbo].[SP_Sync_ClaimID_From_GW_To_Transit];

Author: System Generated
Date: 2025-10-21
============================================================================
*/

-- Drop existing procedure
IF OBJECT_ID('dbo.SP_Sync_ClaimID_From_GW_To_Transit', 'P') IS NOT NULL
BEGIN
    DROP PROCEDURE [dbo].[SP_Sync_ClaimID_From_GW_To_Transit];
    PRINT 'Existing procedure dropped';
END
GO

CREATE PROCEDURE [dbo].[SP_Sync_ClaimID_From_GW_To_Transit]
AS
BEGIN
    SET NOCOUNT ON;
    SET XACT_ABORT ON;
    
    DECLARE @StartTime DATETIME2(3) = SYSDATETIME();
    DECLARE @UpdatedCount INT = 0;
    DECLARE @TotalUpdated INT = 0;
    DECLARE @BatchSize INT = 100000;  -- Process 100K records per batch
    DECLARE @RowsAffected INT = 1;
    DECLARE @BatchNumber INT = 0;
    
    PRINT '============================================================================';
    PRINT 'SYNC CLAIMID: GW_Conv_ClaimID → Wawa_Doc_Migration_Transit_Data';
    PRINT '============================================================================';
    PRINT 'Start Time: ' + CONVERT(VARCHAR(30), @StartTime, 121);
    PRINT 'Batch Size: ' + CAST(@BatchSize AS VARCHAR) + ' records';
    PRINT '============================================================================';
    PRINT '';
    
    BEGIN TRY
        -- ====================================================================
        -- STEP 1: Count records that need updating
        -- ====================================================================
        PRINT 'Step 1: Analyzing records that need ClaimID...';
        PRINT '';
        
        DECLARE @TotalNeedingUpdate INT;
        DECLARE @NullClaimID INT;
        DECLARE @MismatchClaimID INT;
        
        -- Count records with NULL claimID that have matching claimNumber
        SELECT @NullClaimID = COUNT(*)
        FROM [dbo].[Wawa_Doc_Migration_Transit_Data] t WITH (NOLOCK)
        INNER JOIN [dbo].[GW_Conv_ClaimID] g WITH (NOLOCK)
            ON t.claimNumber = g.claimNumber
        WHERE t.claimID IS NULL;
        
        -- Count records where claimID doesn't match (needs update)
        SELECT @MismatchClaimID = COUNT(*)
        FROM [dbo].[Wawa_Doc_Migration_Transit_Data] t WITH (NOLOCK)
        INNER JOIN [dbo].[GW_Conv_ClaimID] g WITH (NOLOCK)
            ON t.claimNumber = g.claimNumber
        WHERE t.claimID IS NOT NULL 
          AND t.claimID != g.claimID;
        
        SET @TotalNeedingUpdate = @NullClaimID + @MismatchClaimID;
        
        PRINT '  Records with NULL claimID: ' + CAST(@NullClaimID AS VARCHAR);
        PRINT '  Records with mismatched claimID: ' + CAST(@MismatchClaimID AS VARCHAR);
        PRINT '  Total records needing update: ' + CAST(@TotalNeedingUpdate AS VARCHAR);
        PRINT '';
        
        IF @TotalNeedingUpdate = 0
        BEGIN
            PRINT '[OK] No records need updating. All ClaimIDs are current.';
            PRINT '';
            
            -- Show statistics
            DECLARE @TotalRecords BIGINT;
            DECLARE @WithClaimID BIGINT;
            DECLARE @WithoutClaimID BIGINT;
            
            SELECT 
                @TotalRecords = COUNT(*),
                @WithClaimID = SUM(CASE WHEN claimID IS NOT NULL THEN 1 ELSE 0 END),
                @WithoutClaimID = SUM(CASE WHEN claimID IS NULL THEN 1 ELSE 0 END)
            FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK);
            
            PRINT 'Transit Table Statistics:';
            PRINT '  Total Records: ' + CAST(@TotalRecords AS VARCHAR);
            PRINT '  Records WITH ClaimID: ' + CAST(@WithClaimID AS VARCHAR) + ' (' + 
                  CAST(CAST(100.0 * @WithClaimID / NULLIF(@TotalRecords, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
            PRINT '  Records WITHOUT ClaimID: ' + CAST(@WithoutClaimID AS VARCHAR) + ' (' + 
                  CAST(CAST(100.0 * @WithoutClaimID / NULLIF(@TotalRecords, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
            PRINT '';
            
            PRINT '============================================================================';
            PRINT 'SYNC COMPLETED - No Updates Needed';
            PRINT 'Duration: ' + CAST(DATEDIFF(SECOND, @StartTime, SYSDATETIME()) AS VARCHAR) + ' seconds';
            PRINT '============================================================================';
            RETURN;
        END
        
        -- ====================================================================
        -- STEP 2: Update in batches for performance
        -- ====================================================================
        PRINT 'Step 2: Updating ClaimID in batches...';
        PRINT '';
        
        -- Process in batches to avoid lock escalation and improve performance
        WHILE @RowsAffected > 0
        BEGIN
            SET @BatchNumber = @BatchNumber + 1;
            
            BEGIN TRANSACTION;
            
            -- Update one batch at a time
            -- Uses filtered index IX_Transit_ClaimNumber_NullClaimID for optimal performance
            UPDATE TOP (@BatchSize) t
            SET t.claimID = g.claimID
            FROM [dbo].[Wawa_Doc_Migration_Transit_Data] t WITH (TABLOCK)
            INNER JOIN [dbo].[GW_Conv_ClaimID] g WITH (NOLOCK)
                ON t.claimNumber = g.claimNumber
            WHERE t.claimID IS NULL 
               OR t.claimID != g.claimID;
            
            SET @RowsAffected = @@ROWCOUNT;
            SET @TotalUpdated = @TotalUpdated + @RowsAffected;
            
            COMMIT TRANSACTION;
            
            IF @RowsAffected > 0
            BEGIN
                DECLARE @PercentComplete DECIMAL(5,2) = 
                    CAST(100.0 * @TotalUpdated / NULLIF(@TotalNeedingUpdate, 0) AS DECIMAL(5,2));
                
                PRINT '  Batch ' + RIGHT('   ' + CAST(@BatchNumber AS VARCHAR), 4) + 
                      ': Updated ' + RIGHT('        ' + CAST(@RowsAffected AS VARCHAR), 8) + 
                      ' records | Total: ' + RIGHT('        ' + CAST(@TotalUpdated AS VARCHAR), 8) + 
                      ' (' + CAST(@PercentComplete AS VARCHAR) + '%)';
            END
            
            -- Small delay to prevent overwhelming the server (only if more batches remain)
            IF @RowsAffected > 0 AND @TotalUpdated < @TotalNeedingUpdate
                WAITFOR DELAY '00:00:00.100';  -- 100ms pause between batches
        END
        
        PRINT '';
        PRINT '[OK] All batches completed';
        PRINT '';
        
        -- ====================================================================
        -- STEP 3: Verification & Statistics
        -- ====================================================================
        PRINT 'Step 3: Verification & Statistics';
        PRINT '';
        
        DECLARE @TotalRecordsAfter BIGINT;
        DECLARE @WithClaimIDAfter BIGINT;
        DECLARE @WithoutClaimIDAfter BIGINT;
        DECLARE @MatchingMapping INT;
        
        SELECT 
            @TotalRecordsAfter = COUNT(*),
            @WithClaimIDAfter = SUM(CASE WHEN claimID IS NOT NULL THEN 1 ELSE 0 END),
            @WithoutClaimIDAfter = SUM(CASE WHEN claimID IS NULL THEN 1 ELSE 0 END)
        FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK);
        
        -- Count records that could have been matched but weren't (data quality check)
        SELECT @MatchingMapping = COUNT(*)
        FROM [dbo].[Wawa_Doc_Migration_Transit_Data] t WITH (NOLOCK)
        WHERE t.claimID IS NULL
          AND EXISTS (
              SELECT 1 FROM [dbo].[GW_Conv_ClaimID] g WITH (NOLOCK)
              WHERE g.claimNumber = t.claimNumber
          );
        
        PRINT 'Transit Table Statistics (After Sync):';
        PRINT '  Total Records: ' + CAST(@TotalRecordsAfter AS VARCHAR);
        PRINT '  Records WITH ClaimID: ' + CAST(@WithClaimIDAfter AS VARCHAR) + 
              ' (' + CAST(CAST(100.0 * @WithClaimIDAfter / NULLIF(@TotalRecordsAfter, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
        PRINT '  Records WITHOUT ClaimID: ' + CAST(@WithoutClaimIDAfter AS VARCHAR) + 
              ' (' + CAST(CAST(100.0 * @WithoutClaimIDAfter / NULLIF(@TotalRecordsAfter, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
        PRINT '';
        
        IF @MatchingMapping > 0
        BEGIN
            PRINT '⚠ WARNING: ' + CAST(@MatchingMapping AS VARCHAR) + 
                  ' records still have NULL claimID but matching claimNumber exists in GW_Conv_ClaimID!';
            PRINT '  This may indicate:';
            PRINT '  - Concurrent updates during sync';
            PRINT '  - Data type mismatch';
            PRINT '  - Run sync again to capture these records';
            PRINT '';
        END
        
        -- Calculate performance metrics
        DECLARE @DurationSeconds INT = DATEDIFF(SECOND, @StartTime, SYSDATETIME());
        DECLARE @RecordsPerSecond INT = 
            CASE WHEN @DurationSeconds > 0 
                 THEN @TotalUpdated / @DurationSeconds 
                 ELSE @TotalUpdated 
            END;
        
        PRINT 'Sync Summary:';
        PRINT '  Records Updated: ' + CAST(@TotalUpdated AS VARCHAR);
        PRINT '  Batches Processed: ' + CAST(@BatchNumber AS VARCHAR);
        PRINT '  Duration: ' + CAST(@DurationSeconds AS VARCHAR) + ' seconds';
        PRINT '  Performance: ~' + CAST(@RecordsPerSecond AS VARCHAR) + ' records/second';
        PRINT '';
        
        -- Show sample of updated records (first 5)
        PRINT 'Sample Updated Records (first 5):';
        SELECT TOP 5
            claimNumber,
            claimID,
            externalID,
            documentTitle,
            documentType
        FROM [dbo].[Wawa_Doc_Migration_Transit_Data]
        WHERE claimID IS NOT NULL
        ORDER BY claimNumber DESC;
        
        PRINT '';
        PRINT '============================================================================';
        PRINT '[OK] SYNC COMPLETED SUCCESSFULLY';
        PRINT '============================================================================';
        PRINT 'Completion Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
        PRINT '============================================================================';
        
    END TRY
    BEGIN CATCH
        -- Rollback if transaction is active
        IF @@TRANCOUNT > 0
            ROLLBACK TRANSACTION;
        
        PRINT '';
        PRINT '============================================================================';
        PRINT '✗ ERROR DURING SYNC';
        PRINT '============================================================================';
        PRINT 'Error Number: ' + CAST(ERROR_NUMBER() AS VARCHAR);
        PRINT 'Error Message: ' + ERROR_MESSAGE();
        PRINT 'Error Line: ' + CAST(ERROR_LINE() AS VARCHAR);
        PRINT 'Error Severity: ' + CAST(ERROR_SEVERITY() AS VARCHAR);
        PRINT 'Error State: ' + CAST(ERROR_STATE() AS VARCHAR);
        PRINT '';
        PRINT 'Records updated before error: ' + CAST(@TotalUpdated AS VARCHAR);
        PRINT 'Last completed batch: ' + CAST(@BatchNumber AS VARCHAR);
        PRINT '';
        PRINT 'Note: All updates in current batch were rolled back.';
        PRINT '      Previous batches are committed and safe.';
        PRINT '      You can rerun the procedure to continue from where it stopped.';
        PRINT '============================================================================';
        
        -- Re-throw error for calling application
        THROW;
    END CATCH;
END;
GO

-- ============================================================================
-- Grant permissions (optional - adjust as needed)
-- ============================================================================
-- GRANT EXECUTE ON [dbo].[SP_Sync_ClaimID_From_GW_To_Transit] TO [YourUserRole];
-- GO

PRINT '';
PRINT '[OK] Stored procedure created: SP_Sync_ClaimID_From_GW_To_Transit';
PRINT '';
PRINT 'Usage:';
PRINT '  EXEC [dbo].[SP_Sync_ClaimID_From_GW_To_Transit];';
PRINT '';
PRINT 'Performance Notes:';
PRINT '  - Processes 100K records per batch';
PRINT '  - Expected time: 2-3 seconds per 100K records';
PRINT '  - For 1M records: ~30-45 seconds';
PRINT '  - For 10M records: ~5-7 minutes';
PRINT '  - For 25M records: ~12-18 minutes';
PRINT '';
GO



*******************************************************************************************************************************************************



USE [Wawa_DMS_Conversion_UAT];
GO

/*
============================================================================
DAILY SYNC WORKFLOW
============================================================================
Purpose:
    Execute daily ClaimID synchronization from GW to Transit table

Prerequisites:
    1. GW_Conv_ClaimID table loaded with today's mappings (via Java/BULK INSERT)
    2. Wawa_Doc_Migration_Transit_Data table loaded with documents (via Java)

This script:
    - Verifies data is loaded
    - Runs ClaimID sync procedure
    - Shows statistics and results

Usage:
    Run this script daily after data is loaded
============================================================================
*/

SET NOCOUNT ON;

DECLARE @WorkflowStart DATETIME2(3) = SYSDATETIME();
DECLARE @DateStr VARCHAR(10) = CONVERT(VARCHAR(10), GETDATE(), 120);  -- YYYY-MM-DD

PRINT '============================================================================';
PRINT 'DAILY SYNC WORKFLOW';
PRINT '============================================================================';
PRINT 'Date: ' + @DateStr;
PRINT 'Start Time: ' + CONVERT(VARCHAR(30), @WorkflowStart, 121);
PRINT '============================================================================';
PRINT '';

-- ============================================================================
-- STEP 1: Verify GW Mapping Data
-- ============================================================================
PRINT '--- Step 1: Verify GW Mapping Data ---';
PRINT '';

DECLARE @TotalGWMappings BIGINT;
DECLARE @GWMappingsToday INT;
DECLARE @GWMappingsLast7Days INT;

SELECT 
    @TotalGWMappings = COUNT(*),
    @GWMappingsToday = SUM(CASE WHEN CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE) THEN 1 ELSE 0 END),
    @GWMappingsLast7Days = SUM(CASE WHEN dataLoadedOn >= DATEADD(DAY, -7, GETDATE()) THEN 1 ELSE 0 END)
FROM [dbo].[GW_Conv_ClaimID] WITH (NOLOCK);

PRINT 'GW_Conv_ClaimID Table Statistics:';
PRINT '  Total Mappings: ' + CAST(@TotalGWMappings AS VARCHAR);
PRINT '  Loaded Today: ' + CAST(ISNULL(@GWMappingsToday, 0) AS VARCHAR);
PRINT '  Loaded Last 7 Days: ' + CAST(ISNULL(@GWMappingsLast7Days, 0) AS VARCHAR);

IF @GWMappingsToday = 0
BEGIN
    PRINT '';
    PRINT '⚠ WARNING: No mapping data loaded today.';
    PRINT '  Sync will use existing mappings only.';
    PRINT '  Ensure Java utility has loaded today''s mapping file if expected.';
END
ELSE
BEGIN
    PRINT '  [OK] Fresh mapping data available';
    
    -- Show sample of today's mappings
    PRINT '';
    PRINT '  Sample Mappings Loaded Today (first 5):';
    SELECT TOP 5
        claimID,
        claimNumber,
        claimMigratedOn,
        dataLoadedOn
    FROM [dbo].[GW_Conv_ClaimID]
    WHERE CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE)
    ORDER BY dataLoadedOn DESC;
END

PRINT '';

-- ============================================================================
-- STEP 2: Verify Transit Data
-- ============================================================================
PRINT '--- Step 2: Verify Transit Data ---';
PRINT '';

DECLARE @TotalTransitRecords BIGINT;
DECLARE @RecordsWithClaimID BIGINT;
DECLARE @RecordsWithoutClaimID BIGINT;
DECLARE @RecordsCanBeMatched INT;

SELECT 
    @TotalTransitRecords = COUNT(*),
    @RecordsWithClaimID = SUM(CASE WHEN claimID IS NOT NULL THEN 1 ELSE 0 END),
    @RecordsWithoutClaimID = SUM(CASE WHEN claimID IS NULL THEN 1 ELSE 0 END)
FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK);

-- Count how many NULL claimID records can be matched with GW mappings
SELECT @RecordsCanBeMatched = COUNT(*)
FROM [dbo].[Wawa_Doc_Migration_Transit_Data] t WITH (NOLOCK)
INNER JOIN [dbo].[GW_Conv_ClaimID] g WITH (NOLOCK) ON t.claimNumber = g.claimNumber
WHERE t.claimID IS NULL;

PRINT 'Wawa_Doc_Migration_Transit_Data Table Statistics:';
PRINT '  Total Records: ' + CAST(@TotalTransitRecords AS VARCHAR);
PRINT '  With ClaimID: ' + CAST(@RecordsWithClaimID AS VARCHAR) + 
      ' (' + CAST(CAST(100.0 * @RecordsWithClaimID / NULLIF(@TotalTransitRecords, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
PRINT '  Without ClaimID: ' + CAST(@RecordsWithoutClaimID AS VARCHAR) + 
      ' (' + CAST(CAST(100.0 * @RecordsWithoutClaimID / NULLIF(@TotalTransitRecords, 0) AS DECIMAL(5,2)) AS VARCHAR) + '%)';
PRINT '  Can be matched today: ' + CAST(@RecordsCanBeMatched AS VARCHAR);

IF @RecordsCanBeMatched = 0
BEGIN
    PRINT '';
    PRINT '[OK] No records can be matched. Sync not needed.';
    PRINT '';
    PRINT '============================================================================';
    PRINT 'WORKFLOW COMPLETED - No Sync Needed';
    PRINT 'Duration: ' + CAST(DATEDIFF(SECOND, @WorkflowStart, SYSDATETIME()) AS VARCHAR) + ' seconds';
    PRINT '============================================================================';
    RETURN;
END

PRINT '';

-- ============================================================================
-- STEP 3: Execute ClaimID Sync
-- ============================================================================
PRINT '--- Step 3: Execute ClaimID Sync ---';
PRINT '';

DECLARE @RecordsBeforeSync BIGINT = @RecordsWithoutClaimID;
DECLARE @SyncStartTime DATETIME2(3) = SYSDATETIME();

-- Run the sync procedure
EXEC [dbo].[SP_Sync_ClaimID_From_GW_To_Transit];

DECLARE @SyncDuration INT = DATEDIFF(SECOND, @SyncStartTime, SYSDATETIME());

-- Get updated counts
SELECT 
    @RecordsWithClaimID = SUM(CASE WHEN claimID IS NOT NULL THEN 1 ELSE 0 END),
    @RecordsWithoutClaimID = SUM(CASE WHEN claimID IS NULL THEN 1 ELSE 0 END)
FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK);

PRINT '';

-- ============================================================================
-- STEP 4: Final Summary
-- ============================================================================
PRINT '============================================================================';
PRINT 'DAILY WORKFLOW SUMMARY';
PRINT '============================================================================';
PRINT '';

DECLARE @RecordsSynced BIGINT = @RecordsBeforeSync - @RecordsWithoutClaimID;

PRINT 'Sync Results:';
PRINT '  Records needing ClaimID before sync: ' + CAST(@RecordsBeforeSync AS VARCHAR);
PRINT '  Records still needing ClaimID: ' + CAST(@RecordsWithoutClaimID AS VARCHAR);
PRINT '  Records synced successfully: ' + CAST(@RecordsSynced AS VARCHAR);
PRINT '  Sync duration: ' + CAST(@SyncDuration AS VARCHAR) + ' seconds';
PRINT '';

IF @RecordsWithoutClaimID > 0
BEGIN
    PRINT 'Records Still Missing ClaimID:';
    PRINT '  Count: ' + CAST(@RecordsWithoutClaimID AS VARCHAR);
    PRINT '  Reason: No matching claimNumber in GW_Conv_ClaimID table';
    PRINT '  Action: These will be synced when mapping data becomes available';
    PRINT '';
    
    -- Show sample of records still missing ClaimID (first 10)
    PRINT 'Sample Records Missing ClaimID (first 10):';
    SELECT TOP 10
        claimNumber,
        externalID,
        documentTitle,
        documentType,
        CC_Extract_file_loaded_date
    FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK)
    WHERE claimID IS NULL
    ORDER BY claimNumber;
    PRINT '';
END
ELSE
BEGIN
    PRINT '[OK] All records now have ClaimID!';
    PRINT '';
END

-- Show recent synced records
IF @RecordsSynced > 0
BEGIN
    PRINT 'Sample Recently Synced Records (first 10):';
    SELECT TOP 10
        claimNumber,
        claimID,
        externalID,
        documentTitle,
        documentType
    FROM [dbo].[Wawa_Doc_Migration_Transit_Data] WITH (NOLOCK)
    WHERE claimID IS NOT NULL
    ORDER BY claimNumber DESC;
    PRINT '';
END

-- Performance metrics
DECLARE @TotalDuration INT = DATEDIFF(SECOND, @WorkflowStart, SYSDATETIME());

PRINT 'Performance Metrics:';
PRINT '  Total Workflow Duration: ' + CAST(@TotalDuration AS VARCHAR) + ' seconds';
PRINT '  Sync Duration: ' + CAST(@SyncDuration AS VARCHAR) + ' seconds';
PRINT '  Records Synced: ' + CAST(@RecordsSynced AS VARCHAR);
IF @SyncDuration > 0 AND @RecordsSynced > 0
BEGIN
    DECLARE @RecordsPerSecond INT = @RecordsSynced / @SyncDuration;
    PRINT '  Sync Rate: ~' + CAST(@RecordsPerSecond AS VARCHAR) + ' records/second';
END
PRINT '';

-- Data quality metrics
DECLARE @CompletionRate DECIMAL(5,2) = 
    CAST(100.0 * @RecordsWithClaimID / NULLIF(@TotalTransitRecords, 0) AS DECIMAL(5,2));

PRINT 'Data Quality:';
PRINT '  ClaimID Completion Rate: ' + CAST(@CompletionRate AS VARCHAR) + '%';
IF @CompletionRate >= 99.0
    PRINT '  Status: [OK] EXCELLENT';
ELSE IF @CompletionRate >= 95.0
    PRINT '  Status: [OK] GOOD';
ELSE IF @CompletionRate >= 90.0
    PRINT '  Status: ⚠ ACCEPTABLE';
ELSE
    PRINT '  Status: ⚠ NEEDS ATTENTION';
PRINT '';

PRINT '============================================================================';
PRINT '[OK] DAILY WORKFLOW COMPLETED SUCCESSFULLY';
PRINT '============================================================================';
PRINT 'Completion Time: ' + CONVERT(VARCHAR(30), SYSDATETIME(), 121);
PRINT 'Total Duration: ' + CAST(@TotalDuration AS VARCHAR) + ' seconds';
PRINT '============================================================================';
GO

-- ============================================================================
-- Optional: Quick verification queries (comment out if not needed)
-- ============================================================================
/*
-- Quick stats query
SELECT 
    'GW_Conv_ClaimID' AS TableName,
    COUNT(*) AS TotalRecords,
    SUM(CASE WHEN CAST(dataLoadedOn AS DATE) = CAST(GETDATE() AS DATE) THEN 1 ELSE 0 END) AS LoadedToday
FROM [dbo].[GW_Conv_ClaimID]
UNION ALL
SELECT 
    'Transit_WithClaimID' AS TableName,
    COUNT(*) AS TotalRecords,
    NULL AS LoadedToday
FROM [dbo].[Wawa_Doc_Migration_Transit_Data]
WHERE claimID IS NOT NULL
UNION ALL
SELECT 
    'Transit_WithoutClaimID' AS TableName,
    COUNT(*) AS TotalRecords,
    NULL AS LoadedToday
FROM [dbo].[Wawa_Doc_Migration_Transit_Data]
WHERE claimID IS NULL;
*/

