Single Instance vs 2 Parallel Instances
Resource	1 Instance	2 Instances	Notes
App Threads	10	8 + 8 = 16	Slightly reduced per instance
DB Connections	15	12 + 12 = 24	Combined load
FileNet Load	10 concurrent	16 concurrent	Still manageable
Memory	4GB	4GB + 4GB = 8GB	Need 8GB+ RAM
CPU	50-70%	70-90%	Higher utilization
Recommended Settings for 2 Parallel Instances
Setting	Instance 1	Instance 2	Why
thread.pool.size	8	8	Reduced to avoid overload
db.pool.size	12	12	Total 24 < SQL Server max
batch.size	100	100	Keep same


CREATE TABLE WawaMigratedDocuments (
    id BIGINT IDENTITY(1,1) PRIMARY KEY,
    claimNumber VARCHAR(20) NOT NULL,
    claimID BIGINT NOT NULL,
    externalID VARCHAR(100) NOT NULL UNIQUE,
    batchID VARCHAR(50) NOT NULL,
    document_GUID VARCHAR(50) NOT NULL,
    DateCreated DATETIME NOT NULL DEFAULT GETDATE(),
    packaging_filename VARCHAR(50) NOT NULL,
    Status VARCHAR(10) NOT NULL,
    
    INDEX IX_ExternalID (externalID),
    INDEX IX_ClaimNumber (claimNumber),
    INDEX IX_BatchID (batchID)
);

package com.wawa.ace.migration;

import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.FileProcessingResult;
import com.wawa.ace.migration.report.ReportGenerator;
import com.wawa.ace.migration.service.BatchProcessorService;
import com.wawa.ace.migration.service.DatabaseService;
import com.wawa.ace.migration.service.FileNetService;
import com.wawa.ace.migration.tracker.DocumentTracker;
import com.wawa.ace.migration.util.DateUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.logging.log4j.core.config.Configurator;

import java.io.File;
import java.time.LocalDateTime;
import java.util.List;

/**
 * Main entry point for the Wawa Document Migration Utility.
 * 
 * This utility migrates documents from CSV files to IBM FileNet P8 Content Engine.
 * 
 * Features:
 * - Multi-threaded batch processing for high performance
 * - Configurable batch sizes and thread pools
 * - Automatic retry for failed documents
 * - Duplicate detection to prevent re-processing
 * - Detailed tracking with success/failure CSV outputs
 * - SQL Server database tracking
 * - Comprehensive reporting
 * 
 * Usage:
 *   java -Dconfig.file=path/to/config.properties -Dlog4j.configurationFile=path/to/Log4j2.xml -jar WawaDocumentMigrationUtility.jar
 * 
 * System Properties:
 *   -Dconfig.file           : Path to WawaDocumentMigrationUtility.properties
 *   -Dlog4j.configurationFile : Path to Log4j2.xml
 *   -Djava.library.path     : Path to native libraries (e.g., SQL Server auth DLL)
 */
public class MigrationApp {
    
    private static final Logger logger = LogManager.getLogger(MigrationApp.class);
    
    private static final String DEFAULT_CONFIG_PATH = "D:/Rameshwar/WawaDocumentMigrationUtility/config/";
    private static final String DEFAULT_PROPERTIES_FILE = "WawaDocumentMigrationUtility.properties";
    private static final String DEFAULT_LOG4J_FILE = "Log4j2.xml";
    private static final String DEFAULT_MAPPING_FILE = "MappingConfiguration.json";
    
    public static void main(String[] args) {
        LocalDateTime startTime = LocalDateTime.now();
        
        // Print system properties for debugging
        System.out.println("===== System Properties =====");
        System.out.println("JRE java.library.path : " + System.getProperty("java.library.path"));
        System.out.println("config.file           : " + System.getProperty("config.file"));
        System.out.println("log4j.configurationFile: " + System.getProperty("log4j.configurationFile"));
        System.out.println("=============================");
        
        // Determine configuration paths from system properties or defaults
        String configFilePath = System.getProperty("config.file");
        String log4jConfigPath = System.getProperty("log4j.configurationFile");
        
        // If not provided via system property, use defaults
        String configDir;
        if (configFilePath != null && !configFilePath.isEmpty()) {
            // Extract directory from config file path
            File configFile = new File(configFilePath);
            configDir = configFile.getParent();
            if (configDir == null) {
                configDir = DEFAULT_CONFIG_PATH;
            }
        } else {
            configDir = DEFAULT_CONFIG_PATH;
            configFilePath = configDir + DEFAULT_PROPERTIES_FILE;
        }
        
        if (log4jConfigPath == null || log4jConfigPath.isEmpty()) {
            log4jConfigPath = configDir + File.separator + DEFAULT_LOG4J_FILE;
        }
        
        String mappingFilePath = configDir + File.separator + DEFAULT_MAPPING_FILE;
        
        // Initialize Log4j2 with configuration file
        File log4jFile = new File(log4jConfigPath);
        if (log4jFile.exists()) {
            Configurator.initialize(null, log4jConfigPath);
            System.out.println("Log4j2 initialized from: " + log4jConfigPath);
        } else {
            System.out.println("WARNING: Log4j2 config not found at: " + log4jConfigPath);
        }
        
        logger.info("========================================");
        logger.info("WAWA DOCUMENT MIGRATION UTILITY");
        logger.info("========================================");
        logger.info("Start Time: {}", DateUtils.formatForDisplay(startTime));
        logger.info("Config File: {}", configFilePath);
        logger.info("Log4j Config: {}", log4jConfigPath);
        logger.info("Mapping Config: {}", mappingFilePath);
        logger.info("========================================");
        
        int exitCode = 0;
        ConfigurationManager configManager = null;
        FileNetService fileNetService = null;
        DatabaseService databaseService = null;
        DocumentTracker documentTracker = null;
        BatchProcessorService batchProcessor = null;
        
        // References for shutdown hook
        final FileNetService[] fileNetServiceRef = new FileNetService[1];
        final DatabaseService[] databaseServiceRef = new DatabaseService[1];
        final DocumentTracker[] documentTrackerRef = new DocumentTracker[1];
        final BatchProcessorService[] batchProcessorRef = new BatchProcessorService[1];
        
        try {
            // Validate configuration files exist
            if (!new File(configFilePath).exists()) {
                throw new RuntimeException("Configuration file not found: " + configFilePath);
            }
            if (!new File(mappingFilePath).exists()) {
                throw new RuntimeException("Mapping configuration file not found: " + mappingFilePath);
            }
            
            // Load configuration
            logger.info("Loading configuration...");
            configManager = new ConfigurationManager(configFilePath, mappingFilePath);
            logger.info("Configuration loaded successfully");
            
            // Add shutdown hook for graceful shutdown on Ctrl+C
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Shutdown hook triggered - cleaning up resources...");
                
                try {
                    if (batchProcessorRef[0] != null) {
                        batchProcessorRef[0].close();
                    }
                } catch (Exception e) {
                    logger.error("Error closing batch processor in shutdown hook", e);
                }
                
                try {
                    if (documentTrackerRef[0] != null) {
                        documentTrackerRef[0].close();
                    }
                } catch (Exception e) {
                    logger.error("Error closing document tracker in shutdown hook", e);
                }
                
                try {
                    if (fileNetServiceRef[0] != null) {
                        fileNetServiceRef[0].close();
                    }
                } catch (Exception e) {
                    logger.error("Error closing FileNet service in shutdown hook", e);
                }
                
                try {
                    if (databaseServiceRef[0] != null) {
                        databaseServiceRef[0].close();
                    }
                } catch (Exception e) {
                    logger.error("Error closing database service in shutdown hook", e);
                }
                
                logger.info("Shutdown hook completed");
            }));
            
            // Initialize services
            logger.info("Initializing services...");
            
            // Database service
            logger.info("Connecting to database...");
            databaseService = new DatabaseService(configManager.getProperties());
            databaseServiceRef[0] = databaseService;
            if (!databaseService.testConnection()) {
                throw new RuntimeException("Database connection test failed");
            }
            logger.info("Database connection successful");
            logger.info("Database pool stats: {}", databaseService.getPoolStats());
            
            // FileNet service
            logger.info("Connecting to FileNet...");
            fileNetService = new FileNetService(
                    configManager.getProperties(), 
                    configManager.getMappingConfiguration()
            );
            fileNetServiceRef[0] = fileNetService;
            if (!fileNetService.testConnection()) {
                throw new RuntimeException("FileNet connection test failed");
            }
            logger.info("FileNet connection successful: {}", fileNetService.getConnectionInfo());
            
            // Document tracker
            documentTracker = new DocumentTracker(configManager);
            documentTrackerRef[0] = documentTracker;
            
            // Batch processor
            batchProcessor = new BatchProcessorService(
                    configManager, 
                    fileNetService, 
                    databaseService, 
                    documentTracker
            );
            batchProcessorRef[0] = batchProcessor;
            
            // Process all files
            logger.info("Starting document migration...");
            List<FileProcessingResult> results = batchProcessor.processAllFiles();
            
            // Generate reports
            LocalDateTime endTime = LocalDateTime.now();
            ReportGenerator reportGenerator = new ReportGenerator(configManager);
            
            // Generate per-file reports
            for (FileProcessingResult result : results) {
                try {
                    reportGenerator.generateFileReport(result);
                } catch (Exception e) {
                    logger.error("Failed to generate report for file: {}", 
                            result.getSourceFileName(), e);
                }
            }
            
            // Generate global report
            BatchProcessorService.ProcessingStats stats = batchProcessor.getStats();
            try {
                reportGenerator.generateGlobalReport(results, stats, startTime, endTime);
            } catch (Exception e) {
                logger.error("Failed to generate global report", e);
            }
            
            // Log final summary
            long durationMs = DateUtils.getElapsedMs(startTime, endTime);
            
            logger.info("========================================");
            logger.info("MIGRATION COMPLETED");
            logger.info("========================================");
            logger.info("End Time: {}", DateUtils.formatForDisplay(endTime));
            logger.info("Total Duration: {}", DateUtils.formatDuration(durationMs));
            logger.info("Files Processed: {}", stats.filesProcessed);
            logger.info("Documents Processed: {}", stats.documentsProcessed);
            logger.info("Success: {}", stats.successCount);
            logger.info("Failed: {}", stats.failedCount);
            logger.info("Success Rate: {}%", String.format("%.2f", stats.getSuccessRate()));
            
            if (durationMs > 0) {
                double throughput = (double) stats.documentsProcessed / durationMs * 1000;
                logger.info("Throughput: {} docs/sec", String.format("%.2f", throughput));
            }
            
            logger.info("========================================");
            
            // Set exit code based on results
            if (stats.failedCount > 0) {
                exitCode = 1; // Partial success
                logger.warn("Migration completed with {} failed documents", stats.failedCount);
            } else if (stats.documentsProcessed == 0) {
                exitCode = 2; // No documents processed
                logger.warn("No documents were processed");
            } else {
                logger.info("Migration completed successfully");
            }
            
        } catch (Exception e) {
            logger.error("Migration failed with error", e);
            e.printStackTrace();
            exitCode = -1;
            
        } finally {
            // Cleanup
            logger.info("Cleaning up resources...");
            
            try {
                if (batchProcessor != null) {
                    batchProcessor.close();
                }
            } catch (Exception e) {
                logger.error("Error closing batch processor", e);
            }
            
            try {
                if (documentTracker != null) {
                    documentTracker.close();
                }
            } catch (Exception e) {
                logger.error("Error closing document tracker", e);
            }
            
            try {
                if (fileNetService != null) {
                    fileNetService.close();
                }
            } catch (Exception e) {
                logger.error("Error closing FileNet service", e);
            }
            
            try {
                if (databaseService != null) {
                    databaseService.close();
                }
            } catch (Exception e) {
                logger.error("Error closing database service", e);
            }
            
            logger.info("Cleanup complete");
        }
        
        LocalDateTime finalEndTime = LocalDateTime.now();
        long totalDuration = DateUtils.getElapsedMs(startTime, finalEndTime);
        
        logger.info("========================================");
        logger.info("Application exiting with code: {}", exitCode);
        logger.info("Total runtime: {}", DateUtils.formatDuration(totalDuration));
        logger.info("========================================");
        
        System.exit(exitCode);
    }
    
    /**
     * Print usage information.
     */
    private static void printUsage() {
        System.out.println("Wawa Document Migration Utility");
        System.out.println("================================");
        System.out.println();
        System.out.println("Usage:");
        System.out.println("  java -Dconfig.file=<path> -Dlog4j.configurationFile=<path> -jar WawaDocumentMigrationUtility.jar");
        System.out.println();
        System.out.println("System Properties:");
        System.out.println("  -Dconfig.file             Path to WawaDocumentMigrationUtility.properties");
        System.out.println("  -Dlog4j.configurationFile Path to Log4j2.xml");
        System.out.println("  -Djava.library.path       Path to native libraries (SQL Server auth DLL)");
        System.out.println();
        System.out.println("Example:");
        System.out.println("  java -Dconfig.file=D:/Rameshwar/WawaDocumentMigrationUtility/config/WawaDocumentMigrationUtility.properties \\");
        System.out.println("       -Dlog4j.configurationFile=D:/Rameshwar/WawaDocumentMigrationUtility/config/Log4j2.xml \\");
        System.out.println("       -Djava.library.path=D:/Rameshwar/WawaDocumentMigrationUtility/config \\");
        System.out.println("       -jar WawaDocumentMigrationUtility.jar");
        System.out.println();
        System.out.println("Configuration files required:");
        System.out.println("  - WawaDocumentMigrationUtility.properties");
        System.out.println("  - MappingConfiguration.json");
        System.out.println("  - Log4j2.xml");
        System.out.println();
        System.out.println("JVM Recommended Settings:");
        System.out.println("  -Xms2G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200");
    }
}


package com.wawa.ace.migration.config;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

/**
 * Manages loading and validation of all configuration files.
 */
public class ConfigurationManager {
    
    private static final Logger logger = LogManager.getLogger(ConfigurationManager.class);
    
    private static final String DEFAULT_PROPERTIES_FILE = "WawaDocumentMigrationUtility.properties";
    private static final String DEFAULT_MAPPING_FILE = "MappingConfiguration.json";
    
    private final MigrationProperties properties;
    private final MappingConfiguration mappingConfiguration;
    
    /**
     * Create ConfigurationManager with default config path.
     */
    public ConfigurationManager(String configPath) throws IOException {
        this(
            Paths.get(configPath, DEFAULT_PROPERTIES_FILE).toString(),
            Paths.get(configPath, DEFAULT_MAPPING_FILE).toString()
        );
    }
    
    /**
     * Create ConfigurationManager with specific file paths.
     */
    public ConfigurationManager(String propertiesFilePath, String mappingFilePath) throws IOException {
        logger.info("Loading configuration from: {}", propertiesFilePath);
        this.properties = loadProperties(propertiesFilePath);
        
        logger.info("Loading mapping configuration from: {}", mappingFilePath);
        this.mappingConfiguration = loadMappingConfiguration(mappingFilePath);
        
        validateConfiguration();
        logger.info("Configuration loaded successfully");
    }
    
    private MigrationProperties loadProperties(String filePath) throws IOException {
        File file = new File(filePath);
        if (!file.exists()) {
            throw new IOException("Properties file not found: " + filePath);
        }
        return new MigrationProperties(filePath);
    }
    
    private MappingConfiguration loadMappingConfiguration(String filePath) throws IOException {
        File file = new File(filePath);
        if (!file.exists()) {
            throw new IOException("Mapping configuration file not found: " + filePath);
        }
        
        ObjectMapper mapper = new ObjectMapper();
        MappingConfiguration config = mapper.readValue(file, MappingConfiguration.class);
        config.initializeLookupMaps();
        return config;
    }
    
    /**
     * Validate the loaded configuration.
     */
    private void validateConfiguration() throws IOException {
        logger.info("Validating configuration...");
        
        // Validate required paths exist
        validateDirectory(properties.getFullSourcePath(), "Source folder");
        validateDirectory(properties.getFullInprocessPath(), "Inprocess folder");
        validateDirectory(properties.getFullCompletedPath(), "Completed folder");
        validateDirectory(properties.getFullFailedPath(), "Failed folder");
        validateDirectory(properties.getFullArchivePath(), "Archive folder");
        validateDirectory(properties.getFullReportsPath(), "Reports folder");
        
        // Validate FileNet settings
        if (properties.getFileNetUri() == null || properties.getFileNetUri().isEmpty()) {
            throw new IOException("FileNet URI is not configured");
        }
        if (properties.getFileNetObjectStore() == null || properties.getFileNetObjectStore().isEmpty()) {
            throw new IOException("FileNet Object Store is not configured");
        }
        
        // Validate mapping configuration
        if (mappingConfiguration.getDocumentClass() == null || mappingConfiguration.getDocumentClass().isEmpty()) {
            throw new IOException("Document class is not configured in mapping");
        }
        if (mappingConfiguration.getMappings() == null || mappingConfiguration.getMappings().isEmpty()) {
            throw new IOException("No property mappings defined");
        }
        if (mappingConfiguration.getContentSettings() == null) {
            throw new IOException("Content settings are not configured");
        }
        if (mappingConfiguration.getTrackingSettings() == null) {
            throw new IOException("Tracking settings are not configured");
        }
        
        // Validate required mappings have values
        for (PropertyMapping mapping : mappingConfiguration.getRequiredMappings()) {
            logger.debug("Required mapping: {} -> {}", mapping.getCsvHeader(), mapping.getPropertySymbolicName());
        }
        
        // Validate database settings
        if (properties.getDbServer() == null || properties.getDbServer().isEmpty()) {
            throw new IOException("Database server is not configured");
        }
        if (properties.getDbName() == null || properties.getDbName().isEmpty()) {
            throw new IOException("Database name is not configured");
        }
        
        logger.info("Configuration validation successful");
        logConfigurationSummary();
    }
    
    private void validateDirectory(Path path, String description) throws IOException {
        if (!Files.exists(path)) {
            logger.info("Creating directory: {} - {}", description, path);
            Files.createDirectories(path);
        }
        if (!Files.isDirectory(path)) {
            throw new IOException(description + " is not a directory: " + path);
        }
    }
    
    private void logConfigurationSummary() {
        logger.info("=== Configuration Summary ===");
        logger.info("Base Path: {}", properties.getBasePath());
        logger.info("FileNet URI: {}", properties.getFileNetUri());
        logger.info("FileNet Object Store: {}", properties.getFileNetObjectStore());
        logger.info("Document Class: {}", mappingConfiguration.getDocumentClass());
        logger.info("Property Mappings: {}", mappingConfiguration.getMappings().size());
        logger.info("Batch Size: {}", properties.getBatchSize());
        logger.info("Thread Pool Size: {}", properties.getThreadPoolSize());
        logger.info("Retry Enabled: {}", properties.isRetryEnabled());
        logger.info("Retry Max Attempts: {}", properties.getRetryMaxAttempts());
        logger.info("Database: {}/{}", properties.getDbServer(), properties.getDbName());
        logger.info("============================");
    }
    
    // Getters
    public MigrationProperties getProperties() {
        return properties;
    }
    
    public MappingConfiguration getMappingConfiguration() {
        return mappingConfiguration;
    }
    
    /**
     * Get the full path for a source file.
     */
    public Path getSourceFilePath(String fileName) {
        return properties.getFullSourcePath().resolve(fileName);
    }
    
    /**
     * Get the full path for an inprocess file.
     */
    public Path getInprocessFilePath(String fileName) {
        return properties.getFullInprocessPath().resolve(fileName);
    }
    
    /**
     * Get the full path for a completed file.
     */
    public Path getCompletedFilePath(String fileName) {
        return properties.getFullCompletedPath().resolve(fileName);
    }
    
    /**
     * Get the full path for a failed file.
     */
    public Path getFailedFilePath(String fileName) {
        return properties.getFullFailedPath().resolve(fileName);
    }
    
    /**
     * Get the full path for an archive file.
     */
    public Path getArchiveFilePath(String fileName) {
        return properties.getFullArchivePath().resolve(fileName);
    }
    
    /**
     * Get the full path for a report file.
     */
    public Path getReportFilePath(String fileName) {
        return properties.getFullReportsPath().resolve(fileName);
    }
}

package com.wawa.ace.migration.config;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Configuration for content-related CSV columns.
 */
@JsonIgnoreProperties(ignoreUnknown = true)
public class ContentSettings {
    
    @JsonProperty("contentFilePathColumn")
    private String contentFilePathColumn;
    
    @JsonProperty("contentRetrievalNameColumn")
    private String contentRetrievalNameColumn;
    
    @JsonProperty("mimeTypeColumn")
    private String mimeTypeColumn;
    
    public ContentSettings() {
    }
    
    public String getContentFilePathColumn() {
        return contentFilePathColumn;
    }
    
    public void setContentFilePathColumn(String contentFilePathColumn) {
        this.contentFilePathColumn = contentFilePathColumn;
    }
    
    public String getContentRetrievalNameColumn() {
        return contentRetrievalNameColumn;
    }
    
    public void setContentRetrievalNameColumn(String contentRetrievalNameColumn) {
        this.contentRetrievalNameColumn = contentRetrievalNameColumn;
    }
    
    public String getMimeTypeColumn() {
        return mimeTypeColumn;
    }
    
    public void setMimeTypeColumn(String mimeTypeColumn) {
        this.mimeTypeColumn = mimeTypeColumn;
    }
    
    @Override
    public String toString() {
        return "ContentSettings{" +
                "contentFilePathColumn='" + contentFilePathColumn + '\'' +
                ", contentRetrievalNameColumn='" + contentRetrievalNameColumn + '\'' +
                ", mimeTypeColumn='" + mimeTypeColumn + '\'' +
                '}';
    }
}

package com.wawa.ace.migration.config;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Root configuration class for the JSON mapping file.
 * Maps CSV columns to FileNet document properties.
 */
@JsonIgnoreProperties(ignoreUnknown = true)
public class MappingConfiguration {
    
    @JsonProperty("documentClass")
    private String documentClass;
    
    @JsonProperty("mappings")
    private List<PropertyMapping> mappings;
    
    @JsonProperty("contentSettings")
    private ContentSettings contentSettings;
    
    @JsonProperty("trackingSettings")
    private TrackingSettings trackingSettings;
    
    // Cached lookup map for fast access
    private transient Map<String, PropertyMapping> mappingByCsvHeader;
    private transient Map<String, PropertyMapping> mappingByPropertyName;
    
    public MappingConfiguration() {
        this.mappings = new ArrayList<>();
    }
    
    /**
     * Initialize lookup maps for fast access.
     * Should be called after deserialization.
     */
    public void initializeLookupMaps() {
        mappingByCsvHeader = new HashMap<>();
        mappingByPropertyName = new HashMap<>();
        
        if (mappings != null) {
            for (PropertyMapping mapping : mappings) {
                if (mapping.getCsvHeader() != null) {
                    mappingByCsvHeader.put(mapping.getCsvHeader(), mapping);
                }
                if (mapping.getPropertySymbolicName() != null) {
                    mappingByPropertyName.put(mapping.getPropertySymbolicName(), mapping);
                }
            }
        }
    }
    
    /**
     * Get property mapping by CSV header name.
     */
    public PropertyMapping getMappingByCsvHeader(String csvHeader) {
        if (mappingByCsvHeader == null) {
            initializeLookupMaps();
        }
        return mappingByCsvHeader.get(csvHeader);
    }
    
    /**
     * Get property mapping by FileNet property symbolic name.
     */
    public PropertyMapping getMappingByPropertyName(String propertyName) {
        if (mappingByPropertyName == null) {
            initializeLookupMaps();
        }
        return mappingByPropertyName.get(propertyName);
    }
    
    /**
     * Check if a CSV header has a mapping defined.
     */
    public boolean hasMappingForCsvHeader(String csvHeader) {
        return getMappingByCsvHeader(csvHeader) != null;
    }
    
    /**
     * Get all required property mappings.
     */
    public List<PropertyMapping> getRequiredMappings() {
        List<PropertyMapping> required = new ArrayList<>();
        if (mappings != null) {
            for (PropertyMapping mapping : mappings) {
                if (mapping.isRequired()) {
                    required.add(mapping);
                }
            }
        }
        return required;
    }
    
    // Getters and Setters
    public String getDocumentClass() {
        return documentClass;
    }
    
    public void setDocumentClass(String documentClass) {
        this.documentClass = documentClass;
    }
    
    public List<PropertyMapping> getMappings() {
        return mappings;
    }
    
    public void setMappings(List<PropertyMapping> mappings) {
        this.mappings = mappings;
        this.mappingByCsvHeader = null; // Reset cache
        this.mappingByPropertyName = null;
    }
    
    public ContentSettings getContentSettings() {
        return contentSettings;
    }
    
    public void setContentSettings(ContentSettings contentSettings) {
        this.contentSettings = contentSettings;
    }
    
    public TrackingSettings getTrackingSettings() {
        return trackingSettings;
    }
    
    public void setTrackingSettings(TrackingSettings trackingSettings) {
        this.trackingSettings = trackingSettings;
    }
    
    @Override
    public String toString() {
        return "MappingConfiguration{" +
                "documentClass='" + documentClass + '\'' +
                ", mappingsCount=" + (mappings != null ? mappings.size() : 0) +
                ", contentSettings=" + contentSettings +
                ", trackingSettings=" + trackingSettings +
                '}';
    }
}

package com.wawa.ace.migration.config;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Properties;

/**
 * Holds all application properties loaded from the properties file.
 */
public class MigrationProperties {
    
    private final Properties properties;
    
    // Application Settings
    private String basePath;
    private String configPath;
    private boolean skipHeaderRow;
    
    // Folder Paths
    private String sourceFolder;
    private String inprocessFolder;
    private String completedFolder;
    private String failedFolder;
    private String archiveFolder;
    private String reportsFolder;
    
    // CSV Settings
    private char csvDelimiter;
    private String csvEncoding;
    
    // FileNet Settings
    private String fileNetUri;
    private String fileNetUsername;
    private String fileNetPassword;
    private String fileNetObjectStore;
    private String fileNetStanza;
    private String fileNetDocumentClass;
    
    // Batch Processing
    private int batchSize;
    private int batchCommitTimeoutSeconds;
    
    // Thread Pool
    private int threadPoolSize;
    private int threadPoolShutdownTimeoutSeconds;
    
    // Retry Configuration
    private boolean retryEnabled;
    private int retryMaxAttempts;
    private long retryDelayMilliseconds;
    
    // Database Settings
    private String dbServer;
    private int dbPort;
    private String dbName;
    private String dbTable;
    private int dbPoolSize;
    private int dbConnectionTimeoutSeconds;
    private boolean dbIntegratedSecurity;
    private String dbUsername;
    private String dbPassword;
    
    // Memory & Performance
    private int memoryWarningThresholdPercent;
    private int progressLogIntervalMinutes;
    
    // Logging
    private String logConfigPath;
    
    public MigrationProperties(String propertiesFilePath) throws IOException {
        this.properties = new Properties();
        try (InputStream input = new FileInputStream(propertiesFilePath)) {
            properties.load(input);
        }
        loadProperties();
    }
    
    public MigrationProperties(Properties properties) {
        this.properties = properties;
        loadProperties();
    }
    
    private void loadProperties() {
        // Application Settings
        basePath = getProperty("app.base.path", "D:/Rameshwar/WawaDocumentMigrationUtility/");
        configPath = getProperty("app.config.path", basePath + "config/");
        skipHeaderRow = getBooleanProperty("app.skip.header.row", true);
        
        // Folder Paths
        sourceFolder = getProperty("migration.source.folder", "Migration/Source/");
        inprocessFolder = getProperty("migration.inprocess.folder", "Migration/Inprocess/");
        completedFolder = getProperty("migration.completed.folder", "Migration/Completed/");
        failedFolder = getProperty("migration.failed.folder", "Migration/Failed/");
        archiveFolder = getProperty("migration.archive.folder", "Migration/Archive/");
        reportsFolder = getProperty("migration.reports.folder", "Migration/Reports/");
        
        // CSV Settings
        String delimiterStr = getProperty("csv.delimiter", "|");
        csvDelimiter = delimiterStr.isEmpty() ? '|' : delimiterStr.charAt(0);
        csvEncoding = getProperty("csv.encoding", "UTF-8");
        
        // FileNet Settings
        fileNetUri = getProperty("filenet.ce.uri", "");
        fileNetUsername = getProperty("filenet.username", "");
        fileNetPassword = getProperty("filenet.password", "");
        fileNetObjectStore = getProperty("filenet.objectstore", "");
        fileNetStanza = getProperty("filenet.stanza", "FileNetP8WSI");
        fileNetDocumentClass = getProperty("filenet.document.class", "ClaimDocument");
        
        // Batch Processing
        batchSize = getIntProperty("batch.size", 100);
        batchCommitTimeoutSeconds = getIntProperty("batch.commit.timeout.seconds", 300);
        
        // Thread Pool
        threadPoolSize = getIntProperty("thread.pool.size", 5);
        threadPoolShutdownTimeoutSeconds = getIntProperty("thread.pool.shutdown.timeout.seconds", 3600);
        
        // Retry Configuration
        retryEnabled = getBooleanProperty("retry.enabled", true);
        retryMaxAttempts = getIntProperty("retry.max.attempts", 3);
        retryDelayMilliseconds = getLongProperty("retry.delay.milliseconds", 1000L);
        
        // Database Settings
        dbServer = getProperty("db.server", "");
        dbPort = getIntProperty("db.port", 1433);
        dbName = getProperty("db.name", "");
        dbTable = getProperty("db.table", "WawaMigratedDocuments");
        dbPoolSize = getIntProperty("db.pool.size", 15);
        dbConnectionTimeoutSeconds = getIntProperty("db.connection.timeout.seconds", 30);
        dbIntegratedSecurity = getBooleanProperty("db.integrated.security", false);
        dbUsername = getProperty("db.username", "");
        dbPassword = getProperty("db.password", "");
        
        // Memory & Performance
        memoryWarningThresholdPercent = getIntProperty("memory.warning.threshold.percent", 80);
        progressLogIntervalMinutes = getIntProperty("progress.log.interval.minutes", 5);
        
        // Logging
        logConfigPath = getProperty("log.config.path", configPath + "Log4j2.xml");
    }
    
    private String getProperty(String key, String defaultValue) {
        return properties.getProperty(key, defaultValue);
    }
    
    private int getIntProperty(String key, int defaultValue) {
        String value = properties.getProperty(key);
        if (value == null || value.trim().isEmpty()) {
            return defaultValue;
        }
        try {
            return Integer.parseInt(value.trim());
        } catch (NumberFormatException e) {
            return defaultValue;
        }
    }
    
    private long getLongProperty(String key, long defaultValue) {
        String value = properties.getProperty(key);
        if (value == null || value.trim().isEmpty()) {
            return defaultValue;
        }
        try {
            return Long.parseLong(value.trim());
        } catch (NumberFormatException e) {
            return defaultValue;
        }
    }
    
    private boolean getBooleanProperty(String key, boolean defaultValue) {
        String value = properties.getProperty(key);
        if (value == null || value.trim().isEmpty()) {
            return defaultValue;
        }
        return Boolean.parseBoolean(value.trim());
    }
    
    // Path helper methods
    public Path getFullSourcePath() {
        return Paths.get(basePath, sourceFolder);
    }
    
    public Path getFullInprocessPath() {
        return Paths.get(basePath, inprocessFolder);
    }
    
    public Path getFullCompletedPath() {
        return Paths.get(basePath, completedFolder);
    }
    
    public Path getFullFailedPath() {
        return Paths.get(basePath, failedFolder);
    }
    
    public Path getFullArchivePath() {
        return Paths.get(basePath, archiveFolder);
    }
    
    public Path getFullReportsPath() {
        return Paths.get(basePath, reportsFolder);
    }
    
    // All getters
    public String getBasePath() { return basePath; }
    public String getConfigPath() { return configPath; }
    public boolean isSkipHeaderRow() { return skipHeaderRow; }
    public String getSourceFolder() { return sourceFolder; }
    public String getInprocessFolder() { return inprocessFolder; }
    public String getCompletedFolder() { return completedFolder; }
    public String getFailedFolder() { return failedFolder; }
    public String getArchiveFolder() { return archiveFolder; }
    public String getReportsFolder() { return reportsFolder; }
    public char getCsvDelimiter() { return csvDelimiter; }
    public String getCsvEncoding() { return csvEncoding; }
    public String getFileNetUri() { return fileNetUri; }
    public String getFileNetUsername() { return fileNetUsername; }
    public String getFileNetPassword() { return fileNetPassword; }
    public String getFileNetObjectStore() { return fileNetObjectStore; }
    public String getFileNetStanza() { return fileNetStanza; }
    public String getFileNetDocumentClass() { return fileNetDocumentClass; }
    public int getBatchSize() { return batchSize; }
    public int getBatchCommitTimeoutSeconds() { return batchCommitTimeoutSeconds; }
    public int getThreadPoolSize() { return threadPoolSize; }
    public int getThreadPoolShutdownTimeoutSeconds() { return threadPoolShutdownTimeoutSeconds; }
    public boolean isRetryEnabled() { return retryEnabled; }
    public int getRetryMaxAttempts() { return retryMaxAttempts; }
    public long getRetryDelayMilliseconds() { return retryDelayMilliseconds; }
    public String getDbServer() { return dbServer; }
    public int getDbPort() { return dbPort; }
    public String getDbName() { return dbName; }
    public String getDbTable() { return dbTable; }
    public int getDbPoolSize() { return dbPoolSize; }
    public int getDbConnectionTimeoutSeconds() { return dbConnectionTimeoutSeconds; }
    public boolean isDbIntegratedSecurity() { return dbIntegratedSecurity; }
    public String getDbUsername() { return dbUsername; }
    public String getDbPassword() { return dbPassword; }
    public int getMemoryWarningThresholdPercent() { return memoryWarningThresholdPercent; }
    public int getProgressLogIntervalMinutes() { return progressLogIntervalMinutes; }
    public String getLogConfigPath() { return logConfigPath; }
    
    @Override
    public String toString() {
        return "MigrationProperties{" +
                "basePath='" + basePath + '\'' +
                ", fileNetUri='" + fileNetUri + '\'' +
                ", fileNetObjectStore='" + fileNetObjectStore + '\'' +
                ", batchSize=" + batchSize +
                ", threadPoolSize=" + threadPoolSize +
                ", retryEnabled=" + retryEnabled +
                ", retryMaxAttempts=" + retryMaxAttempts +
                '}';
    }
}

package com.wawa.ace.migration.config;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Represents a single property mapping from CSV column to FileNet property.
 */
@JsonIgnoreProperties(ignoreUnknown = true)
public class PropertyMapping {
    
    @JsonProperty("csvHeader")
    private String csvHeader;
    
    @JsonProperty("propertySymbolicName")
    private String propertySymbolicName;
    
    @JsonProperty("dataType")
    private DataType dataType;
    
    @JsonProperty("maxLength")
    private Integer maxLength;
    
    @JsonProperty("required")
    private boolean required;
    
    @JsonProperty("dateFormat")
    private String dateFormat;
    
    public PropertyMapping() {
    }
    
    public PropertyMapping(String csvHeader, String propertySymbolicName, DataType dataType) {
        this.csvHeader = csvHeader;
        this.propertySymbolicName = propertySymbolicName;
        this.dataType = dataType;
    }
    
    // Getters and Setters
    public String getCsvHeader() {
        return csvHeader;
    }
    
    public void setCsvHeader(String csvHeader) {
        this.csvHeader = csvHeader;
    }
    
    public String getPropertySymbolicName() {
        return propertySymbolicName;
    }
    
    public void setPropertySymbolicName(String propertySymbolicName) {
        this.propertySymbolicName = propertySymbolicName;
    }
    
    public DataType getDataType() {
        return dataType;
    }
    
    public void setDataType(DataType dataType) {
        this.dataType = dataType;
    }
    
    public Integer getMaxLength() {
        return maxLength;
    }
    
    public void setMaxLength(Integer maxLength) {
        this.maxLength = maxLength;
    }
    
    public boolean isRequired() {
        return required;
    }
    
    public void setRequired(boolean required) {
        this.required = required;
    }
    
    public String getDateFormat() {
        return dateFormat;
    }
    
    public void setDateFormat(String dateFormat) {
        this.dateFormat = dateFormat;
    }
    
    @Override
    public String toString() {
        return "PropertyMapping{" +
                "csvHeader='" + csvHeader + '\'' +
                ", propertySymbolicName='" + propertySymbolicName + '\'' +
                ", dataType=" + dataType +
                ", required=" + required +
                '}';
    }
    
    /**
     * Enum representing the supported data types for FileNet properties.
     */
    public enum DataType {
        STRING,
        LONG,
        DOUBLE,
        BOOLEAN,
        DATE;
        
        public static DataType fromString(String value) {
            if (value == null) return STRING;
            try {
                return DataType.valueOf(value.toUpperCase());
            } catch (IllegalArgumentException e) {
                return STRING;
            }
        }
    }
}

package com.wawa.ace.migration.config;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Configuration for tracking-related CSV columns.
 */
@JsonIgnoreProperties(ignoreUnknown = true)
public class TrackingSettings {
    
    @JsonProperty("externalIdColumn")
    private String externalIdColumn;
    
    @JsonProperty("claimNumberColumn")
    private String claimNumberColumn;
    
    @JsonProperty("claimIdColumn")
    private String claimIdColumn;
    
    @JsonProperty("batchIdColumn")
    private String batchIdColumn;
    
    public TrackingSettings() {
    }
    
    public String getExternalIdColumn() {
        return externalIdColumn;
    }
    
    public void setExternalIdColumn(String externalIdColumn) {
        this.externalIdColumn = externalIdColumn;
    }
    
    public String getClaimNumberColumn() {
        return claimNumberColumn;
    }
    
    public void setClaimNumberColumn(String claimNumberColumn) {
        this.claimNumberColumn = claimNumberColumn;
    }
    
    public String getClaimIdColumn() {
        return claimIdColumn;
    }
    
    public void setClaimIdColumn(String claimIdColumn) {
        this.claimIdColumn = claimIdColumn;
    }
    
    public String getBatchIdColumn() {
        return batchIdColumn;
    }
    
    public void setBatchIdColumn(String batchIdColumn) {
        this.batchIdColumn = batchIdColumn;
    }
    
    @Override
    public String toString() {
        return "TrackingSettings{" +
                "externalIdColumn='" + externalIdColumn + '\'' +
                ", claimNumberColumn='" + claimNumberColumn + '\'' +
                ", claimIdColumn='" + claimIdColumn + '\'' +
                ", batchIdColumn='" + batchIdColumn + '\'' +
                '}';
    }
}

package com.wawa.ace.migration.model;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

/**
 * Represents the result of processing a batch of documents.
 */
public class BatchResult {
    
    private final int batchNumber;
    private final int totalDocuments;
    private final List<ProcessingResult> successResults;
    private final List<ProcessingResult> failedResults;
    private final LocalDateTime startTime;
    private final LocalDateTime endTime;
    private final long processingDurationMs;
    
    private BatchResult(Builder builder) {
        this.batchNumber = builder.batchNumber;
        this.totalDocuments = builder.totalDocuments;
        this.successResults = Collections.unmodifiableList(new ArrayList<>(builder.successResults));
        this.failedResults = Collections.unmodifiableList(new ArrayList<>(builder.failedResults));
        this.startTime = builder.startTime;
        this.endTime = builder.endTime;
        this.processingDurationMs = builder.processingDurationMs;
    }
    
    public static Builder builder(int batchNumber, int totalDocuments) {
        return new Builder(batchNumber, totalDocuments);
    }
    
    // Getters
    public int getBatchNumber() {
        return batchNumber;
    }
    
    public int getTotalDocuments() {
        return totalDocuments;
    }
    
    public int getSuccessCount() {
        return successResults.size();
    }
    
    public int getFailedCount() {
        return failedResults.size();
    }
    
    public List<ProcessingResult> getSuccessResults() {
        return successResults;
    }
    
    public List<ProcessingResult> getFailedResults() {
        return failedResults;
    }
    
    public List<ProcessingResult> getAllResults() {
        List<ProcessingResult> all = new ArrayList<>();
        all.addAll(successResults);
        all.addAll(failedResults);
        return all;
    }
    
    public LocalDateTime getStartTime() {
        return startTime;
    }
    
    public LocalDateTime getEndTime() {
        return endTime;
    }
    
    public long getProcessingDurationMs() {
        return processingDurationMs;
    }
    
    public double getSuccessRate() {
        if (totalDocuments == 0) return 0.0;
        return (double) getSuccessCount() / totalDocuments * 100.0;
    }
    
    public boolean isAllSuccess() {
        return failedResults.isEmpty();
    }
    
    public boolean isAllFailed() {
        return successResults.isEmpty();
    }
    
    @Override
    public String toString() {
        return String.format("BatchResult{batch=%d, total=%d, success=%d, failed=%d, rate=%.1f%%, duration=%dms}",
                batchNumber, totalDocuments, getSuccessCount(), getFailedCount(), getSuccessRate(), processingDurationMs);
    }
    
    public static class Builder {
        private final int batchNumber;
        private final int totalDocuments;
        private final List<ProcessingResult> successResults = new ArrayList<>();
        private final List<ProcessingResult> failedResults = new ArrayList<>();
        private LocalDateTime startTime;
        private LocalDateTime endTime;
        private long processingDurationMs;
        
        private Builder(int batchNumber, int totalDocuments) {
            this.batchNumber = batchNumber;
            this.totalDocuments = totalDocuments;
        }
        
        public Builder addSuccess(ProcessingResult result) {
            this.successResults.add(result);
            return this;
        }
        
        public Builder addFailure(ProcessingResult result) {
            this.failedResults.add(result);
            return this;
        }
        
        public Builder addResult(ProcessingResult result) {
            if (result.isSuccess()) {
                this.successResults.add(result);
            } else {
                this.failedResults.add(result);
            }
            return this;
        }
        
        public Builder startTime(LocalDateTime startTime) {
            this.startTime = startTime;
            return this;
        }
        
        public Builder endTime(LocalDateTime endTime) {
            this.endTime = endTime;
            return this;
        }
        
        public Builder processingDurationMs(long processingDurationMs) {
            this.processingDurationMs = processingDurationMs;
            return this;
        }
        
        public BatchResult build() {
            return new BatchResult(this);
        }
    }
}

package com.wawa.ace.migration.model;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

/**
 * Represents the result of processing an entire CSV file.
 */
public class FileProcessingResult {
    
    private final String sourceFileName;
    private final int totalDocuments;
    private final int totalBatches;
    private final List<BatchResult> batchResults;
    private final LocalDateTime startTime;
    private final LocalDateTime endTime;
    private final long processingDurationMs;
    
    private FileProcessingResult(Builder builder) {
        this.sourceFileName = builder.sourceFileName;
        this.totalDocuments = builder.totalDocuments;
        this.totalBatches = builder.totalBatches;
        this.batchResults = Collections.unmodifiableList(new ArrayList<>(builder.batchResults));
        this.startTime = builder.startTime;
        this.endTime = builder.endTime;
        this.processingDurationMs = builder.processingDurationMs;
    }
    
    public static Builder builder(String sourceFileName) {
        return new Builder(sourceFileName);
    }
    
    // Getters
    public String getSourceFileName() {
        return sourceFileName;
    }
    
    public int getTotalDocuments() {
        return totalDocuments;
    }
    
    public int getTotalBatches() {
        return totalBatches;
    }
    
    public List<BatchResult> getBatchResults() {
        return batchResults;
    }
    
    public int getSuccessCount() {
        return batchResults.stream().mapToInt(BatchResult::getSuccessCount).sum();
    }
    
    public int getFailedCount() {
        return batchResults.stream().mapToInt(BatchResult::getFailedCount).sum();
    }
    
    public LocalDateTime getStartTime() {
        return startTime;
    }
    
    public LocalDateTime getEndTime() {
        return endTime;
    }
    
    public long getProcessingDurationMs() {
        return processingDurationMs;
    }
    
    public double getSuccessRate() {
        if (totalDocuments == 0) return 0.0;
        return (double) getSuccessCount() / totalDocuments * 100.0;
    }
    
    public double getAverageDocumentProcessingTimeMs() {
        if (totalDocuments == 0) return 0.0;
        return (double) processingDurationMs / totalDocuments;
    }
    
    public double getDocumentsPerSecond() {
        if (processingDurationMs == 0) return 0.0;
        return (double) totalDocuments / processingDurationMs * 1000.0;
    }
    
    public List<ProcessingResult> getAllFailedResults() {
        List<ProcessingResult> allFailed = new ArrayList<>();
        for (BatchResult batch : batchResults) {
            allFailed.addAll(batch.getFailedResults());
        }
        return allFailed;
    }
    
    public List<ProcessingResult> getAllSuccessResults() {
        List<ProcessingResult> allSuccess = new ArrayList<>();
        for (BatchResult batch : batchResults) {
            allSuccess.addAll(batch.getSuccessResults());
        }
        return allSuccess;
    }
    
    @Override
    public String toString() {
        return String.format(
                "FileProcessingResult{file='%s', total=%d, success=%d, failed=%d, rate=%.1f%%, duration=%dms, throughput=%.1f docs/sec}",
                sourceFileName, totalDocuments, getSuccessCount(), getFailedCount(), 
                getSuccessRate(), processingDurationMs, getDocumentsPerSecond());
    }
    
    public static class Builder {
        private final String sourceFileName;
        private int totalDocuments;
        private int totalBatches;
        private final List<BatchResult> batchResults = new ArrayList<>();
        private LocalDateTime startTime;
        private LocalDateTime endTime;
        private long processingDurationMs;
        
        private Builder(String sourceFileName) {
            this.sourceFileName = sourceFileName;
        }
        
        public Builder totalDocuments(int totalDocuments) {
            this.totalDocuments = totalDocuments;
            return this;
        }
        
        public Builder totalBatches(int totalBatches) {
            this.totalBatches = totalBatches;
            return this;
        }
        
        public Builder addBatchResult(BatchResult batchResult) {
            this.batchResults.add(batchResult);
            return this;
        }
        
        public Builder startTime(LocalDateTime startTime) {
            this.startTime = startTime;
            return this;
        }
        
        public Builder endTime(LocalDateTime endTime) {
            this.endTime = endTime;
            return this;
        }
        
        public Builder processingDurationMs(long processingDurationMs) {
            this.processingDurationMs = processingDurationMs;
            return this;
        }
        
        public FileProcessingResult build() {
            return new FileProcessingResult(this);
        }
    }
}

package com.wawa.ace.migration.model;

import java.util.LinkedHashMap;
import java.util.Map;

/**
 * Represents a document to be migrated to FileNet.
 * Contains all CSV row data and metadata for processing.
 */
public class MigrationDocument {
    
    private int rowNumber;
    private String externalId;
    private String claimNumber;
    private String claimId;
    private String batchId;
    private String contentFilePath;
    private String contentRetrievalName;
    private String mimeType;
    private String documentTitle;
    
    // All CSV columns stored as key-value pairs
    private Map<String, String> csvData;
    
    // Processing metadata
    private String documentGuid;
    private String sourceFileName;
    private ProcessingStatus status;
    private String errorMessage;
    private String errorStackTrace;
    private int retryCount;
    
    public MigrationDocument() {
        this.csvData = new LinkedHashMap<>();
        this.status = ProcessingStatus.PENDING;
        this.retryCount = 0;
    }
    
    public MigrationDocument(int rowNumber, Map<String, String> csvData) {
        this.rowNumber = rowNumber;
        this.csvData = new LinkedHashMap<>(csvData);
        this.status = ProcessingStatus.PENDING;
        this.retryCount = 0;
    }
    
    // Getters and Setters
    public int getRowNumber() {
        return rowNumber;
    }
    
    public void setRowNumber(int rowNumber) {
        this.rowNumber = rowNumber;
    }
    
    public String getExternalId() {
        return externalId;
    }
    
    public void setExternalId(String externalId) {
        this.externalId = externalId;
    }
    
    public String getClaimNumber() {
        return claimNumber;
    }
    
    public void setClaimNumber(String claimNumber) {
        this.claimNumber = claimNumber;
    }
    
    public String getClaimId() {
        return claimId;
    }
    
    public void setClaimId(String claimId) {
        this.claimId = claimId;
    }
    
    public String getBatchId() {
        return batchId;
    }
    
    public void setBatchId(String batchId) {
        this.batchId = batchId;
    }
    
    public String getContentFilePath() {
        return contentFilePath;
    }
    
    public void setContentFilePath(String contentFilePath) {
        this.contentFilePath = contentFilePath;
    }
    
    public String getContentRetrievalName() {
        return contentRetrievalName;
    }
    
    public void setContentRetrievalName(String contentRetrievalName) {
        this.contentRetrievalName = contentRetrievalName;
    }
    
    public String getMimeType() {
        return mimeType;
    }
    
    public void setMimeType(String mimeType) {
        this.mimeType = mimeType;
    }
    
    public String getDocumentTitle() {
        return documentTitle;
    }
    
    public void setDocumentTitle(String documentTitle) {
        this.documentTitle = documentTitle;
    }
    
    public Map<String, String> getCsvData() {
        return csvData;
    }
    
    public void setCsvData(Map<String, String> csvData) {
        this.csvData = csvData;
    }
    
    public String getCsvValue(String columnName) {
        return csvData.get(columnName);
    }
    
    public void setCsvValue(String columnName, String value) {
        csvData.put(columnName, value);
    }
    
    public String getDocumentGuid() {
        return documentGuid;
    }
    
    public void setDocumentGuid(String documentGuid) {
        this.documentGuid = documentGuid;
    }
    
    public String getSourceFileName() {
        return sourceFileName;
    }
    
    public void setSourceFileName(String sourceFileName) {
        this.sourceFileName = sourceFileName;
    }
    
    public ProcessingStatus getStatus() {
        return status;
    }
    
    public void setStatus(ProcessingStatus status) {
        this.status = status;
    }
    
    public String getErrorMessage() {
        return errorMessage;
    }
    
    public void setErrorMessage(String errorMessage) {
        this.errorMessage = errorMessage;
    }
    
    public String getErrorStackTrace() {
        return errorStackTrace;
    }
    
    public void setErrorStackTrace(String errorStackTrace) {
        this.errorStackTrace = errorStackTrace;
    }
    
    public int getRetryCount() {
        return retryCount;
    }
    
    public void setRetryCount(int retryCount) {
        this.retryCount = retryCount;
    }
    
    public void incrementRetryCount() {
        this.retryCount++;
    }
    
    public boolean isSuccess() {
        return status == ProcessingStatus.SUCCESS;
    }
    
    public boolean isFailed() {
        return status == ProcessingStatus.FAILED;
    }
    
    @Override
    public String toString() {
        return "MigrationDocument{" +
                "rowNumber=" + rowNumber +
                ", externalId='" + externalId + '\'' +
                ", claimNumber='" + claimNumber + '\'' +
                ", status=" + status +
                '}';
    }
}

package com.wawa.ace.migration.model;

import java.time.LocalDateTime;

/**
 * Represents the result of processing a single document.
 */
public class ProcessingResult {
    
    private final MigrationDocument document;
    private final boolean success;
    private final String documentGuid;
    private final String errorMessage;
    private final String errorStackTrace;
    private final LocalDateTime processedTime;
    private final long processingDurationMs;
    
    private ProcessingResult(Builder builder) {
        this.document = builder.document;
        this.success = builder.success;
        this.documentGuid = builder.documentGuid;
        this.errorMessage = builder.errorMessage;
        this.errorStackTrace = builder.errorStackTrace;
        this.processedTime = builder.processedTime != null ? builder.processedTime : LocalDateTime.now();
        this.processingDurationMs = builder.processingDurationMs;
    }
    
    public static Builder success(MigrationDocument document, String documentGuid) {
        return new Builder(document, true).documentGuid(documentGuid);
    }
    
    public static Builder failure(MigrationDocument document, String errorMessage) {
        return new Builder(document, false).errorMessage(errorMessage);
    }
    
    public static Builder failure(MigrationDocument document, Throwable throwable) {
        return new Builder(document, false)
                .errorMessage(throwable.getMessage())
                .errorStackTrace(getStackTraceAsString(throwable));
    }
    
    private static String getStackTraceAsString(Throwable throwable) {
        StringBuilder sb = new StringBuilder();
        sb.append(throwable.toString()).append("\n");
        for (StackTraceElement element : throwable.getStackTrace()) {
            sb.append("\tat ").append(element.toString()).append("\n");
        }
        if (throwable.getCause() != null) {
            sb.append("Caused by: ").append(getStackTraceAsString(throwable.getCause()));
        }
        return sb.toString();
    }
    
    // Getters
    public MigrationDocument getDocument() {
        return document;
    }
    
    public boolean isSuccess() {
        return success;
    }
    
    public String getDocumentGuid() {
        return documentGuid;
    }
    
    public String getErrorMessage() {
        return errorMessage;
    }
    
    public String getErrorStackTrace() {
        return errorStackTrace;
    }
    
    public LocalDateTime getProcessedTime() {
        return processedTime;
    }
    
    public long getProcessingDurationMs() {
        return processingDurationMs;
    }
    
    @Override
    public String toString() {
        return "ProcessingResult{" +
                "externalId='" + (document != null ? document.getExternalId() : "null") + '\'' +
                ", success=" + success +
                ", documentGuid='" + documentGuid + '\'' +
                ", errorMessage='" + errorMessage + '\'' +
                '}';
    }
    
    public static class Builder {
        private final MigrationDocument document;
        private final boolean success;
        private String documentGuid;
        private String errorMessage;
        private String errorStackTrace;
        private LocalDateTime processedTime;
        private long processingDurationMs;
        
        private Builder(MigrationDocument document, boolean success) {
            this.document = document;
            this.success = success;
        }
        
        public Builder documentGuid(String documentGuid) {
            this.documentGuid = documentGuid;
            return this;
        }
        
        public Builder errorMessage(String errorMessage) {
            this.errorMessage = errorMessage;
            return this;
        }
        
        public Builder errorStackTrace(String errorStackTrace) {
            this.errorStackTrace = errorStackTrace;
            return this;
        }
        
        public Builder processedTime(LocalDateTime processedTime) {
            this.processedTime = processedTime;
            return this;
        }
        
        public Builder processingDurationMs(long processingDurationMs) {
            this.processingDurationMs = processingDurationMs;
            return this;
        }
        
        public ProcessingResult build() {
            return new ProcessingResult(this);
        }
    }
}

package com.wawa.ace.migration.model;

/**
 * Enum representing the processing status of a migration document.
 */
public enum ProcessingStatus {
    PENDING("PENDING"),
    IN_PROGRESS("IN_PROGRESS"),
    SUCCESS("SUCCESS"),
    FAILED("FAILED"),
    RETRY("RETRY"),
    SKIPPED("SKIPPED");
    
    private final String value;
    
    ProcessingStatus(String value) {
        this.value = value;
    }
    
    public String getValue() {
        return value;
    }
    
    @Override
    public String toString() {
        return value;
    }
}

package com.wawa.ace.migration.processor;

import com.opencsv.CSVParser;
import com.opencsv.CSVParserBuilder;
import com.opencsv.CSVReader;
import com.opencsv.CSVReaderBuilder;
import com.opencsv.exceptions.CsvValidationException;
import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.config.ContentSettings;
import com.wawa.ace.migration.config.MigrationProperties;
import com.wawa.ace.migration.config.TrackingSettings;
import com.wawa.ace.migration.model.MigrationDocument;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.*;
import java.nio.charset.Charset;
import java.nio.file.Path;
import java.util.*;

/**
 * Processor for reading and parsing CSV files.
 * Converts CSV rows into MigrationDocument objects.
 */
public class CSVFileProcessor {
    
    private static final Logger logger = LogManager.getLogger(CSVFileProcessor.class);
    
    private final ConfigurationManager configManager;
    private final MigrationProperties properties;
    
    public CSVFileProcessor(ConfigurationManager configManager) {
        this.configManager = configManager;
        this.properties = configManager.getProperties();
    }
    
    /**
     * Read all documents from a CSV file.
     */
    public List<MigrationDocument> readDocuments(Path csvFile) throws IOException, CsvValidationException {
        return readDocuments(csvFile, csvFile.getFileName().toString());
    }
    
    /**
     * Read all documents from a CSV file with source file name.
     */
    public List<MigrationDocument> readDocuments(Path csvFile, String sourceFileName) 
            throws IOException, CsvValidationException {
        List<MigrationDocument> documents = new ArrayList<>();
        
        logger.info("Reading CSV file: {}", csvFile);
        
        char delimiter = properties.getCsvDelimiter();
        Charset charset = Charset.forName(properties.getCsvEncoding());
        
        CSVParser parser = new CSVParserBuilder()
                .withSeparator(delimiter)
                .withQuoteChar('"')
                .withIgnoreQuotations(false)
                .build();
        
        try (Reader reader = new BufferedReader(new InputStreamReader(
                new FileInputStream(csvFile.toFile()), charset));
             CSVReader csvReader = new CSVReaderBuilder(reader)
                     .withCSVParser(parser)
                     .build()) {
            
            // Read header
            String[] headers = csvReader.readNext();
            if (headers == null || headers.length == 0) {
                throw new IOException("CSV file is empty or has no headers: " + csvFile);
            }
            
            // Create header index map
            Map<String, Integer> headerIndex = new HashMap<>();
            for (int i = 0; i < headers.length; i++) {
                headerIndex.put(headers[i].trim(), i);
            }
            
            logger.debug("CSV headers: {}", Arrays.toString(headers));
            
            // Get column indices for special fields
            ContentSettings contentSettings = configManager.getMappingConfiguration().getContentSettings();
            TrackingSettings trackingSettings = configManager.getMappingConfiguration().getTrackingSettings();
            
            Integer contentFilePathIdx = headerIndex.get(contentSettings.getContentFilePathColumn());
            Integer contentRetrievalNameIdx = headerIndex.get(contentSettings.getContentRetrievalNameColumn());
            Integer mimeTypeIdx = headerIndex.get(contentSettings.getMimeTypeColumn());
            Integer externalIdIdx = headerIndex.get(trackingSettings.getExternalIdColumn());
            Integer claimNumberIdx = headerIndex.get(trackingSettings.getClaimNumberColumn());
            Integer claimIdIdx = headerIndex.get(trackingSettings.getClaimIdColumn());
            Integer batchIdIdx = headerIndex.get(trackingSettings.getBatchIdColumn());
            Integer documentTitleIdx = headerIndex.get("documentTitle");
            
            // Validate required columns exist
            if (externalIdIdx == null) {
                throw new IOException("Missing required column: " + trackingSettings.getExternalIdColumn());
            }
            if (contentFilePathIdx == null) {
                throw new IOException("Missing required column: " + contentSettings.getContentFilePathColumn());
            }
            
            // Read data rows
            String[] row;
            int rowNumber = 1; // Start from 1 (after header)
            
            while ((row = csvReader.readNext()) != null) {
                rowNumber++;
                
                if (row.length == 0 || (row.length == 1 && row[0].trim().isEmpty())) {
                    logger.debug("Skipping empty row at line {}", rowNumber);
                    continue;
                }
                
                try {
                    MigrationDocument doc = new MigrationDocument();
                    doc.setRowNumber(rowNumber);
                    doc.setSourceFileName(sourceFileName);
                    
                    // Build CSV data map
                    Map<String, String> csvData = new LinkedHashMap<>();
                    for (int i = 0; i < headers.length && i < row.length; i++) {
                        csvData.put(headers[i].trim(), row[i] != null ? row[i].trim() : "");
                    }
                    doc.setCsvData(csvData);
                    
                    // Set special fields
                    doc.setExternalId(getValueSafe(row, externalIdIdx));
                    doc.setContentFilePath(getValueSafe(row, contentFilePathIdx));
                    doc.setContentRetrievalName(getValueSafe(row, contentRetrievalNameIdx));
                    doc.setMimeType(getValueSafe(row, mimeTypeIdx));
                    doc.setClaimNumber(getValueSafe(row, claimNumberIdx));
                    doc.setClaimId(getValueSafe(row, claimIdIdx));
                    doc.setBatchId(getValueSafe(row, batchIdIdx));
                    doc.setDocumentTitle(getValueSafe(row, documentTitleIdx));
                    
                    // Validate required fields
                    if (doc.getExternalId() == null || doc.getExternalId().isEmpty()) {
                        logger.warn("Row {} missing externalId, skipping", rowNumber);
                        continue;
                    }
                    
                    documents.add(doc);
                    
                } catch (Exception e) {
                    logger.error("Error parsing row {}: {}", rowNumber, e.getMessage());
                }
            }
        }
        
        logger.info("Read {} documents from {}", documents.size(), csvFile.getFileName());
        return documents;
    }
    
    /**
     * Read documents in batches for memory efficiency.
     * Returns an iterator that reads batches on demand.
     */
    public DocumentBatchIterator readDocumentsInBatches(Path csvFile, int batchSize) throws IOException {
        return new DocumentBatchIterator(csvFile, batchSize, configManager);
    }
    
    /**
     * Get the headers from a CSV file.
     */
    public String[] getHeaders(Path csvFile) throws IOException, CsvValidationException {
        char delimiter = properties.getCsvDelimiter();
        Charset charset = Charset.forName(properties.getCsvEncoding());
        
        CSVParser parser = new CSVParserBuilder()
                .withSeparator(delimiter)
                .withQuoteChar('"')
                .build();
        
        try (Reader reader = new BufferedReader(new InputStreamReader(
                new FileInputStream(csvFile.toFile()), charset));
             CSVReader csvReader = new CSVReaderBuilder(reader)
                     .withCSVParser(parser)
                     .build()) {
            
            return csvReader.readNext();
        }
    }
    
    /**
     * Count the number of data rows in a CSV file (excluding header).
     */
    public int countRows(Path csvFile) throws IOException, CsvValidationException {
        int count = 0;
        char delimiter = properties.getCsvDelimiter();
        Charset charset = Charset.forName(properties.getCsvEncoding());
        
        CSVParser parser = new CSVParserBuilder()
                .withSeparator(delimiter)
                .withQuoteChar('"')
                .build();
        
        try (Reader reader = new BufferedReader(new InputStreamReader(
                new FileInputStream(csvFile.toFile()), charset));
             CSVReader csvReader = new CSVReaderBuilder(reader)
                     .withCSVParser(parser)
                     .build()) {
            
            // Skip header
            csvReader.readNext();
            
            // Count rows
            while (csvReader.readNext() != null) {
                count++;
            }
        }
        
        return count;
    }
    
    private String getValueSafe(String[] row, Integer index) {
        if (index == null || index < 0 || index >= row.length) {
            return null;
        }
        String value = row[index];
        return value != null ? value.trim() : null;
    }
    
    /**
     * Iterator for reading documents in batches to conserve memory.
     */
    public static class DocumentBatchIterator implements Iterator<List<MigrationDocument>>, Closeable {
        
        private final CSVReader csvReader;
        private final Reader reader;
        private final int batchSize;
        private final String[] headers;
        private final Map<String, Integer> headerIndex;
        private final String sourceFileName;
        
        private int currentRowNumber;
        private boolean hasNext;
        private String[] nextRow;
        
        // Column indices
        private final Integer contentFilePathIdx;
        private final Integer contentRetrievalNameIdx;
        private final Integer mimeTypeIdx;
        private final Integer externalIdIdx;
        private final Integer claimNumberIdx;
        private final Integer claimIdIdx;
        private final Integer batchIdIdx;
        private final Integer documentTitleIdx;
        
        public DocumentBatchIterator(Path csvFile, int batchSize, ConfigurationManager configManager) throws IOException {
            this.batchSize = batchSize;
            this.sourceFileName = csvFile.getFileName().toString();
            this.currentRowNumber = 1;
            
            MigrationProperties properties = configManager.getProperties();
            char delimiter = properties.getCsvDelimiter();
            Charset charset = Charset.forName(properties.getCsvEncoding());
            
            CSVParser parser = new CSVParserBuilder()
                    .withSeparator(delimiter)
                    .withQuoteChar('"')
                    .build();
            
            this.reader = new BufferedReader(new InputStreamReader(
                    new FileInputStream(csvFile.toFile()), charset));
            this.csvReader = new CSVReaderBuilder(reader)
                    .withCSVParser(parser)
                    .build();
            
            try {
                // Read headers
                this.headers = csvReader.readNext();
                if (headers == null || headers.length == 0) {
                    throw new IOException("CSV file is empty or has no headers");
                }
                
                // Create header index
                this.headerIndex = new HashMap<>();
                for (int i = 0; i < headers.length; i++) {
                    headerIndex.put(headers[i].trim(), i);
                }
                
                // Get column indices
                ContentSettings contentSettings = configManager.getMappingConfiguration().getContentSettings();
                TrackingSettings trackingSettings = configManager.getMappingConfiguration().getTrackingSettings();
                
                this.contentFilePathIdx = headerIndex.get(contentSettings.getContentFilePathColumn());
                this.contentRetrievalNameIdx = headerIndex.get(contentSettings.getContentRetrievalNameColumn());
                this.mimeTypeIdx = headerIndex.get(contentSettings.getMimeTypeColumn());
                this.externalIdIdx = headerIndex.get(trackingSettings.getExternalIdColumn());
                this.claimNumberIdx = headerIndex.get(trackingSettings.getClaimNumberColumn());
                this.claimIdIdx = headerIndex.get(trackingSettings.getClaimIdColumn());
                this.batchIdIdx = headerIndex.get(trackingSettings.getBatchIdColumn());
                this.documentTitleIdx = headerIndex.get("documentTitle");
                
                // Read first row to check if there's data
                this.nextRow = csvReader.readNext();
                this.hasNext = (nextRow != null);
            } catch (CsvValidationException e) {
                throw new IOException("CSV validation error: " + e.getMessage(), e);
            }
        }
        
        @Override
        public boolean hasNext() {
            return hasNext;
        }
        
        @Override
        public List<MigrationDocument> next() {
            if (!hasNext) {
                throw new NoSuchElementException();
            }
            
            List<MigrationDocument> batch = new ArrayList<>(batchSize);
            
            try {
                while (nextRow != null && batch.size() < batchSize) {
                    currentRowNumber++;
                    
                    if (nextRow.length > 0 && !(nextRow.length == 1 && nextRow[0].trim().isEmpty())) {
                        MigrationDocument doc = parseRow(nextRow, currentRowNumber);
                        if (doc != null) {
                            batch.add(doc);
                        }
                    }
                    
                    nextRow = csvReader.readNext();
                }
                
                hasNext = (nextRow != null);
                
            } catch (IOException | CsvValidationException e) {
                throw new RuntimeException("Error reading CSV batch", e);
            }
            
            return batch;
        }
        
        private MigrationDocument parseRow(String[] row, int rowNumber) {
            try {
                MigrationDocument doc = new MigrationDocument();
                doc.setRowNumber(rowNumber);
                doc.setSourceFileName(sourceFileName);
                
                // Build CSV data map
                Map<String, String> csvData = new LinkedHashMap<>();
                for (int i = 0; i < headers.length && i < row.length; i++) {
                    csvData.put(headers[i].trim(), row[i] != null ? row[i].trim() : "");
                }
                doc.setCsvData(csvData);
                
                // Set special fields
                doc.setExternalId(getValueSafe(row, externalIdIdx));
                doc.setContentFilePath(getValueSafe(row, contentFilePathIdx));
                doc.setContentRetrievalName(getValueSafe(row, contentRetrievalNameIdx));
                doc.setMimeType(getValueSafe(row, mimeTypeIdx));
                doc.setClaimNumber(getValueSafe(row, claimNumberIdx));
                doc.setClaimId(getValueSafe(row, claimIdIdx));
                doc.setBatchId(getValueSafe(row, batchIdIdx));
                doc.setDocumentTitle(getValueSafe(row, documentTitleIdx));
                
                // Validate required fields
                if (doc.getExternalId() == null || doc.getExternalId().isEmpty()) {
                    return null;
                }
                
                return doc;
                
            } catch (Exception e) {
                return null;
            }
        }
        
        private String getValueSafe(String[] row, Integer index) {
            if (index == null || index < 0 || index >= row.length) {
                return null;
            }
            String value = row[index];
            return value != null ? value.trim() : null;
        }
        
        @Override
        public void close() throws IOException {
            try {
                if (csvReader != null) {
                    csvReader.close();
                }
            } finally {
                if (reader != null) {
                    reader.close();
                }
            }
        }
    }
}

package com.wawa.ace.migration.processor;

import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.model.ProcessingStatus;
import com.wawa.ace.migration.service.DatabaseService;
import com.wawa.ace.migration.service.FileNetService;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.util.List;

/**
 * Processor for individual document operations.
 * Handles single document creation with retry logic.
 */
public class DocumentProcessor {
    
    private static final Logger logger = LogManager.getLogger(DocumentProcessor.class);
    
    private final ConfigurationManager configManager;
    private final FileNetService fileNetService;
    private final DatabaseService databaseService;
    
    private final boolean retryEnabled;
    private final int maxRetryAttempts;
    private final long retryDelayMs;
    
    public DocumentProcessor(ConfigurationManager configManager, 
                             FileNetService fileNetService,
                             DatabaseService databaseService) {
        this.configManager = configManager;
        this.fileNetService = fileNetService;
        this.databaseService = databaseService;
        
        this.retryEnabled = configManager.getProperties().isRetryEnabled();
        this.maxRetryAttempts = configManager.getProperties().getRetryMaxAttempts();
        this.retryDelayMs = configManager.getProperties().getRetryDelayMilliseconds();
    }
    
    /**
     * Process a single document with retry logic.
     */
    public ProcessingResult processDocument(MigrationDocument document) {
        logger.debug("Processing document: {}", document.getExternalId());
        
        // Check for duplicate
        try {
            if (databaseService.isDuplicate(document.getExternalId())) {
                logger.warn("Document already exists (duplicate): {}", document.getExternalId());
                document.setStatus(ProcessingStatus.SKIPPED);
                return ProcessingResult.failure(document, 
                        "Document already exists in database (duplicate)")
                        .build();
            }
        } catch (Exception e) {
            logger.error("Error checking for duplicate: {}", document.getExternalId(), e);
            // Continue processing even if duplicate check fails
        }
        
        // Process with retry
        ProcessingResult result = null;
        int attempts = 0;
        
        while (attempts <= maxRetryAttempts) {
            attempts++;
            document.setRetryCount(attempts - 1);
            
            if (attempts > 1) {
                logger.info("Retry attempt {} for document: {}", attempts - 1, document.getExternalId());
                
                // Wait before retry
                try {
                    Thread.sleep(retryDelayMs);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                }
            }
            
            document.setStatus(ProcessingStatus.IN_PROGRESS);
            
            // Create document in FileNet
            result = fileNetService.createDocument(document);
            
            if (result.isSuccess()) {
                document.setStatus(ProcessingStatus.SUCCESS);
                document.setDocumentGuid(result.getDocumentGuid());
                logger.debug("Document created successfully: {} -> {}", 
                        document.getExternalId(), result.getDocumentGuid());
                break;
            }
            
            // If retry is not enabled, break after first attempt
            if (!retryEnabled) {
                document.setStatus(ProcessingStatus.FAILED);
                document.setErrorMessage(result.getErrorMessage());
                break;
            }
            
            // Check if error is retryable
            if (!isRetryableError(result.getErrorMessage())) {
                logger.debug("Non-retryable error for document: {}", document.getExternalId());
                document.setStatus(ProcessingStatus.FAILED);
                document.setErrorMessage(result.getErrorMessage());
                break;
            }
        }
        
        if (!result.isSuccess()) {
            document.setStatus(ProcessingStatus.FAILED);
            document.setErrorMessage(result.getErrorMessage());
            document.setErrorStackTrace(result.getErrorStackTrace());
        }
        
        return result;
    }
    
    /**
     * Check if documents are duplicates (already exist in database).
     */
    public List<String> checkDuplicates(List<MigrationDocument> documents) {
        try {
            List<String> externalIds = documents.stream()
                    .map(MigrationDocument::getExternalId)
                    .toList();
            
            return databaseService.checkDuplicates(externalIds);
        } catch (Exception e) {
            logger.error("Error checking for duplicates", e);
            return List.of();
        }
    }
    
    /**
     * Determine if an error is retryable.
     */
    private boolean isRetryableError(String errorMessage) {
        if (errorMessage == null) {
            return true;
        }
        
        String lowerError = errorMessage.toLowerCase();
        
        // Non-retryable errors
        if (lowerError.contains("not found") ||
            lowerError.contains("access denied") ||
            lowerError.contains("permission") ||
            lowerError.contains("invalid") ||
            lowerError.contains("duplicate") ||
            lowerError.contains("already exists")) {
            return false;
        }
        
        // Retryable errors (connection issues, timeouts, etc.)
        if (lowerError.contains("timeout") ||
            lowerError.contains("connection") ||
            lowerError.contains("unavailable") ||
            lowerError.contains("temporary") ||
            lowerError.contains("retry")) {
            return true;
        }
        
        // Default to retryable
        return true;
    }
}

package com.wawa.ace.migration.report;

import com.opencsv.CSVWriter;
import com.opencsv.ICSVWriter;
import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.BatchResult;
import com.wawa.ace.migration.model.FileProcessingResult;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.service.BatchProcessorService;
import com.wawa.ace.migration.util.DateUtils;
import com.wawa.ace.migration.util.FileUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.time.LocalDateTime;
import java.util.List;

/**
 * Generates detailed CSV reports for migration processing.
 */
public class ReportGenerator {
    
    private static final Logger logger = LogManager.getLogger(ReportGenerator.class);
    
    private final ConfigurationManager configManager;
    
    public ReportGenerator(ConfigurationManager configManager) {
        this.configManager = configManager;
    }
    
    /**
     * Generate a detailed report for a file processing result.
     */
    public Path generateFileReport(FileProcessingResult result) throws IOException {
        String baseName = FileUtils.getFileNameWithoutExtension(Path.of(result.getSourceFileName()));
        String timestamp = DateUtils.getCurrentFileTimestamp();
        String reportFileName = String.format("%s_report_%s.csv", baseName, timestamp);
        
        Path reportPath = configManager.getReportFilePath(reportFileName);
        Files.createDirectories(reportPath.getParent());
        
        try (Writer writer = new BufferedWriter(new OutputStreamWriter(
                new FileOutputStream(reportPath.toFile()), StandardCharsets.UTF_8));
             CSVWriter csvWriter = new CSVWriter(writer, ',', 
                     ICSVWriter.DEFAULT_QUOTE_CHARACTER,
                     ICSVWriter.DEFAULT_ESCAPE_CHARACTER,
                     ICSVWriter.DEFAULT_LINE_END)) {
            
            // Write summary section
            csvWriter.writeNext(new String[]{"=== MIGRATION REPORT ==="});
            csvWriter.writeNext(new String[]{""});
            
            // Report metadata
            csvWriter.writeNext(new String[]{"Report Type", "Value"});
            csvWriter.writeNext(new String[]{"SourceFile", result.getSourceFileName()});
            csvWriter.writeNext(new String[]{"ProcessingStartTime", 
                    DateUtils.formatForDisplay(result.getStartTime())});
            csvWriter.writeNext(new String[]{"ProcessingEndTime", 
                    DateUtils.formatForDisplay(result.getEndTime())});
            csvWriter.writeNext(new String[]{"TotalDocuments", 
                    String.valueOf(result.getTotalDocuments())});
            csvWriter.writeNext(new String[]{"SuccessCount", 
                    String.valueOf(result.getSuccessCount())});
            csvWriter.writeNext(new String[]{"FailedCount", 
                    String.valueOf(result.getFailedCount())});
            csvWriter.writeNext(new String[]{"SuccessRate", 
                    String.format("%.2f%%", result.getSuccessRate())});
            csvWriter.writeNext(new String[]{"ProcessingDurationMs", 
                    String.valueOf(result.getProcessingDurationMs())});
            csvWriter.writeNext(new String[]{"ProcessingDuration", 
                    DateUtils.formatDuration(result.getProcessingDurationMs())});
            csvWriter.writeNext(new String[]{"AverageDocProcessingTimeMs", 
                    String.format("%.2f", result.getAverageDocumentProcessingTimeMs())});
            csvWriter.writeNext(new String[]{"DocumentsPerSecond", 
                    String.format("%.2f", result.getDocumentsPerSecond())});
            csvWriter.writeNext(new String[]{"TotalBatches", 
                    String.valueOf(result.getTotalBatches())});
            csvWriter.writeNext(new String[]{"BatchSize", 
                    String.valueOf(configManager.getProperties().getBatchSize())});
            csvWriter.writeNext(new String[]{"ThreadPoolSize", 
                    String.valueOf(configManager.getProperties().getThreadPoolSize())});
            
            csvWriter.writeNext(new String[]{""});
            
            // Batch details section
            csvWriter.writeNext(new String[]{"=== BATCH DETAILS ==="});
            csvWriter.writeNext(new String[]{"BatchNumber", "TotalDocs", "Success", "Failed", 
                    "SuccessRate", "DurationMs"});
            
            for (BatchResult batch : result.getBatchResults()) {
                csvWriter.writeNext(new String[]{
                        String.valueOf(batch.getBatchNumber()),
                        String.valueOf(batch.getTotalDocuments()),
                        String.valueOf(batch.getSuccessCount()),
                        String.valueOf(batch.getFailedCount()),
                        String.format("%.1f%%", batch.getSuccessRate()),
                        String.valueOf(batch.getProcessingDurationMs())
                });
            }
            
            csvWriter.writeNext(new String[]{""});
            
            // Failed documents detail
            List<ProcessingResult> failedResults = result.getAllFailedResults();
            if (!failedResults.isEmpty()) {
                csvWriter.writeNext(new String[]{"=== FAILED DOCUMENTS DETAIL ==="});
                csvWriter.writeNext(new String[]{"ExternalID", "ClaimNumber", "ErrorMessage", 
                        "RetryAttempts"});
                
                for (ProcessingResult failedResult : failedResults) {
                    csvWriter.writeNext(new String[]{
                            failedResult.getDocument().getExternalId(),
                            failedResult.getDocument().getClaimNumber(),
                            truncateError(failedResult.getErrorMessage()),
                            String.valueOf(failedResult.getDocument().getRetryCount())
                    });
                }
            } else {
                csvWriter.writeNext(new String[]{"=== NO FAILED DOCUMENTS ==="});
            }
        }
        
        logger.info("Generated report: {}", reportPath);
        return reportPath;
    }
    
    /**
     * Generate a global summary report for all processed files.
     */
    public Path generateGlobalReport(List<FileProcessingResult> results, 
                                      BatchProcessorService.ProcessingStats stats,
                                      LocalDateTime startTime, 
                                      LocalDateTime endTime) throws IOException {
        
        String timestamp = DateUtils.getCurrentFileTimestamp();
        String reportFileName = String.format("GlobalMigrationReport_%s.csv", timestamp);
        
        Path reportPath = configManager.getReportFilePath(reportFileName);
        Files.createDirectories(reportPath.getParent());
        
        long totalDurationMs = DateUtils.getElapsedMs(startTime, endTime);
        
        try (Writer writer = new BufferedWriter(new OutputStreamWriter(
                new FileOutputStream(reportPath.toFile()), StandardCharsets.UTF_8));
             CSVWriter csvWriter = new CSVWriter(writer, ',',
                     ICSVWriter.DEFAULT_QUOTE_CHARACTER,
                     ICSVWriter.DEFAULT_ESCAPE_CHARACTER,
                     ICSVWriter.DEFAULT_LINE_END)) {
            
            // Global summary
            csvWriter.writeNext(new String[]{"=== GLOBAL MIGRATION SUMMARY ==="});
            csvWriter.writeNext(new String[]{""});
            
            csvWriter.writeNext(new String[]{"Metric", "Value"});
            csvWriter.writeNext(new String[]{"StartTime", DateUtils.formatForDisplay(startTime)});
            csvWriter.writeNext(new String[]{"EndTime", DateUtils.formatForDisplay(endTime)});
            csvWriter.writeNext(new String[]{"TotalDuration", DateUtils.formatDuration(totalDurationMs)});
            csvWriter.writeNext(new String[]{"TotalFilesProcessed", String.valueOf(stats.filesProcessed)});
            csvWriter.writeNext(new String[]{"TotalDocumentsProcessed", String.valueOf(stats.documentsProcessed)});
            csvWriter.writeNext(new String[]{"TotalSuccess", String.valueOf(stats.successCount)});
            csvWriter.writeNext(new String[]{"TotalFailed", String.valueOf(stats.failedCount)});
            csvWriter.writeNext(new String[]{"OverallSuccessRate", 
                    String.format("%.2f%%", stats.getSuccessRate())});
            
            if (totalDurationMs > 0) {
                double docsPerSecond = (double) stats.documentsProcessed / totalDurationMs * 1000;
                csvWriter.writeNext(new String[]{"OverallThroughput", 
                        String.format("%.2f docs/sec", docsPerSecond)});
            }
            
            csvWriter.writeNext(new String[]{""});
            
            // Per-file summary
            csvWriter.writeNext(new String[]{"=== FILE SUMMARY ==="});
            csvWriter.writeNext(new String[]{"FileName", "TotalDocs", "Success", "Failed", 
                    "SuccessRate", "DurationMs", "DocsPerSec"});
            
            for (FileProcessingResult result : results) {
                csvWriter.writeNext(new String[]{
                        result.getSourceFileName(),
                        String.valueOf(result.getTotalDocuments()),
                        String.valueOf(result.getSuccessCount()),
                        String.valueOf(result.getFailedCount()),
                        String.format("%.1f%%", result.getSuccessRate()),
                        String.valueOf(result.getProcessingDurationMs()),
                        String.format("%.2f", result.getDocumentsPerSecond())
                });
            }
            
            csvWriter.writeNext(new String[]{""});
            
            // Configuration used
            csvWriter.writeNext(new String[]{"=== CONFIGURATION ==="});
            csvWriter.writeNext(new String[]{"Setting", "Value"});
            csvWriter.writeNext(new String[]{"BatchSize", 
                    String.valueOf(configManager.getProperties().getBatchSize())});
            csvWriter.writeNext(new String[]{"ThreadPoolSize", 
                    String.valueOf(configManager.getProperties().getThreadPoolSize())});
            csvWriter.writeNext(new String[]{"RetryEnabled", 
                    String.valueOf(configManager.getProperties().isRetryEnabled())});
            csvWriter.writeNext(new String[]{"RetryMaxAttempts", 
                    String.valueOf(configManager.getProperties().getRetryMaxAttempts())});
            csvWriter.writeNext(new String[]{"FileNetObjectStore", 
                    configManager.getProperties().getFileNetObjectStore()});
            csvWriter.writeNext(new String[]{"DocumentClass", 
                    configManager.getMappingConfiguration().getDocumentClass()});
        }
        
        logger.info("Generated global report: {}", reportPath);
        return reportPath;
    }
    
    /**
     * Truncate error message for report.
     */
    private String truncateError(String error) {
        if (error == null) return "";
        if (error.length() > 200) {
            return error.substring(0, 200) + "...";
        }
        return error.replace("\n", " ").replace("\r", " ");
    }
}

package com.wawa.ace.migration.service;

import com.opencsv.exceptions.CsvValidationException;
import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.*;
import com.wawa.ace.migration.processor.CSVFileProcessor;
import com.wawa.ace.migration.tracker.DocumentTracker;
import com.wawa.ace.migration.util.DateUtils;
import com.wawa.ace.migration.util.FileUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.IOException;
import java.nio.channels.FileLock;
import java.nio.file.Path;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Service for batch processing of documents with multi-threading support.
 * Orchestrates the migration process for CSV files.
 */
public class BatchProcessorService implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(BatchProcessorService.class);
    
    private final ConfigurationManager configManager;
    private final FileNetService fileNetService;
    private final DatabaseService databaseService;
    private final DocumentTracker documentTracker;
    private final CSVFileProcessor csvProcessor;
    
    private final ExecutorService executorService;
    private final int batchSize;
    private final int threadPoolSize;
    
    // Progress tracking
    private final AtomicInteger totalFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalDocumentsProcessed = new AtomicInteger(0);
    private final AtomicInteger totalSuccessCount = new AtomicInteger(0);
    private final AtomicInteger totalFailedCount = new AtomicInteger(0);
    
    public BatchProcessorService(ConfigurationManager configManager,
                                  FileNetService fileNetService,
                                  DatabaseService databaseService,
                                  DocumentTracker documentTracker) {
        this.configManager = configManager;
        this.fileNetService = fileNetService;
        this.databaseService = databaseService;
        this.documentTracker = documentTracker;
        this.csvProcessor = new CSVFileProcessor(configManager);
        
        this.batchSize = configManager.getProperties().getBatchSize();
        this.threadPoolSize = configManager.getProperties().getThreadPoolSize();
        
        // Create thread pool
        this.executorService = Executors.newFixedThreadPool(threadPoolSize, new ThreadFactory() {
            private final AtomicInteger counter = new AtomicInteger(0);
            
            @Override
            public Thread newThread(Runnable r) {
                Thread thread = new Thread(r);
                thread.setName("BatchProcessor-" + counter.incrementAndGet());
                thread.setDaemon(false);
                return thread;
            }
        });
        
        logger.info("BatchProcessorService initialized with {} threads, batch size {}", 
                threadPoolSize, batchSize);
    }
    
    /**
     * Process all CSV files in the source folder.
     */
    public List<FileProcessingResult> processAllFiles() throws IOException {
        List<FileProcessingResult> results = new ArrayList<>();
        
        Path sourceFolder = configManager.getProperties().getFullSourcePath();
        List<Path> csvFiles = FileUtils.getCsvFiles(sourceFolder);
        
        if (csvFiles.isEmpty()) {
            logger.info("No CSV files found in source folder: {}", sourceFolder);
            return results;
        }
        
        logger.info("Found {} CSV files to process", csvFiles.size());
        
        for (Path csvFile : csvFiles) {
            try {
                FileProcessingResult result = processFile(csvFile);
                results.add(result);
            } catch (Exception e) {
                logger.error("Failed to process file: {}", csvFile, e);
            }
        }
        
        logFinalSummary();
        return results;
    }
    
    /**
     * Process a single CSV file.
     */
    public FileProcessingResult processFile(Path csvFile) throws IOException {
        String fileName = csvFile.getFileName().toString();
        logger.info("=== Starting processing of file: {} ===", fileName);
        
        LocalDateTime startTime = LocalDateTime.now();
        
        // Try to acquire file lock to verify no other process is using it
        FileLock lock = FileUtils.tryLock(csvFile);
        if (lock == null) {
            logger.warn("Could not acquire lock on file: {}", csvFile);
            return FileProcessingResult.builder(fileName)
                    .totalDocuments(0)
                    .totalBatches(0)
                    .startTime(startTime)
                    .endTime(LocalDateTime.now())
                    .processingDurationMs(0)
                    .build();
        }
        
        // Release lock immediately so we can move the file
        // The lock was only to verify no other process is using the file
        FileUtils.releaseLock(lock);
        lock = null;
        
        try {
            // Move file to inprocess folder
            Path inprocessFile = configManager.getInprocessFilePath(fileName);
            FileUtils.moveFile(csvFile, inprocessFile);
            logger.info("Moved file to inprocess: {}", inprocessFile);
            
            // Initialize document tracker for this file
            documentTracker.initializeForFile(fileName);
            
            // Count total rows
            int totalRows;
            try {
                totalRows = csvProcessor.countRows(inprocessFile);
            } catch (CsvValidationException e) {
                throw new IOException("CSV validation error while counting rows: " + e.getMessage(), e);
            }
            logger.info("Total documents to process: {}", totalRows);
            
            // Calculate number of batches
            int totalBatches = (int) Math.ceil((double) totalRows / batchSize);
            logger.info("Processing in {} batches of {} documents each", totalBatches, batchSize);
            
            // Process in batches
            List<BatchResult> batchResults = new ArrayList<>();
            List<Future<BatchResult>> futures = new ArrayList<>();
            
            try (CSVFileProcessor.DocumentBatchIterator iterator = 
                    csvProcessor.readDocumentsInBatches(inprocessFile, batchSize)) {
                
                int batchNumber = 0;
                
                while (iterator.hasNext()) {
                    batchNumber++;
                    List<MigrationDocument> batch = iterator.next();
                    
                    if (batch.isEmpty()) {
                        continue;
                    }
                    
                    final int currentBatchNum = batchNumber;
                    final List<MigrationDocument> currentBatch = new ArrayList<>(batch);
                    
                    // Submit batch for processing
                    Future<BatchResult> future = executorService.submit(() -> 
                            processBatch(currentBatchNum, currentBatch, fileName));
                    futures.add(future);
                }
            }
            
            // Wait for all batches to complete
            for (Future<BatchResult> future : futures) {
                try {
                    BatchResult batchResult = future.get();
                    batchResults.add(batchResult);
                    
                    // Update counters
                    totalDocumentsProcessed.addAndGet(batchResult.getTotalDocuments());
                    totalSuccessCount.addAndGet(batchResult.getSuccessCount());
                    totalFailedCount.addAndGet(batchResult.getFailedCount());
                    
                    // Log progress
                    logProgress(batchResult);
                    
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    logger.error("Batch processing interrupted", e);
                } catch (ExecutionException e) {
                    logger.error("Batch processing failed", e.getCause());
                }
            }
            
            // Finalize document tracker (close output files)
            documentTracker.finalizeForFile(fileName);
            
            LocalDateTime endTime = LocalDateTime.now();
            long durationMs = DateUtils.getElapsedMs(startTime, endTime);
            
            // Move file to archive
            Path archiveFile = configManager.getArchiveFilePath(fileName);
            FileUtils.moveFile(inprocessFile, archiveFile);
            logger.info("Moved file to archive: {}", archiveFile);
            
            totalFilesProcessed.incrementAndGet();
            
            // Build result
            FileProcessingResult result = FileProcessingResult.builder(fileName)
                    .totalDocuments(totalRows)
                    .totalBatches(totalBatches)
                    .startTime(startTime)
                    .endTime(endTime)
                    .processingDurationMs(durationMs)
                    .build();
            
            for (BatchResult br : batchResults) {
                result = FileProcessingResult.builder(fileName)
                        .totalDocuments(result.getTotalDocuments())
                        .totalBatches(result.getTotalBatches())
                        .startTime(result.getStartTime())
                        .endTime(result.getEndTime())
                        .processingDurationMs(result.getProcessingDurationMs())
                        .addBatchResult(br)
                        .build();
            }
            
            logger.info("=== Completed processing of file: {} ===", fileName);
            logger.info("Summary: {} documents, {} success, {} failed, {} ms",
                    totalRows, 
                    batchResults.stream().mapToInt(BatchResult::getSuccessCount).sum(),
                    batchResults.stream().mapToInt(BatchResult::getFailedCount).sum(),
                    durationMs);
            
            return result;
            
        } finally {
            // Lock was already released before file move, but release again just in case
            if (lock != null) {
                FileUtils.releaseLock(lock);
            }
        }
    }
    
    /**
     * Process a single batch of documents with retry support.
     */
    private BatchResult processBatch(int batchNumber, List<MigrationDocument> documents, String sourceFileName) {
        logger.info("Processing batch {} with {} documents", batchNumber, documents.size());
        
        LocalDateTime startTime = LocalDateTime.now();
        
        // Check for duplicates
        try {
            List<String> externalIds = documents.stream()
                    .map(MigrationDocument::getExternalId)
                    .toList();
            
            List<String> duplicates = databaseService.checkDuplicates(externalIds);
            Set<String> duplicateSet = Set.copyOf(duplicates);
            
            // Filter out duplicates
            if (!duplicates.isEmpty()) {
                logger.warn("Batch {} contains {} duplicate documents, skipping them", 
                        batchNumber, duplicates.size());
                
                List<MigrationDocument> filtered = new ArrayList<>();
                for (MigrationDocument doc : documents) {
                    if (duplicateSet.contains(doc.getExternalId())) {
                        doc.setStatus(ProcessingStatus.SKIPPED);
                        doc.setErrorMessage("Duplicate document - already exists in database");
                        documentTracker.trackFailed(doc, "Duplicate document", "", sourceFileName);
                    } else {
                        filtered.add(doc);
                    }
                }
                documents = filtered;
            }
        } catch (Exception e) {
            logger.warn("Failed to check duplicates for batch {}: {}", batchNumber, e.getMessage());
        }
        
        if (documents.isEmpty()) {
            return BatchResult.builder(batchNumber, 0)
                    .startTime(startTime)
                    .endTime(LocalDateTime.now())
                    .processingDurationMs(DateUtils.getElapsedMs(startTime))
                    .build();
        }
        
        // Process documents in batch WITH RETRY
        List<ProcessingResult> results = processBatchWithRetry(batchNumber, documents, sourceFileName);
        
        // Track results
        BatchResult.Builder batchResultBuilder = BatchResult.builder(batchNumber, documents.size())
                .startTime(startTime);
        
        List<ProcessingResult> successResults = new ArrayList<>();
        
        for (ProcessingResult result : results) {
            batchResultBuilder.addResult(result);
            
            if (result.isSuccess()) {
                documentTracker.trackSuccess(result, sourceFileName);
                successResults.add(result);
            } else {
                MigrationDocument doc = result.getDocument();
                documentTracker.trackFailed(doc, result.getErrorMessage(), 
                        result.getErrorStackTrace(), sourceFileName);
            }
        }
        
        // Insert successful documents into database with retry
        if (!successResults.isEmpty()) {
            insertWithRetry(successResults, sourceFileName, batchNumber);
        }
        
        LocalDateTime endTime = LocalDateTime.now();
        
        BatchResult batchResult = batchResultBuilder
                .endTime(endTime)
                .processingDurationMs(DateUtils.getElapsedMs(startTime, endTime))
                .build();
        
        logger.info("Batch {} completed: {} success, {} failed, {} ms",
                batchNumber, batchResult.getSuccessCount(), batchResult.getFailedCount(),
                batchResult.getProcessingDurationMs());
        
        return batchResult;
    }
    
    /**
     * Process batch with retry logic for transient failures.
     */
    private List<ProcessingResult> processBatchWithRetry(int batchNumber, 
            List<MigrationDocument> documents, String sourceFileName) {
        
        int maxRetries = configManager.getProperties().isRetryEnabled() 
                ? configManager.getProperties().getRetryMaxAttempts() : 1;
        long retryDelay = configManager.getProperties().getRetryDelayMilliseconds();
        
        List<MigrationDocument> remaining = new ArrayList<>(documents);
        List<ProcessingResult> allResults = new ArrayList<>();
        
        for (int attempt = 1; attempt <= maxRetries && !remaining.isEmpty(); attempt++) {
            if (attempt > 1) {
                logger.info("Batch {} - Retry attempt {} for {} documents", 
                        batchNumber, attempt, remaining.size());
                try {
                    Thread.sleep(retryDelay);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                }
            }
            
            List<ProcessingResult> results = fileNetService.createDocumentsBatch(remaining);
            
            List<MigrationDocument> failedDocs = new ArrayList<>();
            
            for (ProcessingResult result : results) {
                if (result.isSuccess()) {
                    allResults.add(result);
                } else {
                    // Check if error is retryable
                    if (attempt < maxRetries && isRetryableError(result.getErrorMessage())) {
                        failedDocs.add(result.getDocument());
                    } else {
                        allResults.add(result);
                    }
                }
            }
            
            remaining = failedDocs;
        }
        
        return allResults;
    }
    
    /**
     * Check if an error is retryable (transient).
     */
    private boolean isRetryableError(String errorMessage) {
        if (errorMessage == null) return false;
        String lower = errorMessage.toLowerCase();
        return lower.contains("timeout") 
            || lower.contains("connection") 
            || lower.contains("temporarily")
            || lower.contains("unavailable")
            || lower.contains("network")
            || lower.contains("socket");
    }
    
    /**
     * Insert into database with retry.
     */
    private void insertWithRetry(List<ProcessingResult> successResults, 
            String sourceFileName, int batchNumber) {
        int maxRetries = 3;
        
        for (int attempt = 1; attempt <= maxRetries; attempt++) {
            try {
                databaseService.insertDocumentsBatch(successResults, sourceFileName);
                return; // Success
            } catch (Exception e) {
                logger.error("Failed to insert batch {} into database (attempt {}): {}", 
                        batchNumber, attempt, e.getMessage());
                
                if (attempt < maxRetries) {
                    try {
                        Thread.sleep(2000 * attempt); // Exponential backoff
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                } else {
                    // Final failure - log critical error
                    logger.error("CRITICAL: Batch {} documents created in FileNet but NOT tracked in database!", 
                            batchNumber);
                    logger.error("Affected ExternalIDs: {}", 
                            successResults.stream()
                                .map(r -> r.getDocument().getExternalId())
                                .toList());
                }
            }
        }
    }
    
    private void logProgress(BatchResult batchResult) {
        int total = totalDocumentsProcessed.get();
        int success = totalSuccessCount.get();
        int failed = totalFailedCount.get();
        double rate = total > 0 ? (double) success / total * 100 : 0;
        
        logger.info("Progress: {} total, {} success ({}%), {} failed",
                total, success, String.format("%.1f", rate), failed);
    }
    
    private void logFinalSummary() {
        logger.info("========================================");
        logger.info("FINAL MIGRATION SUMMARY");
        logger.info("========================================");
        logger.info("Total files processed: {}", totalFilesProcessed.get());
        logger.info("Total documents processed: {}", totalDocumentsProcessed.get());
        logger.info("Total success: {}", totalSuccessCount.get());
        logger.info("Total failed: {}", totalFailedCount.get());
        
        int total = totalDocumentsProcessed.get();
        double successRate = total > 0 ? (double) totalSuccessCount.get() / total * 100 : 0;
        logger.info("Success rate: {}%", String.format("%.2f", successRate));
        logger.info("========================================");
    }
    
    /**
     * Get current processing statistics.
     */
    public ProcessingStats getStats() {
        return new ProcessingStats(
                totalFilesProcessed.get(),
                totalDocumentsProcessed.get(),
                totalSuccessCount.get(),
                totalFailedCount.get()
        );
    }
    
    @Override
    public void close() {
        logger.info("Shutting down BatchProcessorService...");
        
        executorService.shutdown();
        try {
            int timeout = configManager.getProperties().getThreadPoolShutdownTimeoutSeconds();
            if (!executorService.awaitTermination(timeout, TimeUnit.SECONDS)) {
                logger.warn("Thread pool did not terminate in {} seconds, forcing shutdown", timeout);
                executorService.shutdownNow();
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            executorService.shutdownNow();
        }
        
        logger.info("BatchProcessorService shut down");
    }
    
    /**
     * Simple class to hold processing statistics.
     */
    public static class ProcessingStats {
        public final int filesProcessed;
        public final int documentsProcessed;
        public final int successCount;
        public final int failedCount;
        
        public ProcessingStats(int filesProcessed, int documentsProcessed, 
                               int successCount, int failedCount) {
            this.filesProcessed = filesProcessed;
            this.documentsProcessed = documentsProcessed;
            this.successCount = successCount;
            this.failedCount = failedCount;
        }
        
        public double getSuccessRate() {
            return documentsProcessed > 0 ? (double) successCount / documentsProcessed * 100 : 0;
        }
        
        @Override
        public String toString() {
            return String.format("Stats{files=%d, docs=%d, success=%d, failed=%d, rate=%.1f%%}",
                    filesProcessed, documentsProcessed, successCount, failedCount, getSuccessRate());
        }
    }
}

package com.wawa.ace.migration.service;

import com.wawa.ace.migration.config.MigrationProperties;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.util.PropertyConverter;
import com.zaxxer.hikari.HikariConfig;
import com.zaxxer.hikari.HikariDataSource;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.sql.*;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;

/**
 * Service for database operations using SQL Server.
 * Manages connection pooling and tracking of migrated documents.
 * 
 * Table Schema:
 * CREATE TABLE WawaMigratedDocuments (
 *     id BIGINT IDENTITY(1,1) PRIMARY KEY,
 *     claimNumber VARCHAR(20) NOT NULL,
 *     claimID BIGINT NOT NULL,
 *     externalID VARCHAR(100) NOT NULL UNIQUE,
 *     batchID VARCHAR(50) NOT NULL,
 *     document_GUID VARCHAR(50) NOT NULL,
 *     DateCreated DATETIME NOT NULL DEFAULT GETDATE(),
 *     packaging_filename VARCHAR(50) NOT NULL,
 *     Status VARCHAR(10) NOT NULL
 * );
 */
public class DatabaseService implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(DatabaseService.class);
    
    // Column length constants matching actual DB schema
    private static final int MAX_CLAIM_NUMBER_LENGTH = 20;
    private static final int MAX_EXTERNAL_ID_LENGTH = 100;
    private static final int MAX_BATCH_ID_LENGTH = 50;
    private static final int MAX_DOCUMENT_GUID_LENGTH = 50;
    private static final int MAX_PACKAGING_FILENAME_LENGTH = 50;
    private static final int MAX_STATUS_LENGTH = 10;
    
    private final MigrationProperties properties;
    private final HikariDataSource dataSource;
    private final String tableName;
    
    // Pre-formatted SQL statements for performance
    private final String insertDocumentSql;
    private final String checkDuplicateSql;
    private final String updateStatusSql;
    
    public DatabaseService(MigrationProperties properties) throws SQLException {
        this.properties = properties;
        this.tableName = properties.getDbTable();
        
        // Pre-format SQL statements
        this.insertDocumentSql = String.format(
                "INSERT INTO %s (claimNumber, claimID, externalID, batchID, document_GUID, DateCreated, packaging_filename, Status) " +
                "VALUES (?, ?, ?, ?, ?, ?, ?, ?)", tableName);
        this.checkDuplicateSql = String.format("SELECT COUNT(*) FROM %s WHERE externalID = ?", tableName);
        this.updateStatusSql = String.format(
                "UPDATE %s SET Status = ?, document_GUID = ?, DateCreated = ? WHERE externalID = ?", tableName);
        
        this.dataSource = createDataSource();
        
        // Verify connection
        try (Connection conn = dataSource.getConnection()) {
            logger.info("Database connection established successfully");
            verifyTableExists(conn);
        }
    }
    
    private HikariDataSource createDataSource() {
        HikariConfig config = new HikariConfig();
        
        // Build connection URL
        String jdbcUrl;
        if (properties.isDbIntegratedSecurity()) {
            // Windows Integrated Authentication
            jdbcUrl = String.format(
                    "jdbc:sqlserver://%s:%d;databaseName=%s;integratedSecurity=true;trustServerCertificate=true;encrypt=false;",
                    properties.getDbServer(),
                    properties.getDbPort(),
                    properties.getDbName()
            );
            logger.info("Using Windows Integrated Authentication");
        } else {
            // SQL Server Authentication with username/password
            jdbcUrl = String.format(
                    "jdbc:sqlserver://%s:%d;databaseName=%s;trustServerCertificate=true;encrypt=false;",
                    properties.getDbServer(),
                    properties.getDbPort(),
                    properties.getDbName()
            );
            // Set username and password for SQL Server authentication
            config.setUsername(properties.getDbUsername());
            config.setPassword(properties.getDbPassword());
            logger.info("Using SQL Server Authentication with username: {}", properties.getDbUsername());
        }
        
        config.setJdbcUrl(jdbcUrl);
        config.setDriverClassName("com.microsoft.sqlserver.jdbc.SQLServerDriver");
        
        // Pool settings optimized for high-volume processing
        config.setMaximumPoolSize(properties.getDbPoolSize());
        config.setMinimumIdle(Math.min(5, properties.getDbPoolSize()));
        config.setConnectionTimeout(properties.getDbConnectionTimeoutSeconds() * 1000L);
        config.setIdleTimeout(300000); // 5 minutes
        config.setMaxLifetime(1800000); // 30 minutes
        config.setValidationTimeout(5000); // 5 seconds
        
        // Performance optimizations
        config.addDataSourceProperty("cachePrepStmts", "true");
        config.addDataSourceProperty("prepStmtCacheSize", "250");
        config.addDataSourceProperty("prepStmtCacheSqlLimit", "2048");
        config.addDataSourceProperty("useServerPrepStmts", "true");
        config.addDataSourceProperty("rewriteBatchedStatements", "true");
        
        // Connection test
        config.setConnectionTestQuery("SELECT 1");
        
        // Pool name for monitoring
        config.setPoolName("WawaMigration-Pool");
        
        // Leak detection (helpful for debugging)
        config.setLeakDetectionThreshold(60000); // 1 minute
        
        logger.info("Creating database connection pool: {}", jdbcUrl);
        
        return new HikariDataSource(config);
    }
    
    /**
     * Verify the tracking table exists (does not create it).
     */
    private void verifyTableExists(Connection conn) throws SQLException {
        String checkSql = String.format(
                "SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '%s'", tableName);
        
        try (Statement stmt = conn.createStatement();
             ResultSet rs = stmt.executeQuery(checkSql)) {
            
            if (rs.next() && rs.getInt(1) > 0) {
                logger.info("Table {} exists and is ready", tableName);
            } else {
                logger.warn("Table {} does not exist! Please create it before running migration.", tableName);
                throw new SQLException("Required table " + tableName + " does not exist");
            }
        }
    }
    
    /**
     * Check if a document with the given externalID already exists.
     */
    public boolean isDuplicate(String externalId) throws SQLException {
        try (Connection conn = dataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(checkDuplicateSql)) {
            
            stmt.setString(1, truncate(externalId, MAX_EXTERNAL_ID_LENGTH));
            
            try (ResultSet rs = stmt.executeQuery()) {
                if (rs.next()) {
                    return rs.getInt(1) > 0;
                }
            }
        }
        return false;
    }
    
    /**
     * Check for duplicates in batch.
     * Returns list of external IDs that already exist.
     */
    public List<String> checkDuplicates(List<String> externalIds) throws SQLException {
        List<String> duplicates = new ArrayList<>();
        
        if (externalIds == null || externalIds.isEmpty()) {
            return duplicates;
        }
        
        // Process in chunks to avoid SQL parameter limits (max ~2100 in SQL Server)
        int chunkSize = 1000;
        for (int i = 0; i < externalIds.size(); i += chunkSize) {
            int end = Math.min(i + chunkSize, externalIds.size());
            List<String> chunk = externalIds.subList(i, end);
            duplicates.addAll(checkDuplicatesChunk(chunk));
        }
        
        if (!duplicates.isEmpty()) {
            logger.warn("Found {} duplicate external IDs", duplicates.size());
        }
        
        return duplicates;
    }
    
    private List<String> checkDuplicatesChunk(List<String> externalIds) throws SQLException {
        List<String> duplicates = new ArrayList<>();
        
        // Build IN clause with proper parameterization
        StringBuilder inClause = new StringBuilder();
        for (int i = 0; i < externalIds.size(); i++) {
            if (i > 0) inClause.append(",");
            inClause.append("?");
        }
        
        String sql = String.format(
                "SELECT externalID FROM %s WITH (NOLOCK) WHERE externalID IN (%s)",
                tableName, inClause);
        
        try (Connection conn = dataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(sql)) {
            
            for (int i = 0; i < externalIds.size(); i++) {
                stmt.setString(i + 1, truncate(externalIds.get(i), MAX_EXTERNAL_ID_LENGTH));
            }
            
            try (ResultSet rs = stmt.executeQuery()) {
                while (rs.next()) {
                    duplicates.add(rs.getString("externalID"));
                }
            }
        }
        
        return duplicates;
    }
    
    /**
     * Insert a successfully processed document record.
     */
    public void insertDocument(ProcessingResult result, String packagingFilename) throws SQLException {
        MigrationDocument doc = result.getDocument();
        
        try (Connection conn = dataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(insertDocumentSql)) {
            
            setInsertParameters(stmt, doc, result.getDocumentGuid(), packagingFilename, 
                    result.isSuccess() ? "SUCCESS" : "FAILED");
            
            stmt.executeUpdate();
            logger.debug("Inserted document record: {}", doc.getExternalId());
        }
    }
    
    /**
     * Batch insert multiple document records.
     * Optimized for high-volume processing with proper resource management.
     */
    public void insertDocumentsBatch(List<ProcessingResult> results, String packagingFilename) throws SQLException {
        if (results == null || results.isEmpty()) {
            return;
        }
        
        // Filter only successful results
        List<ProcessingResult> successResults = results.stream()
                .filter(ProcessingResult::isSuccess)
                .toList();
        
        if (successResults.isEmpty()) {
            return;
        }
        
        Connection conn = null;
        PreparedStatement stmt = null;
        
        try {
            conn = dataSource.getConnection();
            conn.setAutoCommit(false);
            
            stmt = conn.prepareStatement(insertDocumentSql);
            
            int batchCount = 0;
            int totalInserted = 0;
            
            for (ProcessingResult result : successResults) {
                MigrationDocument doc = result.getDocument();
                
                setInsertParameters(stmt, doc, result.getDocumentGuid(), packagingFilename, "SUCCESS");
                stmt.addBatch();
                batchCount++;
                
                // Execute batch every 500 records to manage memory
                if (batchCount >= 500) {
                    int[] counts = stmt.executeBatch();
                    totalInserted += countSuccessful(counts);
                    stmt.clearBatch();
                    batchCount = 0;
                }
            }
            
            // Execute remaining batch
            if (batchCount > 0) {
                int[] counts = stmt.executeBatch();
                totalInserted += countSuccessful(counts);
            }
            
            conn.commit();
            logger.info("Batch inserted {} document records", totalInserted);
            
        } catch (SQLException e) {
            if (conn != null) {
                try {
                    conn.rollback();
                } catch (SQLException rollbackEx) {
                    logger.error("Error during rollback", rollbackEx);
                }
            }
            throw e;
        } finally {
            // Proper resource cleanup
            if (stmt != null) {
                try {
                    stmt.close();
                } catch (SQLException e) {
                    logger.warn("Error closing statement", e);
                }
            }
            if (conn != null) {
                try {
                    conn.setAutoCommit(true);
                    conn.close();
                } catch (SQLException e) {
                    logger.warn("Error closing connection", e);
                }
            }
        }
    }
    
    /**
     * Set parameters for INSERT statement.
     */
    private void setInsertParameters(PreparedStatement stmt, MigrationDocument doc, 
                                      String documentGuid, String packagingFilename, 
                                      String status) throws SQLException {
        // claimNumber VARCHAR(20)
        stmt.setString(1, truncate(doc.getClaimNumber(), MAX_CLAIM_NUMBER_LENGTH));
        
        // claimID BIGINT
        Long claimId = PropertyConverter.parseLongForDb(doc.getClaimId());
        stmt.setLong(2, claimId != null ? claimId : 0L);
        
        // externalID VARCHAR(100)
        stmt.setString(3, truncate(doc.getExternalId(), MAX_EXTERNAL_ID_LENGTH));
        
        // batchID VARCHAR(50)
        stmt.setString(4, truncate(doc.getBatchId(), MAX_BATCH_ID_LENGTH));
        
        // document_GUID VARCHAR(50)
        stmt.setString(5, truncate(documentGuid, MAX_DOCUMENT_GUID_LENGTH));
        
        // DateCreated DATETIME
        stmt.setTimestamp(6, Timestamp.valueOf(LocalDateTime.now()));
        
        // packaging_filename VARCHAR(50)
        stmt.setString(7, truncate(packagingFilename, MAX_PACKAGING_FILENAME_LENGTH));
        
        // Status VARCHAR(10)
        stmt.setString(8, truncate(status, MAX_STATUS_LENGTH));
    }
    
    /**
     * Count successful batch inserts.
     */
    private int countSuccessful(int[] counts) {
        int count = 0;
        for (int c : counts) {
            if (c >= 0 || c == Statement.SUCCESS_NO_INFO) {
                count++;
            }
        }
        return count;
    }
    
    /**
     * Update the status of an existing document.
     */
    public void updateDocumentStatus(String externalId, String status, String documentGuid) throws SQLException {
        try (Connection conn = dataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(updateStatusSql)) {
            
            stmt.setString(1, truncate(status, MAX_STATUS_LENGTH));
            stmt.setString(2, truncate(documentGuid, MAX_DOCUMENT_GUID_LENGTH));
            stmt.setTimestamp(3, Timestamp.valueOf(LocalDateTime.now()));
            stmt.setString(4, truncate(externalId, MAX_EXTERNAL_ID_LENGTH));
            
            int updated = stmt.executeUpdate();
            if (updated > 0) {
                logger.debug("Updated document status: {} -> {}", externalId, status);
            }
        }
    }
    
    /**
     * Truncate string to max length, handling null.
     */
    private String truncate(String value, int maxLength) {
        if (value == null) {
            return null;
        }
        return value.length() <= maxLength ? value : value.substring(0, maxLength);
    }
    
    /**
     * Get connection pool statistics for monitoring.
     */
    public String getPoolStats() {
        if (dataSource.getHikariPoolMXBean() != null) {
            return String.format(
                    "Pool Stats: Active=%d, Idle=%d, Total=%d, Waiting=%d",
                    dataSource.getHikariPoolMXBean().getActiveConnections(),
                    dataSource.getHikariPoolMXBean().getIdleConnections(),
                    dataSource.getHikariPoolMXBean().getTotalConnections(),
                    dataSource.getHikariPoolMXBean().getThreadsAwaitingConnection()
            );
        }
        return "Pool Stats: N/A";
    }
    
    /**
     * Test database connection.
     */
    public boolean testConnection() {
        try (Connection conn = dataSource.getConnection()) {
            return conn.isValid(5);
        } catch (SQLException e) {
            logger.error("Database connection test failed", e);
            return false;
        }
    }
    
    /**
     * Check if connection pool is healthy.
     */
    public boolean isHealthy() {
        return dataSource != null && !dataSource.isClosed() && testConnection();
    }
    
    @Override
    public void close() {
        if (dataSource != null && !dataSource.isClosed()) {
            logger.info("Closing database connection pool...");
            logger.info("Final pool stats: {}", getPoolStats());
            dataSource.close();
            logger.info("Database connection pool closed");
        }
    }
}



package com.wawa.ace.migration.service;

import com.wawa.ace.migration.config.MappingConfiguration;
import com.wawa.ace.migration.config.MigrationProperties;
import com.wawa.ace.migration.config.PropertyMapping;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.util.FileUtils;
import com.wawa.ace.migration.util.PropertyConverter;

import com.filenet.api.collection.ContentElementList;
import com.filenet.api.constants.AutoClassify;
import com.filenet.api.constants.CheckinType;
import com.filenet.api.constants.RefreshMode;
import com.filenet.api.core.*;
import com.filenet.api.property.Properties;
import com.filenet.api.util.Id;
import com.filenet.api.util.UserContext;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import javax.security.auth.Subject;
import java.io.FileInputStream;
import java.io.InputStream;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Map;

/**
 * Service for FileNet Content Engine operations.
 * Handles document creation, property setting, and content attachment.
 * 
 * Memory-optimized for high-volume document migration.
 */
public class FileNetService implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(FileNetService.class);
    
    private final MigrationProperties properties;
    private final MappingConfiguration mappingConfig;
    private final Connection connection;
    private final Domain domain;
    private final ObjectStore objectStore;
    private final Subject subject;
    
    // Thread-local for tracking if subject is pushed
    private final ThreadLocal<Boolean> subjectPushed = ThreadLocal.withInitial(() -> Boolean.FALSE);
    
    public FileNetService(MigrationProperties properties, MappingConfiguration mappingConfig) {
        this.properties = properties;
        this.mappingConfig = mappingConfig;
        
        logger.info("Connecting to FileNet CE: {}", properties.getFileNetUri());
        
        // Establish connection
        this.connection = Factory.Connection.getConnection(properties.getFileNetUri());
        
        // Authenticate
        this.subject = UserContext.createSubject(
                connection,
                properties.getFileNetUsername(),
                properties.getFileNetPassword(),
                properties.getFileNetStanza()
        );
        
        UserContext.get().pushSubject(subject);
        subjectPushed.set(Boolean.TRUE);
        
        // Get domain and object store
        this.domain = Factory.Domain.fetchInstance(connection, null, null);
        logger.info("Connected to domain: {}", domain.get_Name());
        
        this.objectStore = Factory.ObjectStore.fetchInstance(
                domain,
                properties.getFileNetObjectStore(),
                null
        );
        logger.info("Connected to object store: {}", objectStore.get_Name());
    }
    
    /**
     * Ensure subject is pushed for current thread.
     * Required for multi-threaded batch processing.
     */
    private void ensureSubjectPushed() {
        if (!Boolean.TRUE.equals(subjectPushed.get())) {
            UserContext.get().pushSubject(subject);
            subjectPushed.set(Boolean.TRUE);
        }
    }
    
    /**
     * Create a single document in FileNet.
     */
    public ProcessingResult createDocument(MigrationDocument migrationDoc) {
        long startTime = System.currentTimeMillis();
        
        try {
            ensureSubjectPushed();
            
            // Validate content file exists
            String contentPath = migrationDoc.getContentFilePath();
            if (contentPath == null || contentPath.isEmpty()) {
                return ProcessingResult.failure(migrationDoc, "Content file path is not specified")
                        .processingDurationMs(System.currentTimeMillis() - startTime)
                        .build();
            }
            
            Path contentFile = Paths.get(contentPath);
            if (!FileUtils.exists(contentFile)) {
                return ProcessingResult.failure(migrationDoc, "Content file not found: " + contentPath)
                        .processingDurationMs(System.currentTimeMillis() - startTime)
                        .build();
            }
            
            // Create document
            String documentClass = mappingConfig.getDocumentClass();
            Document document = Factory.Document.createInstance(
                    objectStore,
                    documentClass
            );
            
            // Set properties
            setDocumentProperties(document, migrationDoc);
            
            // Set document title
            if (migrationDoc.getDocumentTitle() != null && !migrationDoc.getDocumentTitle().isEmpty()) {
                document.getProperties().putValue("DocumentTitle", migrationDoc.getDocumentTitle());
            }
            
            // Attach content (memory-efficient)
            attachContent(document, migrationDoc);
            
            // Check in the document
            document.checkin(AutoClassify.DO_NOT_AUTO_CLASSIFY, CheckinType.MAJOR_VERSION);
            
            // Save the document
            document.save(RefreshMode.REFRESH);
            
            // Get the document GUID
            Id docId = document.get_Id();
            String documentGuid = docId.toString();
            
            logger.debug("Created document: {} with GUID: {}", migrationDoc.getExternalId(), documentGuid);
            
            return ProcessingResult.success(migrationDoc, documentGuid)
                    .processingDurationMs(System.currentTimeMillis() - startTime)
                    .build();
            
        } catch (Exception e) {
            logger.error("Failed to create document: {}", migrationDoc.getExternalId(), e);
            return ProcessingResult.failure(migrationDoc, e)
                    .processingDurationMs(System.currentTimeMillis() - startTime)
                    .build();
        }
    }
    
    /**
     * Create multiple documents in a batch.
     * This method prepares all documents and commits them in a single batch operation.
     * Memory-optimized to handle large batches.
     */
    public List<ProcessingResult> createDocumentsBatch(List<MigrationDocument> documents) {
        List<ProcessingResult> results = new ArrayList<>();
        List<Document> pendingDocuments = new ArrayList<>();
        List<MigrationDocument> pendingMigrationDocs = new ArrayList<>();
        
        long batchStartTime = System.currentTimeMillis();
        
        try {
            ensureSubjectPushed();
            
            // Prepare all documents
            for (MigrationDocument migrationDoc : documents) {
                long docStartTime = System.currentTimeMillis();
                
                try {
                    // Validate content file exists
                    String contentPath = migrationDoc.getContentFilePath();
                    if (contentPath == null || contentPath.isEmpty()) {
                        results.add(ProcessingResult.failure(migrationDoc, "Content file path is not specified")
                                .processingDurationMs(System.currentTimeMillis() - docStartTime)
                                .build());
                        continue;
                    }
                    
                    Path contentFile = Paths.get(contentPath);
                    if (!FileUtils.exists(contentFile)) {
                        results.add(ProcessingResult.failure(migrationDoc, "Content file not found: " + contentPath)
                                .processingDurationMs(System.currentTimeMillis() - docStartTime)
                                .build());
                        continue;
                    }
                    
                    // Create document
                    String documentClass = mappingConfig.getDocumentClass();
                    Document document = Factory.Document.createInstance(
                            objectStore,
                            documentClass
                    );
                    
                    // Set properties
                    setDocumentProperties(document, migrationDoc);
                    
                    // Set document title
                    if (migrationDoc.getDocumentTitle() != null && !migrationDoc.getDocumentTitle().isEmpty()) {
                        document.getProperties().putValue("DocumentTitle", migrationDoc.getDocumentTitle());
                    }
                    
                    // Attach content (memory-efficient)
                    attachContent(document, migrationDoc);
                    
                    // Check in the document
                    document.checkin(AutoClassify.DO_NOT_AUTO_CLASSIFY, CheckinType.MAJOR_VERSION);
                    
                    // Add to pending list
                    pendingDocuments.add(document);
                    pendingMigrationDocs.add(migrationDoc);
                    
                } catch (Exception e) {
                    logger.error("Failed to prepare document: {}", migrationDoc.getExternalId(), e);
                    results.add(ProcessingResult.failure(migrationDoc, e)
                            .processingDurationMs(System.currentTimeMillis() - docStartTime)
                            .build());
                }
            }
            
            // Execute batch save using UpdatingBatch
            if (!pendingDocuments.isEmpty()) {
                executeBatchSave(pendingDocuments, pendingMigrationDocs, results, batchStartTime);
            }
            
        } catch (Exception e) {
            logger.error("Batch processing error", e);
            // Mark all remaining pending documents as failed
            for (MigrationDocument migrationDoc : pendingMigrationDocs) {
                if (results.stream().noneMatch(r -> r.getDocument() == migrationDoc)) {
                    results.add(ProcessingResult.failure(migrationDoc, 
                            "Batch processing error: " + e.getMessage())
                            .errorStackTrace(getStackTraceAsString(e))
                            .processingDurationMs(System.currentTimeMillis() - batchStartTime)
                            .build());
                }
            }
        } finally {
            // Clear references to help GC
            pendingDocuments.clear();
            pendingMigrationDocs.clear();
        }
        
        return results;
    }
    
    /**
     * Execute batch save operation.
     */
    private void executeBatchSave(List<Document> pendingDocuments, 
                                   List<MigrationDocument> pendingMigrationDocs,
                                   List<ProcessingResult> results,
                                   long batchStartTime) {
        try {
            UpdatingBatch batch = UpdatingBatch.createUpdatingBatchInstance(
                    domain,
                    RefreshMode.REFRESH
            );
            
            for (Document doc : pendingDocuments) {
                batch.add(doc, null);
            }
            
            logger.info("Executing batch save for {} documents...", pendingDocuments.size());
            batch.updateBatch();
            
            // Process results
            for (int i = 0; i < pendingDocuments.size(); i++) {
                Document doc = pendingDocuments.get(i);
                MigrationDocument migrationDoc = pendingMigrationDocs.get(i);
                
                try {
                    Id docId = doc.get_Id();
                    String documentGuid = docId.toString();
                    
                    results.add(ProcessingResult.success(migrationDoc, documentGuid)
                            .processingDurationMs(System.currentTimeMillis() - batchStartTime)
                            .build());
                    
                    logger.debug("Batch created document: {} with GUID: {}", 
                            migrationDoc.getExternalId(), documentGuid);
                    
                } catch (Exception e) {
                    results.add(ProcessingResult.failure(migrationDoc, e)
                            .processingDurationMs(System.currentTimeMillis() - batchStartTime)
                            .build());
                }
            }
            
            long successCount = results.stream().filter(ProcessingResult::isSuccess).count();
            long failCount = results.stream().filter(r -> !r.isSuccess()).count();
            
            logger.info("Batch save completed: {} successful, {} failed", successCount, failCount);
            
        } catch (Exception e) {
            logger.error("Batch save failed", e);
            
            // Mark all pending documents as failed
            for (MigrationDocument migrationDoc : pendingMigrationDocs) {
                results.add(ProcessingResult.failure(migrationDoc, 
                        "Batch save failed: " + e.getMessage())
                        .errorStackTrace(getStackTraceAsString(e))
                        .processingDurationMs(System.currentTimeMillis() - batchStartTime)
                        .build());
            }
        }
    }
    
    /**
     * Set document properties from the migration document using the mapping configuration.
     */
    private void setDocumentProperties(Document document, MigrationDocument migrationDoc) {
        Properties props = document.getProperties();
        Map<String, String> csvData = migrationDoc.getCsvData();
        
        for (PropertyMapping mapping : mappingConfig.getMappings()) {
            String csvHeader = mapping.getCsvHeader();
            String value = csvData.get(csvHeader);
            
            if (value == null || value.trim().isEmpty()) {
                continue;
            }
            
            try {
                Object convertedValue = PropertyConverter.convert(value, mapping);
                if (convertedValue != null) {
                    String propertyName = mapping.getPropertySymbolicName();
                    
                    switch (mapping.getDataType()) {
                        case STRING:
                            props.putValue(propertyName, (String) convertedValue);
                            break;
                        case LONG:
                            props.putValue(propertyName, ((Long) convertedValue).intValue());
                            break;
                        case DOUBLE:
                            props.putValue(propertyName, (Double) convertedValue);
                            break;
                        case BOOLEAN:
                            props.putValue(propertyName, (Boolean) convertedValue);
                            break;
                        case DATE:
                            props.putValue(propertyName, (Date) convertedValue);
                            break;
                    }
                    
                    logger.trace("Set property {} = {}", propertyName, convertedValue);
                }
            } catch (Exception e) {
                logger.warn("Failed to set property {} for document {}: {}", 
                        mapping.getPropertySymbolicName(), migrationDoc.getExternalId(), e.getMessage());
            }
        }
    }
    
    /**
     * Attach content to the document using streaming (memory-efficient).
     * Uses FileInputStream to avoid loading entire file into memory.
     */
    @SuppressWarnings("unchecked")
    private void attachContent(Document document, MigrationDocument migrationDoc) throws Exception {
        String contentPath = migrationDoc.getContentFilePath();
        Path filePath = Paths.get(contentPath);
        
        // Create content element with FileInputStream (not loading into memory)
        ContentTransfer contentTransfer = Factory.ContentTransfer.createInstance();
        
        // Use FileInputStream for memory-efficient streaming
        InputStream contentStream = new FileInputStream(filePath.toFile());
        contentTransfer.setCaptureSource(contentStream);
        contentTransfer.set_ContentType(migrationDoc.getMimeType());
        contentTransfer.set_RetrievalName(migrationDoc.getContentRetrievalName());
        
        // Create content element list and add content
        ContentElementList contentList = Factory.ContentElement.createList();
        contentList.add(contentTransfer);
        
        document.set_ContentElements(contentList);
        
        // Note: FileNet CE will handle closing the stream after content is transferred
        
        if (logger.isTraceEnabled()) {
            long fileSize = FileUtils.getFileSize(filePath);
            logger.trace("Attached content: {} ({}, {} bytes)", 
                    migrationDoc.getContentRetrievalName(), 
                    migrationDoc.getMimeType(),
                    fileSize);
        }
    }
    
    /**
     * Test the FileNet connection.
     */
    public boolean testConnection() {
        try {
            ensureSubjectPushed();
            
            // Try to fetch object store to verify connection
            Factory.ObjectStore.fetchInstance(
                    domain,
                    properties.getFileNetObjectStore(),
                    null
            );
            return true;
        } catch (Exception e) {
            logger.error("FileNet connection test failed", e);
            return false;
        }
    }
    
    /**
     * Get connection information.
     */
    public String getConnectionInfo() {
        return String.format("FileNet CE: %s, ObjectStore: %s, Domain: %s",
                properties.getFileNetUri(),
                objectStore.get_Name(),
                domain.get_Name());
    }
    
    /**
     * Check if service is healthy.
     */
    public boolean isHealthy() {
        return connection != null && testConnection();
    }
    
    private String getStackTraceAsString(Throwable throwable) {
        StringBuilder sb = new StringBuilder();
        sb.append(throwable.toString()).append("\n");
        
        StackTraceElement[] stackTrace = throwable.getStackTrace();
        int maxFrames = Math.min(stackTrace.length, 20); // Limit stack trace depth
        
        for (int i = 0; i < maxFrames; i++) {
            sb.append("\tat ").append(stackTrace[i].toString()).append("\n");
        }
        
        if (stackTrace.length > maxFrames) {
            sb.append("\t... ").append(stackTrace.length - maxFrames).append(" more\n");
        }
        
        if (throwable.getCause() != null && throwable.getCause() != throwable) {
            sb.append("Caused by: ").append(throwable.getCause().toString()).append("\n");
        }
        
        return sb.toString();
    }
    
    @Override
    public void close() {
        logger.info("Closing FileNet connection...");
        
        try {
            // Pop subject if pushed
            if (Boolean.TRUE.equals(subjectPushed.get())) {
                try {
                    UserContext.get().popSubject();
                } catch (Exception e) {
                    logger.warn("Error popping subject: {}", e.getMessage());
                }
                subjectPushed.set(Boolean.FALSE);
            }
            
            // Remove ThreadLocal to prevent memory leak
            subjectPushed.remove();
            
            logger.info("FileNet connection closed");
        } catch (Exception e) {
            logger.error("Error closing FileNet connection", e);
        }
    }
}


package com.wawa.ace.migration.tracker;

import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.util.DateUtils;
import com.wawa.ace.migration.util.FileUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.IOException;
import java.nio.file.Path;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Tracks document processing and manages output CSV files for success and failure.
 * Thread-safe for use with multi-threaded batch processing.
 */
public class DocumentTracker implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(DocumentTracker.class);
    
    private final ConfigurationManager configManager;
    private final Map<String, SuccessWriter> successWriters;
    private final Map<String, FailureWriter> failureWriters;
    
    // Global counters
    private final AtomicInteger totalTracked = new AtomicInteger(0);
    private final AtomicInteger totalSuccess = new AtomicInteger(0);
    private final AtomicInteger totalFailed = new AtomicInteger(0);
    
    public DocumentTracker(ConfigurationManager configManager) {
        this.configManager = configManager;
        this.successWriters = new ConcurrentHashMap<>();
        this.failureWriters = new ConcurrentHashMap<>();
    }
    
    /**
     * Initialize tracking for a new file.
     */
    public synchronized void initializeForFile(String sourceFileName) throws IOException {
        String timestamp = DateUtils.getCurrentFileTimestamp();
        String baseName = FileUtils.getFileNameWithoutExtension(Path.of(sourceFileName));
        
        // Create success writer
        String successFileName = String.format("%s_completed_%s.csv", baseName, timestamp);
        Path successPath = configManager.getCompletedFilePath(successFileName);
        SuccessWriter successWriter = new SuccessWriter(successPath, configManager);
        successWriters.put(sourceFileName, successWriter);
        
        // Create failure writer
        String failedFileName = String.format("%s_failed_%s.csv", baseName, timestamp);
        Path failedPath = configManager.getFailedFilePath(failedFileName);
        FailureWriter failureWriter = new FailureWriter(failedPath, configManager);
        failureWriters.put(sourceFileName, failureWriter);
        
        logger.info("Initialized tracking for file: {}", sourceFileName);
        logger.info("  Success output: {}", successPath);
        logger.info("  Failed output: {}", failedPath);
    }
    
    /**
     * Track a successfully processed document.
     */
    public void trackSuccess(ProcessingResult result, String sourceFileName) {
        try {
            SuccessWriter writer = successWriters.get(sourceFileName);
            if (writer != null) {
                writer.writeSuccess(result);
                totalSuccess.incrementAndGet();
                totalTracked.incrementAndGet();
            } else {
                logger.warn("No success writer found for file: {}", sourceFileName);
            }
        } catch (IOException e) {
            logger.error("Failed to track success for document: {}", 
                    result.getDocument().getExternalId(), e);
        }
    }
    
    /**
     * Track a failed document.
     */
    public void trackFailed(MigrationDocument document, String errorMessage, 
                           String errorStackTrace, String sourceFileName) {
        try {
            FailureWriter writer = failureWriters.get(sourceFileName);
            if (writer != null) {
                writer.writeFailure(document, errorMessage, errorStackTrace);
                totalFailed.incrementAndGet();
                totalTracked.incrementAndGet();
            } else {
                logger.warn("No failure writer found for file: {}", sourceFileName);
            }
        } catch (IOException e) {
            logger.error("Failed to track failure for document: {}", 
                    document.getExternalId(), e);
        }
    }
    
    /**
     * Track a failed processing result.
     */
    public void trackFailed(ProcessingResult result, String sourceFileName) {
        trackFailed(result.getDocument(), result.getErrorMessage(), 
                result.getErrorStackTrace(), sourceFileName);
    }
    
    /**
     * Finalize tracking for a file (close writers).
     */
    public synchronized void finalizeForFile(String sourceFileName) {
        try {
            SuccessWriter successWriter = successWriters.remove(sourceFileName);
            if (successWriter != null) {
                int count = successWriter.getWrittenCount();
                successWriter.close();
                logger.info("Closed success writer for {}: {} records", sourceFileName, count);
            }
            
            FailureWriter failureWriter = failureWriters.remove(sourceFileName);
            if (failureWriter != null) {
                int count = failureWriter.getWrittenCount();
                failureWriter.close();
                logger.info("Closed failure writer for {}: {} records", sourceFileName, count);
            }
        } catch (Exception e) {
            logger.error("Error finalizing tracking for file: {}", sourceFileName, e);
        }
    }
    
    /**
     * Get success writer for a file.
     */
    public SuccessWriter getSuccessWriter(String sourceFileName) {
        return successWriters.get(sourceFileName);
    }
    
    /**
     * Get failure writer for a file.
     */
    public FailureWriter getFailureWriter(String sourceFileName) {
        return failureWriters.get(sourceFileName);
    }
    
    /**
     * Get total tracked count.
     */
    public int getTotalTracked() {
        return totalTracked.get();
    }
    
    /**
     * Get total success count.
     */
    public int getTotalSuccess() {
        return totalSuccess.get();
    }
    
    /**
     * Get total failed count.
     */
    public int getTotalFailed() {
        return totalFailed.get();
    }
    
    @Override
    public void close() throws Exception {
        logger.info("Closing DocumentTracker...");
        
        for (Map.Entry<String, SuccessWriter> entry : successWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                logger.error("Error closing success writer for: {}", entry.getKey(), e);
            }
        }
        successWriters.clear();
        
        for (Map.Entry<String, FailureWriter> entry : failureWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                logger.error("Error closing failure writer for: {}", entry.getKey(), e);
            }
        }
        failureWriters.clear();
        
        logger.info("DocumentTracker closed. Total tracked: {}, Success: {}, Failed: {}",
                totalTracked.get(), totalSuccess.get(), totalFailed.get());
    }
}

package com.wawa.ace.migration.tracker;

import com.opencsv.CSVWriter;
import com.opencsv.ICSVWriter;
import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.util.DateUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.ReentrantLock;

/**
 * Writer for failed document records.
 * Writes to a CSV file with all original columns plus error information.
 * Thread-safe for concurrent writing.
 */
public class FailureWriter implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(FailureWriter.class);
    
    private final Path outputPath;
    private final ConfigurationManager configManager;
    private final CSVWriter csvWriter;
    private final ReentrantLock writeLock;
    private final AtomicInteger writtenCount;
    
    private String[] headers;
    private boolean headersWritten;
    
    // Additional columns for failure records
    private static final String[] ADDITIONAL_COLUMNS = {
            "ErrorMessage", "ErrorStackTrace", "RetryCount", "FailedTimestamp"
    };
    
    public FailureWriter(Path outputPath, ConfigurationManager configManager) throws IOException {
        this.outputPath = outputPath;
        this.configManager = configManager;
        this.writeLock = new ReentrantLock();
        this.writtenCount = new AtomicInteger(0);
        this.headersWritten = false;
        
        // Ensure parent directory exists
        Files.createDirectories(outputPath.getParent());
        
        // Create CSV writer with pipe delimiter
        char delimiter = configManager.getProperties().getCsvDelimiter();
        Writer writer = new BufferedWriter(new OutputStreamWriter(
                new FileOutputStream(outputPath.toFile()), StandardCharsets.UTF_8));
        
        this.csvWriter = new CSVWriter(writer, delimiter, 
                '"', // Use quotes for error messages which may contain special chars
                ICSVWriter.DEFAULT_ESCAPE_CHARACTER, 
                ICSVWriter.DEFAULT_LINE_END);
        
        logger.debug("Created FailureWriter: {}", outputPath);
    }
    
    /**
     * Write a failed document record.
     */
    public void writeFailure(MigrationDocument doc, String errorMessage, String errorStackTrace) 
            throws IOException {
        writeLock.lock();
        try {
            Map<String, String> csvData = doc.getCsvData();
            
            // Write headers if not yet written
            if (!headersWritten) {
                writeHeaders(csvData);
            }
            
            // Build row with original data plus error columns
            List<String> row = new ArrayList<>();
            
            // Add original CSV data in order
            for (String header : headers) {
                if (isOriginalColumn(header)) {
                    String value = csvData.getOrDefault(header, "");
                    row.add(value);
                }
            }
            
            // Add error columns
            row.add(sanitizeForCsv(errorMessage));
            row.add(sanitizeForCsv(errorStackTrace));
            row.add(String.valueOf(doc.getRetryCount()));
            row.add(DateUtils.getCurrentDisplayTimestamp());
            
            csvWriter.writeNext(row.toArray(new String[0]));
            csvWriter.flush();
            
            writtenCount.incrementAndGet();
            
        } finally {
            writeLock.unlock();
        }
    }
    
    private void writeHeaders(Map<String, String> sampleData) {
        List<String> headerList = new ArrayList<>();
        
        // Add original headers
        headerList.addAll(sampleData.keySet());
        
        // Add additional columns
        for (String col : ADDITIONAL_COLUMNS) {
            headerList.add(col);
        }
        
        this.headers = headerList.toArray(new String[0]);
        csvWriter.writeNext(headers);
        headersWritten = true;
        
        logger.debug("Wrote headers: {} columns", headers.length);
    }
    
    private boolean isOriginalColumn(String column) {
        for (String additional : ADDITIONAL_COLUMNS) {
            if (additional.equals(column)) {
                return false;
            }
        }
        return true;
    }
    
    /**
     * Sanitize a string for CSV output.
     * Removes or escapes problematic characters.
     */
    private String sanitizeForCsv(String value) {
        if (value == null) {
            return "";
        }
        
        // Replace newlines with spaces
        value = value.replace("\r\n", " ").replace("\n", " ").replace("\r", " ");
        
        // Truncate very long error messages
        if (value.length() > 2000) {
            value = value.substring(0, 2000) + "... [truncated]";
        }
        
        return value;
    }
    
    /**
     * Get the number of records written.
     */
    public int getWrittenCount() {
        return writtenCount.get();
    }
    
    /**
     * Get the output file path.
     */
    public Path getOutputPath() {
        return outputPath;
    }
    
    @Override
    public void close() throws IOException {
        writeLock.lock();
        try {
            if (csvWriter != null) {
                csvWriter.close();
            }
            logger.info("Closed FailureWriter: {}, {} records written", outputPath, writtenCount.get());
        } finally {
            writeLock.unlock();
        }
    }
}

package com.wawa.ace.migration.tracker;

import com.opencsv.CSVWriter;
import com.opencsv.ICSVWriter;
import com.wawa.ace.migration.config.ConfigurationManager;
import com.wawa.ace.migration.model.MigrationDocument;
import com.wawa.ace.migration.model.ProcessingResult;
import com.wawa.ace.migration.util.DateUtils;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.ReentrantLock;

/**
 * Writer for successful document records.
 * Writes to a CSV file with all original columns plus additional metadata.
 * Thread-safe for concurrent writing.
 */
public class SuccessWriter implements AutoCloseable {
    
    private static final Logger logger = LogManager.getLogger(SuccessWriter.class);
    
    private final Path outputPath;
    private final ConfigurationManager configManager;
    private final CSVWriter csvWriter;
    private final ReentrantLock writeLock;
    private final AtomicInteger writtenCount;
    
    private String[] headers;
    private boolean headersWritten;
    
    // Additional columns for success records
    private static final String[] ADDITIONAL_COLUMNS = {
            "document_GUID", "DateCreated", "Status"
    };
    
    public SuccessWriter(Path outputPath, ConfigurationManager configManager) throws IOException {
        this.outputPath = outputPath;
        this.configManager = configManager;
        this.writeLock = new ReentrantLock();
        this.writtenCount = new AtomicInteger(0);
        this.headersWritten = false;
        
        // Ensure parent directory exists
        Files.createDirectories(outputPath.getParent());
        
        // Create CSV writer with pipe delimiter
        char delimiter = configManager.getProperties().getCsvDelimiter();
        Writer writer = new BufferedWriter(new OutputStreamWriter(
                new FileOutputStream(outputPath.toFile()), StandardCharsets.UTF_8));
        
        this.csvWriter = new CSVWriter(writer, delimiter, 
                ICSVWriter.NO_QUOTE_CHARACTER, 
                ICSVWriter.DEFAULT_ESCAPE_CHARACTER, 
                ICSVWriter.DEFAULT_LINE_END);
        
        logger.debug("Created SuccessWriter: {}", outputPath);
    }
    
    /**
     * Write a successful processing result.
     */
    public void writeSuccess(ProcessingResult result) throws IOException {
        writeLock.lock();
        try {
            MigrationDocument doc = result.getDocument();
            Map<String, String> csvData = doc.getCsvData();
            
            // Write headers if not yet written
            if (!headersWritten) {
                writeHeaders(csvData);
            }
            
            // Build row with original data plus additional columns
            List<String> row = new ArrayList<>();
            
            // Add original CSV data in order
            for (String header : headers) {
                if (isOriginalColumn(header)) {
                    String value = csvData.getOrDefault(header, "");
                    row.add(value);
                }
            }
            
            // Add additional columns
            row.add(result.getDocumentGuid() != null ? result.getDocumentGuid() : "");
            row.add(DateUtils.getCurrentDisplayTimestamp());
            row.add("SUCCESS");
            
            csvWriter.writeNext(row.toArray(new String[0]));
            csvWriter.flush();
            
            writtenCount.incrementAndGet();
            
        } finally {
            writeLock.unlock();
        }
    }
    
    private void writeHeaders(Map<String, String> sampleData) {
        List<String> headerList = new ArrayList<>();
        
        // Add original headers
        headerList.addAll(sampleData.keySet());
        
        // Add additional columns
        for (String col : ADDITIONAL_COLUMNS) {
            headerList.add(col);
        }
        
        this.headers = headerList.toArray(new String[0]);
        csvWriter.writeNext(headers);
        headersWritten = true;
        
        logger.debug("Wrote headers: {} columns", headers.length);
    }
    
    private boolean isOriginalColumn(String column) {
        for (String additional : ADDITIONAL_COLUMNS) {
            if (additional.equals(column)) {
                return false;
            }
        }
        return true;
    }
    
    /**
     * Get the number of records written.
     */
    public int getWrittenCount() {
        return writtenCount.get();
    }
    
    /**
     * Get the output file path.
     */
    public Path getOutputPath() {
        return outputPath;
    }
    
    @Override
    public void close() throws IOException {
        writeLock.lock();
        try {
            if (csvWriter != null) {
                csvWriter.close();
            }
            logger.info("Closed SuccessWriter: {}, {} records written", outputPath, writtenCount.get());
        } finally {
            writeLock.unlock();
        }
    }
}

package com.wawa.ace.migration.util;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.time.*;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeFormatterBuilder;
import java.time.format.DateTimeParseException;
import java.time.temporal.ChronoField;
import java.util.Date;

/**
 * Utility class for date parsing and formatting.
 */
public class DateUtils {
    
    private static final Logger logger = LogManager.getLogger(DateUtils.class);
    
    // Format for timestamps in file names
    public static final String FILE_TIMESTAMP_FORMAT = "yyyyMMdd_HHmmss";
    public static final DateTimeFormatter FILE_TIMESTAMP_FORMATTER = 
            DateTimeFormatter.ofPattern(FILE_TIMESTAMP_FORMAT);
    
    // Format for display timestamps
    public static final String DISPLAY_TIMESTAMP_FORMAT = "yyyy-MM-dd HH:mm:ss";
    public static final DateTimeFormatter DISPLAY_TIMESTAMP_FORMATTER = 
            DateTimeFormatter.ofPattern(DISPLAY_TIMESTAMP_FORMAT);
    
    // ISO format with offset
    public static final String ISO_OFFSET_FORMAT = "yyyy-MM-dd HH:mm:ss.SSSSSSXXX";
    
    // Flexible formatter for parsing the CSV date format
    // Handles: 2025-06-07 17:23:58.542084+00:00
    private static final DateTimeFormatter CSV_DATE_FORMATTER = new DateTimeFormatterBuilder()
            .appendPattern("yyyy-MM-dd HH:mm:ss")
            .optionalStart()
            .appendFraction(ChronoField.MICRO_OF_SECOND, 0, 6, true)
            .optionalEnd()
            .optionalStart()
            .appendOffset("+HH:MM", "+00:00")
            .optionalEnd()
            .optionalStart()
            .appendOffset("+HHmm", "+0000")
            .optionalEnd()
            .optionalStart()
            .appendLiteral('Z')
            .optionalEnd()
            .toFormatter();
    
    private DateUtils() {
        // Utility class
    }
    
    /**
     * Get current timestamp formatted for file names.
     */
    public static String getCurrentFileTimestamp() {
        return LocalDateTime.now().format(FILE_TIMESTAMP_FORMATTER);
    }
    
    /**
     * Get current timestamp formatted for display.
     */
    public static String getCurrentDisplayTimestamp() {
        return LocalDateTime.now().format(DISPLAY_TIMESTAMP_FORMATTER);
    }
    
    /**
     * Format a LocalDateTime for display.
     */
    public static String formatForDisplay(LocalDateTime dateTime) {
        if (dateTime == null) return "";
        return dateTime.format(DISPLAY_TIMESTAMP_FORMATTER);
    }
    
    /**
     * Format a LocalDateTime for file names.
     */
    public static String formatForFileName(LocalDateTime dateTime) {
        if (dateTime == null) return "";
        return dateTime.format(FILE_TIMESTAMP_FORMATTER);
    }
    
    /**
     * Parse a date string from CSV format to Date.
     * Handles format: 2025-06-07 17:23:58.542084+00:00
     */
    public static Date parseCsvDate(String dateString) {
        if (dateString == null || dateString.trim().isEmpty()) {
            return null;
        }
        
        try {
            // Try parsing with offset
            OffsetDateTime odt = OffsetDateTime.parse(dateString.trim(), CSV_DATE_FORMATTER);
            return Date.from(odt.toInstant());
        } catch (DateTimeParseException e) {
            logger.debug("Failed to parse with offset, trying without: {}", dateString);
        }
        
        try {
            // Try parsing without offset (assume UTC)
            LocalDateTime ldt = LocalDateTime.parse(dateString.trim(), CSV_DATE_FORMATTER);
            return Date.from(ldt.atZone(ZoneOffset.UTC).toInstant());
        } catch (DateTimeParseException e) {
            logger.warn("Failed to parse date: {}", dateString, e);
            return null;
        }
    }
    
    /**
     * Parse a date string with a custom format.
     */
    public static Date parseDate(String dateString, String format) {
        if (dateString == null || dateString.trim().isEmpty()) {
            return null;
        }
        
        try {
            DateTimeFormatter formatter = DateTimeFormatter.ofPattern(format);
            
            // Try parsing as OffsetDateTime first
            try {
                OffsetDateTime odt = OffsetDateTime.parse(dateString.trim(), formatter);
                return Date.from(odt.toInstant());
            } catch (DateTimeParseException e) {
                // Try as LocalDateTime
                LocalDateTime ldt = LocalDateTime.parse(dateString.trim(), formatter);
                return Date.from(ldt.atZone(ZoneOffset.UTC).toInstant());
            }
        } catch (DateTimeParseException e) {
            logger.warn("Failed to parse date '{}' with format '{}'", dateString, format, e);
            return null;
        }
    }
    
    /**
     * Convert Date to LocalDateTime.
     */
    public static LocalDateTime toLocalDateTime(Date date) {
        if (date == null) return null;
        return LocalDateTime.ofInstant(date.toInstant(), ZoneId.systemDefault());
    }
    
    /**
     * Convert LocalDateTime to Date.
     */
    public static Date toDate(LocalDateTime localDateTime) {
        if (localDateTime == null) return null;
        return Date.from(localDateTime.atZone(ZoneId.systemDefault()).toInstant());
    }
    
    /**
     * Get current date as java.util.Date.
     */
    public static Date getCurrentDate() {
        return new Date();
    }
    
    /**
     * Format duration in milliseconds to human-readable string.
     */
    public static String formatDuration(long durationMs) {
        if (durationMs < 1000) {
            return durationMs + "ms";
        }
        
        long seconds = durationMs / 1000;
        long minutes = seconds / 60;
        long hours = minutes / 60;
        
        if (hours > 0) {
            return String.format("%dh %dm %ds", hours, minutes % 60, seconds % 60);
        } else if (minutes > 0) {
            return String.format("%dm %ds", minutes, seconds % 60);
        } else {
            return String.format("%ds %dms", seconds, durationMs % 1000);
        }
    }
    
    /**
     * Calculate elapsed time in milliseconds.
     */
    public static long getElapsedMs(LocalDateTime start, LocalDateTime end) {
        return Duration.between(start, end).toMillis();
    }
    
    /**
     * Calculate elapsed time from start to now in milliseconds.
     */
    public static long getElapsedMs(LocalDateTime start) {
        return Duration.between(start, LocalDateTime.now()).toMillis();
    }
}

package com.wawa.ace.migration.util;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.channels.FileChannel;
import java.nio.channels.FileLock;
import java.nio.file.*;
import java.util.ArrayList;
import java.util.List;

/**
 * Utility class for file operations including file locking.
 */
public class FileUtils {
    
    private static final Logger logger = LogManager.getLogger(FileUtils.class);
    
    private FileUtils() {
        // Utility class
    }
    
    /**
     * Get all CSV files from a directory.
     */
    public static List<Path> getCsvFiles(Path directory) throws IOException {
        List<Path> csvFiles = new ArrayList<>();
        
        if (!Files.exists(directory)) {
            logger.warn("Directory does not exist: {}", directory);
            return csvFiles;
        }
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory, "*.csv")) {
            for (Path path : stream) {
                if (Files.isRegularFile(path)) {
                    csvFiles.add(path);
                }
            }
        }
        
        logger.info("Found {} CSV files in {}", csvFiles.size(), directory);
        return csvFiles;
    }
    
    /**
     * Move a file from source to destination with locking.
     * Returns true if successful, false if file is locked.
     */
    public static boolean moveFile(Path source, Path destination) throws IOException {
        if (!Files.exists(source)) {
            throw new IOException("Source file does not exist: " + source);
        }
        
        // Ensure destination directory exists
        Files.createDirectories(destination.getParent());
        
        // Try to move the file
        try {
            Files.move(source, destination, StandardCopyOption.REPLACE_EXISTING);
            logger.debug("Moved file: {} -> {}", source, destination);
            return true;
        } catch (IOException e) {
            logger.error("Failed to move file: {} -> {}", source, destination, e);
            throw e;
        }
    }
    
    /**
     * Copy a file from source to destination.
     */
    public static void copyFile(Path source, Path destination) throws IOException {
        if (!Files.exists(source)) {
            throw new IOException("Source file does not exist: " + source);
        }
        
        // Ensure destination directory exists
        Files.createDirectories(destination.getParent());
        
        Files.copy(source, destination, StandardCopyOption.REPLACE_EXISTING);
        logger.debug("Copied file: {} -> {}", source, destination);
    }
    
    /**
     * Try to acquire an exclusive lock on a file.
     * Returns the FileLock if successful, null if file is already locked.
     */
    public static FileLock tryLock(Path filePath) {
        try {
            File file = filePath.toFile();
            @SuppressWarnings("resource")
            RandomAccessFile raf = new RandomAccessFile(file, "rw");
            FileChannel channel = raf.getChannel();
            FileLock lock = channel.tryLock();
            
            if (lock == null) {
                raf.close();
                logger.debug("Could not acquire lock on file: {}", filePath);
                return null;
            }
            
            logger.debug("Acquired lock on file: {}", filePath);
            return lock;
        } catch (IOException e) {
            logger.error("Error acquiring lock on file: {}", filePath, e);
            return null;
        }
    }
    
    /**
     * Release a file lock.
     */
    public static void releaseLock(FileLock lock) {
        if (lock != null) {
            try {
                FileChannel channel = lock.channel();
                lock.release();
                channel.close();
                logger.debug("Released file lock");
            } catch (IOException e) {
                logger.error("Error releasing lock", e);
            }
        }
    }
    
    /**
     * Check if a file exists.
     */
    public static boolean exists(Path path) {
        return Files.exists(path);
    }
    
    /**
     * Check if a file exists.
     */
    public static boolean exists(String path) {
        return Files.exists(Paths.get(path));
    }
    
    /**
     * Get the file name without extension.
     */
    public static String getFileNameWithoutExtension(Path path) {
        String fileName = path.getFileName().toString();
        int lastDot = fileName.lastIndexOf('.');
        if (lastDot > 0) {
            return fileName.substring(0, lastDot);
        }
        return fileName;
    }
    
    /**
     * Get the file extension.
     */
    public static String getFileExtension(Path path) {
        String fileName = path.getFileName().toString();
        int lastDot = fileName.lastIndexOf('.');
        if (lastDot > 0 && lastDot < fileName.length() - 1) {
            return fileName.substring(lastDot + 1);
        }
        return "";
    }
    
    /**
     * Create a timestamped file name.
     */
    public static String createTimestampedFileName(String baseName, String extension, String timestamp) {
        return String.format("%s_%s.%s", baseName, timestamp, extension);
    }
    
    /**
     * Ensure a directory exists, creating it if necessary.
     */
    public static void ensureDirectoryExists(Path directory) throws IOException {
        if (!Files.exists(directory)) {
            Files.createDirectories(directory);
            logger.info("Created directory: {}", directory);
        }
    }
    
    /**
     * Get file size in bytes.
     */
    public static long getFileSize(Path path) throws IOException {
        return Files.size(path);
    }
    
    /**
     * Get file size as human-readable string.
     */
    public static String getFileSizeReadable(Path path) throws IOException {
        long bytes = Files.size(path);
        return formatFileSize(bytes);
    }
    
    /**
     * Format file size as human-readable string.
     */
    public static String formatFileSize(long bytes) {
        if (bytes < 1024) return bytes + " B";
        int exp = (int) (Math.log(bytes) / Math.log(1024));
        String pre = "KMGTPE".charAt(exp - 1) + "";
        return String.format("%.1f %sB", bytes / Math.pow(1024, exp), pre);
    }
    
    /**
     * Read file content as byte array.
     */
    public static byte[] readFileAsBytes(Path path) throws IOException {
        return Files.readAllBytes(path);
    }
    
    /**
     * Read file content as byte array.
     */
    public static byte[] readFileAsBytes(String path) throws IOException {
        return Files.readAllBytes(Paths.get(path));
    }
}

package com.wawa.ace.migration.util;

import com.wawa.ace.migration.config.PropertyMapping;
import com.wawa.ace.migration.config.PropertyMapping.DataType;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.util.Date;

/**
 * Utility class for converting CSV string values to appropriate Java types
 * based on the property mapping configuration.
 */
public class PropertyConverter {
    
    private static final Logger logger = LogManager.getLogger(PropertyConverter.class);
    
    private PropertyConverter() {
        // Utility class
    }
    
    /**
     * Convert a CSV string value to the appropriate Java type based on the mapping.
     * 
     * @param value The string value from CSV
     * @param mapping The property mapping configuration
     * @return The converted value, or null if conversion fails or value is empty
     */
    public static Object convert(String value, PropertyMapping mapping) {
        if (value == null || value.trim().isEmpty()) {
            return null;
        }
        
        String trimmedValue = value.trim();
        DataType dataType = mapping.getDataType();
        
        try {
            switch (dataType) {
                case STRING:
                    return convertToString(trimmedValue, mapping.getMaxLength());
                    
                case LONG:
                    return convertToLong(trimmedValue);
                    
                case DOUBLE:
                    return convertToDouble(trimmedValue);
                    
                case BOOLEAN:
                    return convertToBoolean(trimmedValue);
                    
                case DATE:
                    return convertToDate(trimmedValue, mapping.getDateFormat());
                    
                default:
                    logger.warn("Unknown data type: {}, treating as STRING", dataType);
                    return trimmedValue;
            }
        } catch (Exception e) {
            logger.warn("Failed to convert value '{}' to type {} for property {}: {}", 
                    value, dataType, mapping.getPropertySymbolicName(), e.getMessage());
            return null;
        }
    }
    
    /**
     * Convert value to String, respecting max length.
     */
    public static String convertToString(String value, Integer maxLength) {
        if (value == null) return null;
        
        if (maxLength != null && maxLength > 0 && value.length() > maxLength) {
            logger.debug("Truncating value from {} to {} characters", value.length(), maxLength);
            return value.substring(0, maxLength);
        }
        return value;
    }
    
    /**
     * Convert value to Long.
     */
    public static Long convertToLong(String value) {
        if (value == null || value.trim().isEmpty()) return null;
        
        try {
            // Handle potential decimal values by parsing as double first
            double d = Double.parseDouble(value.trim());
            return (long) d;
        } catch (NumberFormatException e) {
            logger.warn("Failed to parse Long value: {}", value);
            return null;
        }
    }
    
    /**
     * Convert value to Double.
     */
    public static Double convertToDouble(String value) {
        if (value == null || value.trim().isEmpty()) return null;
        
        try {
            return Double.parseDouble(value.trim());
        } catch (NumberFormatException e) {
            logger.warn("Failed to parse Double value: {}", value);
            return null;
        }
    }
    
    /**
     * Convert value to Boolean.
     * Accepts: true/false, yes/no, 1/0, y/n
     */
    public static Boolean convertToBoolean(String value) {
        if (value == null || value.trim().isEmpty()) return null;
        
        String lower = value.trim().toLowerCase();
        
        if ("true".equals(lower) || "yes".equals(lower) || "1".equals(lower) || "y".equals(lower)) {
            return Boolean.TRUE;
        }
        if ("false".equals(lower) || "no".equals(lower) || "0".equals(lower) || "n".equals(lower)) {
            return Boolean.FALSE;
        }
        
        logger.warn("Unrecognized boolean value: {}, returning null", value);
        return null;
    }
    
    /**
     * Convert value to Date using the specified format or default CSV format.
     */
    public static Date convertToDate(String value, String dateFormat) {
        if (value == null || value.trim().isEmpty()) return null;
        
        if (dateFormat != null && !dateFormat.isEmpty()) {
            return DateUtils.parseDate(value.trim(), dateFormat);
        }
        
        // Use default CSV date parser
        return DateUtils.parseCsvDate(value.trim());
    }
    
    /**
     * Parse a Long value from string (for claimId DB field).
     */
    public static Long parseLongForDb(String value) {
        if (value == null || value.trim().isEmpty()) return null;
        
        try {
            return Long.parseLong(value.trim());
        } catch (NumberFormatException e) {
            // Try parsing as double first (in case of decimal)
            try {
                return (long) Double.parseDouble(value.trim());
            } catch (NumberFormatException e2) {
                logger.warn("Failed to parse Long for DB: {}", value);
                return null;
            }
        }
    }
    
    /**
     * Safely truncate a string to max length for DB fields.
     */
    public static String truncateForDb(String value, int maxLength) {
        if (value == null) return null;
        if (value.length() <= maxLength) return value;
        return value.substring(0, maxLength);
    }
}

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.wawa.ace.migration</groupId>
  <artifactId>WawaDocumentMigrationUtility</artifactId>
  <version>1.0.0</version>
  <packaging>jar</packaging>

  <name>Wawa Document Migration Utility</name>
  <description>High-performance utility for migrating documents to IBM FileNet P8</description>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <maven.compiler.release>17</maven.compiler.release>
  </properties>

  <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>org.junit</groupId>
        <artifactId>junit-bom</artifactId>
        <version>5.11.0</version>
        <type>pom</type>
        <scope>import</scope>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <dependencies>
    <!-- Test Dependencies -->
    <dependency>
      <groupId>org.junit.jupiter</groupId>
      <artifactId>junit-jupiter-api</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.junit.jupiter</groupId>
      <artifactId>junit-jupiter-params</artifactId>
      <scope>test</scope>
    </dependency>

    <!-- Apache Commons -->
    <dependency>
      <groupId>commons-io</groupId>
      <artifactId>commons-io</artifactId>
      <version>2.18.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-lang3</artifactId>
      <version>3.18.0</version>
    </dependency>

    <!-- Database Connection Pooling -->
    <dependency>
      <groupId>com.zaxxer</groupId>
      <artifactId>HikariCP</artifactId>
      <version>5.1.0</version>
    </dependency>
    <dependency>
      <groupId>com.microsoft.sqlserver</groupId>
      <artifactId>mssql-jdbc</artifactId>
      <version>12.10.1.jre11</version>
    </dependency>

    <!-- Jackson JSON Processing -->
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-annotations</artifactId>
      <version>2.15.2</version>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-core</artifactId>
      <version>2.15.2</version>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-databind</artifactId>
      <version>2.15.2</version>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.datatype</groupId>
      <artifactId>jackson-datatype-jsr310</artifactId>
      <version>2.15.2</version>
    </dependency>

    <!-- Logging -->
    <dependency>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-api</artifactId>
      <version>2.20.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-core</artifactId>
      <version>2.20.0</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>2.0.9</version>
    </dependency>
    <dependency>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-slf4j2-impl</artifactId>
      <version>2.20.0</version>
    </dependency>

    <!-- CSV Processing -->
    <dependency>
      <groupId>com.opencsv</groupId>
      <artifactId>opencsv</artifactId>
      <version>5.7.1</version>
    </dependency>
  </dependencies>

  <build>
    <pluginManagement>
      <plugins>
        <plugin>
          <artifactId>maven-clean-plugin</artifactId>
          <version>3.4.0</version>
        </plugin>
        <plugin>
          <artifactId>maven-resources-plugin</artifactId>
          <version>3.3.1</version>
        </plugin>
        <plugin>
          <artifactId>maven-compiler-plugin</artifactId>
          <version>3.13.0</version>
        </plugin>
        <plugin>
          <artifactId>maven-surefire-plugin</artifactId>
          <version>3.3.0</version>
        </plugin>
        <plugin>
          <artifactId>maven-jar-plugin</artifactId>
          <version>3.4.2</version>
        </plugin>
        <plugin>
          <artifactId>maven-install-plugin</artifactId>
          <version>3.1.2</version>
        </plugin>
        <plugin>
          <artifactId>maven-deploy-plugin</artifactId>
          <version>3.1.2</version>
        </plugin>
        <plugin>
          <artifactId>maven-site-plugin</artifactId>
          <version>3.12.1</version>
        </plugin>
        <plugin>
          <artifactId>maven-project-info-reports-plugin</artifactId>
          <version>3.6.1</version>
        </plugin>
      </plugins>
    </pluginManagement>
    <plugins>
      <!-- Maven Shade Plugin to create executable JAR with all dependencies -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>3.5.1</version>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <shadedArtifactAttached>false</shadedArtifactAttached>
              <finalName>WawaDocumentMigrationUtility</finalName>
              <transformers>
                <!-- Set main class in manifest -->
                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                  <mainClass>com.wawa.ace.migration.MigrationApp</mainClass>
                </transformer>
                <!-- Merge service files -->
                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                <!-- Merge Log4j2 plugin cache files -->
                <transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
                  <resource>META-INF/org/apache/logging/log4j/core/config/plugins/Log4j2Plugins.dat</resource>
                </transformer>
              </transformers>
              <filters>
                <filter>
                  <!-- Exclude signature files to avoid security exceptions -->
                  <artifact>*:*</artifact>
                  <excludes>
                    <exclude>META-INF/*.SF</exclude>
                    <exclude>META-INF/*.DSA</exclude>
                    <exclude>META-INF/*.RSA</exclude>
                  </excludes>
                </filter>
              </filters>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
</project>


{
  "documentClass": "ClaimDocument",
  "mappings": [
    {
      "csvHeader": "customerID",
      "propertySymbolicName": "CustomerID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "author",
      "propertySymbolicName": "Author",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "batchID",
      "propertySymbolicName": "BatchID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "amount",
      "propertySymbolicName": "Amount",
      "dataType": "DOUBLE",
      "required": false
    },
    {
      "csvHeader": "documentDescription",
      "propertySymbolicName": "DocumentDescription",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "reviewed",
      "propertySymbolicName": "Reviewed",
      "dataType": "BOOLEAN",
      "required": false
    },
    {
      "csvHeader": "documentSubtype",
      "propertySymbolicName": "DocumentSubtype",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "gwDocumentID",
      "propertySymbolicName": "GWDocumentID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "jobID",
      "propertySymbolicName": "JobID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "documentType",
      "propertySymbolicName": "DocumentType",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "inputMethod",
      "propertySymbolicName": "InputMethod",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "hidden",
      "propertySymbolicName": "Hidden",
      "dataType": "BOOLEAN",
      "required": false
    },
    {
      "csvHeader": "duplicate",
      "propertySymbolicName": "Duplicate",
      "dataType": "BOOLEAN",
      "required": false
    },
    {
      "csvHeader": "sensitive",
      "propertySymbolicName": "Sensitive",
      "dataType": "BOOLEAN",
      "required": false
    },
    {
      "csvHeader": "setID",
      "propertySymbolicName": "SetID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "claimNumber",
      "propertySymbolicName": "ClaimNumber",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "exposureID",
      "propertySymbolicName": "ExposureID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "claimant",
      "propertySymbolicName": "Claimant",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "coverage",
      "propertySymbolicName": "Coverage",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "insuredName",
      "propertySymbolicName": "InsuredName",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "primaryMembershipNumber",
      "propertySymbolicName": "PrimaryMembershipNumber",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "policyNumber",
      "propertySymbolicName": "PolicyNumber",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "claimID",
      "propertySymbolicName": "ClaimID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    },
    {
      "csvHeader": "batchDocCount",
      "propertySymbolicName": "BatchDocCount",
      "dataType": "LONG",
      "required": false
    },
    {
      "csvHeader": "setDocCount",
      "propertySymbolicName": "SetDocCount",
      "dataType": "LONG",
      "required": false
    },
    {
      "csvHeader": "setDocIndex",
      "propertySymbolicName": "SetDocIndex",
      "dataType": "LONG",
      "required": false
    },
    {
      "csvHeader": "doNotCreateActivity",
      "propertySymbolicName": "DoNotCreateActivity",
      "dataType": "BOOLEAN",
      "required": false
    },
    {
      "csvHeader": "OrigDateCreated",
      "propertySymbolicName": "OriginalDateCreated",
      "dataType": "DATE",
      "dateFormat": "yyyy-MM-dd HH:mm:ss.SSSSSSXXX",
      "required": false
    },
    {
      "csvHeader": "externalID",
      "propertySymbolicName": "ExternalID",
      "dataType": "STRING",
      "maxLength": 64,
      "required": true
    },
    {
      "csvHeader": "documentTitle",
      "propertySymbolicName": "DocumentTitle",
      "dataType": "STRING",
      "maxLength": 64,
      "required": false
    }
  ],
  "contentSettings": {
    "contentFilePathColumn": "contentFilePath",
    "contentRetrievalNameColumn": "contentRetrievalName",
    "mimeTypeColumn": "mimeType"
  },
  "trackingSettings": {
    "externalIdColumn": "externalID",
    "claimNumberColumn": "claimNumber",
    "claimIdColumn": "claimID",
    "batchIdColumn": "batchID"
  }
}

<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">

    <Properties>
        <Property name="LOG_PATH">D:/Rameshwar/WawaDocumentMigrationUtility/logs</Property>
        <Property name="LOG_PATTERN">%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n</Property>
        <Property name="LOG_PATTERN_SIMPLE">%d{yyyy-MM-dd HH:mm:ss} - %msg%n</Property>
    </Properties>

    <Appenders>
        <!-- Main Application Log with Rolling -->
        <RollingFile name="FileLogger" 
                     fileName="${LOG_PATH}/WawaDocumentMigration.log"
                     filePattern="${LOG_PATH}/WawaDocumentMigration-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="100MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="30"/>
        </RollingFile>

        <!-- Batch Processing Log (detailed batch operations) -->
        <RollingFile name="BatchProcessingLogger" 
                     fileName="${LOG_PATH}/BatchProcessing.log"
                     filePattern="${LOG_PATH}/BatchProcessing-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="14"/>
        </RollingFile>

        <!-- FileNet Operations Log -->
        <RollingFile name="FileNetLogger" 
                     fileName="${LOG_PATH}/FileNetOperations.log"
                     filePattern="${LOG_PATH}/FileNetOperations-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="14"/>
        </RollingFile>

        <!-- Database Operations Log -->
        <RollingFile name="DatabaseLogger" 
                     fileName="${LOG_PATH}/DatabaseOperations.log"
                     filePattern="${LOG_PATH}/DatabaseOperations-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="14"/>
        </RollingFile>

        <!-- Progress Monitoring Log -->
        <RollingFile name="ProgressLogger" 
                     fileName="${LOG_PATH}/Progress.log"
                     filePattern="${LOG_PATH}/Progress-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="${LOG_PATTERN_SIMPLE}"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="20MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="7"/>
        </RollingFile>

        <!-- Error-Only Log (with full stack traces) -->
        <RollingFile name="ErrorLogger" 
                     fileName="${LOG_PATH}/Errors.log"
                     filePattern="${LOG_PATH}/Errors-%d{yyyy-MM-dd}-%i.log.gz">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n%throwable"/>
            <Policies>
                <SizeBasedTriggeringPolicy size="50MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
            <DefaultRolloverStrategy max="30"/>
            <ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>

        <!-- Console Appender -->
        <Console name="ConsoleLogger" target="SYSTEM_OUT">
            <PatternLayout pattern="%d{HH:mm:ss} [%t] %-5level %logger{20} - %msg%n"/>
        </Console>
    </Appenders>

    <Loggers>
        <!-- FileNet Service Logger -->
        <Logger name="com.wawa.ace.migration.service.FileNetService" level="info" additivity="false">
            <AppenderRef ref="FileNetLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Database Service Logger -->
        <Logger name="com.wawa.ace.migration.service.DatabaseService" level="info" additivity="false">
            <AppenderRef ref="DatabaseLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Batch Processing Logger -->
        <Logger name="com.wawa.ace.migration.service.BatchProcessorService" level="info" additivity="false">
            <AppenderRef ref="BatchProcessingLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Document Processor Logger -->
        <Logger name="com.wawa.ace.migration.processor" level="info" additivity="false">
            <AppenderRef ref="BatchProcessingLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Report Generator Logger -->
        <Logger name="com.wawa.ace.migration.report" level="info" additivity="false">
            <AppenderRef ref="ProgressLogger"/>
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- Tracker Logger -->
        <Logger name="com.wawa.ace.migration.tracker" level="info" additivity="false">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Config Logger -->
        <Logger name="com.wawa.ace.migration.config" level="info" additivity="false">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
        </Logger>

        <!-- HikariCP Logger (reduce verbosity) -->
        <Logger name="com.zaxxer.hikari" level="warn" additivity="false">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Logger>

        <!-- Root logger -->
        <Root level="info">
            <AppenderRef ref="FileLogger"/>
            <AppenderRef ref="ConsoleLogger"/>
            <AppenderRef ref="ErrorLogger"/>
        </Root>
    </Loggers>

</Configuration>


# ===========================================================
# Wawa Document Migration Utility - Configuration
# ===========================================================
# Last Updated: 2025-12-13
# Optimized for high-volume production processing (1M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base.path=D:/Rameshwar/WawaDocumentMigrationUtility/
app.config.path=D:/Rameshwar/WawaDocumentMigrationUtility/config/
app.skip.header.row=true

# ===== FOLDER PATHS (relative to app.base.path) =====
migration.source.folder=Migration/Source/
migration.inprocess.folder=Migration/Inprocess/
migration.completed.folder=Migration/Completed/
migration.failed.folder=Migration/Failed/
migration.archive.folder=Migration/Archive/
migration.reports.folder=Migration/Reports/

# ===== CSV SETTINGS =====
csv.delimiter=|
csv.encoding=UTF-8

# ===== FILENET CONNECTION =====
filenet.ce.uri=http://192.168.0.12:9080/wsi/FNCEWS40MTOM/
filenet.username=p8admin
filenet.password=Ebla1234
filenet.objectstore=OBJ1
filenet.stanza=FileNetP8WSI
filenet.document.class=ClaimDocument

# ===== BATCH PROCESSING =====
# Number of documents per batch commit to FileNet
batch.size=100
# Timeout for batch commit operations (seconds)
batch.commit.timeout.seconds=300

# ===== THREAD POOL =====
# Number of parallel threads for processing documents
thread.pool.size=5
# Maximum time to wait for thread pool shutdown (seconds)
thread.pool.shutdown.timeout.seconds=3600

# ===== RETRY CONFIGURATION =====
# Enable/disable retry for failed documents
retry.enabled=true
# Maximum number of retry attempts for failed documents
retry.max.attempts=3
# Delay between retry attempts (milliseconds)
retry.delay.milliseconds=1000

# ===== DATABASE SETTINGS (SQL Server) =====
db.server=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.name=Wawa_DMS_Conversion_UAT
db.table=WawaMigratedDocuments
# Connection pool size (should be >= thread.pool.size + buffer)
db.pool.size=15
# Connection timeout (seconds)
db.connection.timeout.seconds=30

# Use Windows Integrated Authentication (true) or SQL Authentication (false)
# If false, you must provide db.username and db.password
db.integrated.security=false

# SQL Server Authentication credentials (only used if db.integrated.security=false)
db.username=sa
db.password=Ebla1234

# ===== MEMORY & PERFORMANCE =====
# Memory warning threshold (percentage of max heap)
memory.warning.threshold.percent=80
# Progress log interval (minutes) - set to 0 to disable
progress.log.interval.minutes=5

# ===== LOGGING =====
log.config.path=D:/Rameshwar/WawaDocumentMigrationUtility/config/Log4j2.xml

# ===========================================================
# NOTES
# ===========================================================
# Performance Estimates for 50K documents per CSV:
# - With batch size 100: ~500 batch commits per file
# - Thread pool of 5: Processes batches in parallel
# - Connection pool of 15: Adequate for 5 threads + overhead
#
# JVM Recommended Settings:
# -Xms2G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# FileNet JARs Required in lib folder:
# - Jace.jar
# - stax-api.jar
# - xlxpScanner.jar
# - xlxpScannerUtils.jar
# - log4j.jar (FileNet's version)
#
# DATABASE AUTHENTICATION:
# Option 1: Windows Integrated Authentication
#   - Set db.integrated.security=true
#   - Requires mssql-jdbc_auth-12.10.1.x64.dll in java.library.path
#
# Option 2: SQL Server Authentication
#   - Set db.integrated.security=false
#   - Set db.username and db.password
# ===========================================================
