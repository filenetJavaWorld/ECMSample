package com.abcdefg.abc.index;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.index.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.index.connection.ConnectionManager;
import com.abcdefg.abc.index.constants.Constants;
import com.abcdefg.abc.index.utils.Utils;

/**
 * CCDataIndexAndPackagingUtility - Main Service Class
 * 
 * Purpose:
 * - Process CSV files from CCDataMergeUtility output
 * - Apply batching/indexing logic to documents
 * - Update database with batch/set metadata
 * - Generate packaging CSV files for migration
 * 
 * Processing Flow:
 * Step 1: Validate/Create folder structure
 * Step 2: Move CSV files from Completed → Indexing\Data
 * Step 3: Extract claim numbers → Query DB → Apply batching logic
 * Step 4: Update database with indexing metadata
 * Step 5a: Write success/failed tracking files → Archive CSV
 * Step 5b: Generate packaging CSV files (pipe-delimited, max 50K rows)
 */
public class CCDataIndexPackagingService {

    private static final Logger logger = LogManager.getLogger(CCDataIndexPackagingService.class);

    public static void main(String[] args) {

        logger.info("===== CCDataIndexAndPackagingUtility STARTED =====");
        ConnectionManager connManager = null;

        try {
            // Load configuration
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            logger.info("=== CONFIGURATION ===");
            logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            logger.info("Config File: {}", System.getProperty("config.file"));
            logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String basePath = config.getProperty("app.base_path");
            logger.info("Base Path: {}", basePath);
            
            // STEP 1: Validate and create required folder structure
            logger.info("=== STEP 1: Validating Folder Structure ===");
            if (!validateAndCreateFolderStructure(basePath)) {
                logger.error("Failed to validate/create folder structure. Exiting...");
                return;
            }
            logger.info("Folder structure validated successfully");
            
            // Initialize connection manager
            connManager = new ConnectionManager(config);
            logger.info("Database connection pool initialized");
            
            // Initialize utility class
            Utils utils = new Utils(config, connManager);
            
            // STEP 2: Move CSV files from Completed → Indexing\Data
            logger.info("=== STEP 2: Moving CSV Files ===");
            int movedFileCount = moveCSVFilesToIndexingData(basePath);
            logger.info("Moved {} CSV file(s) to Indexing\\Data folder", movedFileCount);
            
            if (movedFileCount == 0) {
                logger.warn("No CSV files found to process. Exiting...");
                return;
            }
            
            // STEP 3-5: Process all CSV files in Indexing\Data folder
            logger.info("=== STEP 3-5: Processing CSV Files ===");
            Path indexingDataDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
            int processedFileCount = processCSVFiles(indexingDataDir, utils);
            
            logger.info("=== PROCESSING COMPLETE ===");
            logger.info("Total CSV files processed: {}", processedFileCount);
            logger.info("===== CCDataIndexAndPackagingUtility COMPLETED SUCCESSFULLY =====");

        } catch (Exception e) {
            logger.error("Critical error in CCDataIndexPackagingService", e);
            e.printStackTrabc();
        } finally {
            // Close connection pool
            if (connManager != null) {
                try {
                    logger.info("Closing database connection pool...");
                    connManager.close();
                    logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    logger.error("Error closing connection manager", e);
                }
            }
            
            logger.info("Application terminated");
        }
    }

    // ==================== STEP 1: VALIDATE AND CREATE FOLDERS ====================
    
    /**
     * Step 1: Validate and create required folder structure
     * 
     * Required folders:
     * - D:\Rameshwar\DoubleDoubleDataMerge\Completed (source)
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Data
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Archive
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Completed
     * - D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Failed
     * - D:\Rameshwar\DoubleDoubleDataMerge\Packaging
     */
    private static boolean validateAndCreateFolderStructure(String basePath) {
        boolean success = true;
        
        // Define required folders
        String[] requiredFolders = {
            Constants.COMPLETED_SOURCE_FOLDER,
            Constants.INDEXING_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.DATA_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.ARCHIVE_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.COMPLETED_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.FAILED_FOLDER,
            Constants.PACKAGING_FOLDER
        };
        
        for (String folderName : requiredFolders) {
            File folder = new File(basePath, folderName);
            
            if (folder.exists() && folder.isDirectory()) {
                logger.info("✓ Folder exists: {}", folder.getAbsolutePath());
            } else {
                // Create folder
                if (folder.mkdirs()) {
                    logger.info("✓ Folder created: {}", folder.getAbsolutePath());
                } else {
                    logger.error("✗ Failed to create folder: {}", folder.getAbsolutePath());
                    success = false;
                }
            }
        }
        
        return success;
    }
    
    // ==================== STEP 2: MOVE CSV FILES ====================
    
    /**
     * Step 2: Move all CSV files from Completed → Indexing\Data
     * 
     * Source: D:\Rameshwar\DoubleDoubleDataMerge\Completed
     * Target: D:\Rameshwar\DoubleDoubleDataMerge\Indexing\Data
     */
    private static int moveCSVFilesToIndexingData(String basePath) {
        Path sourceDir = Paths.get(basePath, Constants.COMPLETED_SOURCE_FOLDER);
        Path targetDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
        
        logger.info("Source Directory: {}", sourceDir);
        logger.info("Target Directory: {}", targetDir);
        
        int movedCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    try {
                        String fileName = csvFile.getFileName().toString();
                        Path targetFile = targetDir.resolve(fileName);
                        
                        Files.move(csvFile, targetFile, StandardCopyOption.REPLabc_EXISTING);
                        logger.info("Moved: {} → {}", fileName, targetDir);
                        movedCount++;
                        
                    } catch (IOException e) {
                        logger.error("Failed to move file: {}", csvFile.getFileName(), e);
                    }
                }
            }
        } catch (IOException e) {
            logger.error("Error reading source directory: {}", sourceDir, e);
        }
        
        return movedCount;
    }
    
    // ==================== STEP 3-5: PROCESS CSV FILES ====================
    
    /**
     * Step 3-5: Process all CSV files in Indexing\Data folder
     * 
     * For each CSV file:
     * - Extract claim numbers
     * - Query database for documents
     * - Apply batching/indexing logic
     * - Update database
     * - Write tracking files
     * - Generate packaging CSV files
     * - Archive processed CSV
     */
    private static int processCSVFiles(Path indexingDataDir, Utils utils) {
        logger.info("Scanning Indexing\\Data directory for CSV files: {}", indexingDataDir);
        
        int fileCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexingDataDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    fileCount++;
                    logger.info("========================================");
                    logger.info("Processing CSV file [{}]: {}", fileCount, csvFile.getFileName());
                    logger.info("========================================");
                    
                    try {
                        // Process the CSV file (Steps 3-5)
                        utils.processCSVFile(csvFile);
                        
                    } catch (Exception e) {
                        logger.error("Error processing CSV file: {}", csvFile.getFileName(), e);
                    }
                }
            }
            
            if (fileCount == 0) {
                logger.warn("No CSV files found in Indexing\\Data directory: {}", indexingDataDir);
            } else {
                logger.info("Total {} CSV file(s) processed", fileCount);
            }
            
        } catch (IOException e) {
            logger.error("Error reading Indexing\\Data directory: {}", indexingDataDir, e);
        }
        
        return fileCount;
    }
}


=========================

package com.abcdefg.abc.index.constants;

/**
 * Constants for CCDataIndexAndPackagingUtility
 * Defines folder structure and batch/set configuration
 */
public class Constants {

    // ==================== FOLDER STRUCTURE ====================
    // Root folders under base path (D:\Rameshwar\DoubleDoubleDataMerge)
    public static final String COMPLETED_SOURCE_FOLDER = "Completed";      // Source folder from CCDataMergeUtility
    public static final String INDEXING_FOLDER         = "Indexing";       // Main indexing folder
    public static final String PACKAGING_FOLDER        = "Packaging";      // Output folder for packaged CSV files
    
    // Sub-folders under Indexing folder
    public static final String DATA_FOLDER             = "Data";           // CSV files being indexed
    public static final String ARCHIVE_FOLDER          = "Archive";        // Archived CSV files after processing
    public static final String COMPLETED_FOLDER        = "Completed";      // Success tracking files
    public static final String FAILED_FOLDER           = "Failed";         // Failed tracking files
    
    // ==================== BATCH & SET CONFIGURATION ====================
    // Maximum documents per batch (configurable)
    public static final int DEFAULT_BATCH_DOC_COUNT = 500;
    
    // Maximum documents per set within a batch (configurable)
    public static final int DEFAULT_SET_DOC_COUNT = 25;
    
    // Maximum rows per packaging CSV file
    public static final int MAX_PACKAGING_FILE_ROWS = 50000;
    
    // ==================== BATCH ID GENERATION ====================
    // BatchID format: wawa_<yyyyMMddHHmmssSSSSS>
    public static final String BATCH_ID_PREFIX = "wawa_";
    
    // JobID format: wawa_migrate_<yyyyMMddHHmmssSSSSS>
    public static final String JOB_ID_PREFIX = "wawa_migrate_";
    
    // Packaging file name format: wawa_migration_docs_<yyyyMMddHHmmssSSSSS>.csv
    public static final String PACKAGING_FILE_PREFIX = "wawa_migration_docs_";
    public static final String PACKAGING_FILE_EXTENSION = ".csv";
    
    // ==================== FILE NAMING CONVENTIONS ====================
    public static final String INDEX_SUCCESS_SUFFIX = "_INDEX_SUCCESS.csv";
    public static final String INDEX_FAILED_SUFFIX  = "_INDEX_FAILED.csv";
    
    // ==================== CSV DELIMITERS ====================
    public static final String PIPE_DELIMITER = "|";
    
}



==================================================


package com.abcdefg.abc.index.model;

import java.util.Date;

/**
 * Data Transfer Object for DoubleDouble Document
 * Used for indexing, batching, and packaging document metadata
 */
public class DoubleDoubleDocumentDTO {
    
    // ==================== CORE FIELDS ====================
    private String externalID;          // Unique identifier (Primary key for updates)
    private String claimNumber;         // Claim number (used for grouping)
    private String claimID;
    private String gwDocumentID;
    
    // ==================== INDEXING FIELDS (Updated during Step 3-4) ====================
    private Integer batchDocCount;      // Max documents per batch (e.g., 500)
    private String batchID;             // Unique batch ID: wawa_<timestamp>
    private String jobID;               // Job ID: wawa_migrate_<timestamp>
    private Integer setDocCount;        // Max documents per set (e.g., 25)
    private Integer setID;              // Set ID within batch (1, 2, 3...)
    private Integer setDocIndex;        // Document index within set (1-25)
    private Boolean isIndexed;          // Flag: indexing completed
    private Boolean isDataMerged;       // Flag: data merge completed (query filter)
    private Boolean isProcessed;        // Flag: packaging completed
    private Date dateProcessed;         // Timestamp of packaging
    
    // ==================== PACKAGING OUTPUT FIELDS (Step 5) ====================
    private String amount;
    private String author;
    private String claimant;
    private String coverage;
    private String customerID;
    private String documentDescription;
    private String documentSubtype;
    private String documentTitle;
    private String documentType;
    private String doNotCreateActivity;
    private String duplicate;
    private String exposureID;
    private String hidden;
    private String inputMethod;
    private String insuredName;
    private String mimeType;
    private String origDateCreated;
    private String policyNumber;
    private String primaryMembershipNumber;
    private String reviewed;
    private String sensitive;
    private String clbSecurityController;
    private String contentFilePath;
    private String contentRetrievalName;
    
    // Metadata
    private String csvFileName;
    
    // ==================== CONSTRUCTORS ====================
    public DoubleDoubleDocumentDTO() {
    }
    
    // ==================== GETTERS AND SETTERS ====================
    
    public String getExternalID() {
        return externalID;
    }
    
    public void setExternalID(String externalID) {
        this.externalID = externalID;
    }
    
    public String getClaimNumber() {
        return claimNumber;
    }
    
    public void setClaimNumber(String claimNumber) {
        this.claimNumber = claimNumber;
    }
    
    public String getClaimID() {
        return claimID;
    }
    
    public void setClaimID(String claimID) {
        this.claimID = claimID;
    }
    
    public String getGwDocumentID() {
        return gwDocumentID;
    }
    
    public void setGwDocumentID(String gwDocumentID) {
        this.gwDocumentID = gwDocumentID;
    }
    
    public Integer getBatchDocCount() {
        return batchDocCount;
    }
    
    public void setBatchDocCount(Integer batchDocCount) {
        this.batchDocCount = batchDocCount;
    }
    
    public String getBatchID() {
        return batchID;
    }
    
    public void setBatchID(String batchID) {
        this.batchID = batchID;
    }
    
    public String getJobID() {
        return jobID;
    }
    
    public void setJobID(String jobID) {
        this.jobID = jobID;
    }
    
    public Integer getSetDocCount() {
        return setDocCount;
    }
    
    public void setSetDocCount(Integer setDocCount) {
        this.setDocCount = setDocCount;
    }
    
    public Integer getSetID() {
        return setID;
    }
    
    public void setSetID(Integer setID) {
        this.setID = setID;
    }
    
    public Integer getSetDocIndex() {
        return setDocIndex;
    }
    
    public void setSetDocIndex(Integer setDocIndex) {
        this.setDocIndex = setDocIndex;
    }
    
    public Boolean getIsIndexed() {
        return isIndexed;
    }
    
    public void setIsIndexed(Boolean isIndexed) {
        this.isIndexed = isIndexed;
    }
    
    public Boolean getIsDataMerged() {
        return isDataMerged;
    }
    
    public void setIsDataMerged(Boolean isDataMerged) {
        this.isDataMerged = isDataMerged;
    }
    
    public Boolean getIsProcessed() {
        return isProcessed;
    }
    
    public void setIsProcessed(Boolean isProcessed) {
        this.isProcessed = isProcessed;
    }
    
    public Date getDateProcessed() {
        return dateProcessed;
    }
    
    public void setDateProcessed(Date dateProcessed) {
        this.dateProcessed = dateProcessed;
    }
    
    public String getAmount() {
        return amount;
    }
    
    public void setAmount(String amount) {
        this.amount = amount;
    }
    
    public String getAuthor() {
        return author;
    }
    
    public void setAuthor(String author) {
        this.author = author;
    }
    
    public String getClaimant() {
        return claimant;
    }
    
    public void setClaimant(String claimant) {
        this.claimant = claimant;
    }
    
    public String getCoverage() {
        return coverage;
    }
    
    public void setCoverage(String coverage) {
        this.coverage = coverage;
    }
    
    public String getCustomerID() {
        return customerID;
    }
    
    public void setCustomerID(String customerID) {
        this.customerID = customerID;
    }
    
    public String getDocumentDescription() {
        return documentDescription;
    }
    
    public void setDocumentDescription(String documentDescription) {
        this.documentDescription = documentDescription;
    }
    
    public String getDocumentSubtype() {
        return documentSubtype;
    }
    
    public void setDocumentSubtype(String documentSubtype) {
        this.documentSubtype = documentSubtype;
    }
    
    public String getDocumentTitle() {
        return documentTitle;
    }
    
    public void setDocumentTitle(String documentTitle) {
        this.documentTitle = documentTitle;
    }
    
    public String getDocumentType() {
        return documentType;
    }
    
    public void setDocumentType(String documentType) {
        this.documentType = documentType;
    }
    
    public String getDoNotCreateActivity() {
        return doNotCreateActivity;
    }
    
    public void setDoNotCreateActivity(String doNotCreateActivity) {
        this.doNotCreateActivity = doNotCreateActivity;
    }
    
    public String getDuplicate() {
        return duplicate;
    }
    
    public void setDuplicate(String duplicate) {
        this.duplicate = duplicate;
    }
    
    public String getExposureID() {
        return exposureID;
    }
    
    public void setExposureID(String exposureID) {
        this.exposureID = exposureID;
    }
    
    public String getHidden() {
        return hidden;
    }
    
    public void setHidden(String hidden) {
        this.hidden = hidden;
    }
    
    public String getInputMethod() {
        return inputMethod;
    }
    
    public void setInputMethod(String inputMethod) {
        this.inputMethod = inputMethod;
    }
    
    public String getInsuredName() {
        return insuredName;
    }
    
    public void setInsuredName(String insuredName) {
        this.insuredName = insuredName;
    }
    
    public String getMimeType() {
        return mimeType;
    }
    
    public void setMimeType(String mimeType) {
        this.mimeType = mimeType;
    }
    
    public String getOrigDateCreated() {
        return origDateCreated;
    }
    
    public void setOrigDateCreated(String origDateCreated) {
        this.origDateCreated = origDateCreated;
    }
    
    public String getPolicyNumber() {
        return policyNumber;
    }
    
    public void setPolicyNumber(String policyNumber) {
        this.policyNumber = policyNumber;
    }
    
    public String getPrimaryMembershipNumber() {
        return primaryMembershipNumber;
    }
    
    public void setPrimaryMembershipNumber(String primaryMembershipNumber) {
        this.primaryMembershipNumber = primaryMembershipNumber;
    }
    
    public String getReviewed() {
        return reviewed;
    }
    
    public void setReviewed(String reviewed) {
        this.reviewed = reviewed;
    }
    
    public String getSensitive() {
        return sensitive;
    }
    
    public void setSensitive(String sensitive) {
        this.sensitive = sensitive;
    }
    
    public String getClbSecurityController() {
        return clbSecurityController;
    }
    
    public void setClbSecurityController(String clbSecurityController) {
        this.clbSecurityController = clbSecurityController;
    }
    
    public String getContentFilePath() {
        return contentFilePath;
    }
    
    public void setContentFilePath(String contentFilePath) {
        this.contentFilePath = contentFilePath;
    }
    
    public String getContentRetrievalName() {
        return contentRetrievalName;
    }
    
    public void setContentRetrievalName(String contentRetrievalName) {
        this.contentRetrievalName = contentRetrievalName;
    }
    
    public String getCsvFileName() {
        return csvFileName;
    }
    
    public void setCsvFileName(String csvFileName) {
        this.csvFileName = csvFileName;
    }
    
    /**
     * Generates pipe-delimited packaging output line in specified column order
     * Order: amount|author|batchDocCount|batchID|claimant|claimID|claimNumber|coverage|
     *        customerID|documentDescription|documentSubtype|documentTitle|documentType|
     *        doNotCreateActivity|duplicate|exposureID|gwDocumentID|hidden|inputMethod|
     *        insuredName|jobID|mimeType|OrigDateCreated|policyNumber|primaryMembershipNumber|
     *        reviewed|sensitive|setDocCount|setID|SetDocIndex|ClbSecurityController|
     *        contentFilePath|contentRetrievalName
     */
    public String toPackagingCSVLine() {
        return String.join("|",
            nullSafe(amount),
            nullSafe(author),
            nullSafe(batchDocCount),
            nullSafe(batchID),
            nullSafe(claimant),
            nullSafe(claimID),
            nullSafe(claimNumber),
            nullSafe(coverage),
            nullSafe(customerID),
            nullSafe(documentDescription),
            nullSafe(documentSubtype),
            nullSafe(documentTitle),
            nullSafe(documentType),
            nullSafe(doNotCreateActivity),
            nullSafe(duplicate),
            nullSafe(exposureID),
            nullSafe(gwDocumentID),
            nullSafe(hidden),
            nullSafe(inputMethod),
            nullSafe(insuredName),
            nullSafe(jobID),
            nullSafe(mimeType),
            nullSafe(origDateCreated),
            nullSafe(policyNumber),
            nullSafe(primaryMembershipNumber),
            nullSafe(reviewed),
            nullSafe(sensitive),
            nullSafe(setDocCount),
            nullSafe(setID),
            nullSafe(setDocIndex),
            nullSafe(clbSecurityController),
            nullSafe(contentFilePath),
            nullSafe(contentRetrievalName)
        );
    }
    
    /**
     * Returns packaging CSV header in correct column order
     */
    public static String getPackagingCSVHeader() {
        return "amount|author|batchDocCount|batchID|claimant|claimID|claimNumber|coverage|" +
               "customerID|documentDescription|documentSubtype|documentTitle|documentType|" +
               "doNotCreateActivity|duplicate|exposureID|gwDocumentID|hidden|inputMethod|" +
               "insuredName|jobID|mimeType|OrigDateCreated|policyNumber|primaryMembershipNumber|" +
               "reviewed|sensitive|setDocCount|setID|SetDocIndex|ClbSecurityController|" +
               "contentFilePath|contentRetrievalName";
    }
    
    private String nullSafe(Object value) {
        return value == null ? "" : value.toString();
    }
    
    @Override
    public String toString() {
        return "DoubleDoubleDocumentDTO{" +
                "externalID='" + externalID + '\'' +
                ", claimNumber='" + claimNumber + '\'' +
                ", batchID='" + batchID + '\'' +
                ", setID=" + setID +
                ", setDocIndex=" + setDocIndex +
                '}';
    }
}




=============================================


package com.abcdefg.abc.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Set;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.abcdefg.abc.index.configuration.PropertiesConfigLoader;
import com.abcdefg.abc.index.connection.ConnectionManager;
import com.abcdefg.abc.index.constants.Constants;
import com.abcdefg.abc.index.model.DoubleDoubleDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int batchDocCount;
    private int setDocCount;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.DoubleDoubledbtable");
        
        // Load batch/set configuration (with defaults)
        String batchDocCountStr = config.getProperty("app.batch.doc.count");
        this.batchDocCount = (batchDocCountStr != null) ? 
            Integer.parseInt(batchDocCountStr) : Constants.DEFAULT_BATCH_DOC_COUNT;
        
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        logger.info("Utils initialized with batchDocCount={}, setDocCount={}", batchDocCount, setDocCount);
    }
    
    /**
     * Inner class to track processing results
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Query DB for documents
     * - Apply batching/indexing logic
     * - Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     */
    public void processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} =====", csvFileName);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return;
            }
            
            // STEP 3: Query DB for documents matching claim numbers
            List<DoubleDoubleDocumentDTO> documents = fetchDocumentsForClaims(claimNumbers);
            result.totalDocuments = documents.size();
            logger.info("Retrieved {} documents from database for indexing", documents.size());
            
            if (documents.isEmpty()) {
                logger.warn("No documents found with isDataMerged=1. Skipping...");
                archiveCSVFile(csvFilePath);
                return;
            }
            
            // STEP 3: Apply batching and indexing logic
            applyBatchingLogic(documents, result);
            logger.info("Created {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            
            // STEP 4: Update database with indexing metadata
            updateDatabaseWithIndexing(documents, successLines, failedLines, result);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLabc_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replabc("\"", "");
                    if (header.equalsIgnoreCase("claimNumber") || header.equalsIgnoreCase("ClaimNumber")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents matching claim numbers with isDataMerged = 1
     */
    private List<DoubleDoubleDocumentDTO> fetchDocumentsForClaims(Set<String> claimNumbers) throws SQLException {
        List<DoubleDoubleDocumentDTO> documents = new ArrayList<>();
        
        // Build IN clause for claim numbers
        StringBuilder inClause = new StringBuilder();
        for (int i = 0; i < claimNumbers.size(); i++) {
            if (i > 0) inClause.append(",");
            inClause.append("?");
        }
        
        String selectSQL = "SELECT externalID, claimNumber, claimID, gwDocumentID, " +
                          "amount, author, claimant, coverage, customerID, " +
                          "documentDescription, documentSubtype, documentTitle, documentType, " +
                          "doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, " +
                          "insuredName, mimeType, OrigDateCreated, policyNumber, " +
                          "primaryMembershipNumber, reviewed, sensitive, " +
                          "ClbSecurityController, contentFilePath, contentRetrievalName " +
                          "FROM " + dbTableName + " " +
                          "WHERE claimNumber IN (" + inClause + ") AND isDataMerged = 1 " +
                          "ORDER BY claimNumber, externalID";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String claimNumber : claimNumbers) {
                pstmt.setString(paramIndex++, claimNumber);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    DoubleDoubleDocumentDTO dto = new DoubleDoubleDocumentDTO();
                    
                    // Core fields
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    
                    // Packaging fields
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getString("doNotCreateActivity"));
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getString("sensitive"));
                    dto.setClbSecurityController(rs.getString("ClbSecurityController"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    documents.add(dto);
                }
            }
        }
        
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents
     * - Batch: Max batchDocCount documents per batch (e.g., 500)
     * - Set: Max setDocCount documents per set (e.g., 25)
     * - Generate unique batchID and jobID for each batch
     */
    private void applyBatchingLogic(List<DoubleDoubleDocumentDTO> documents, ProcessingResult result) {
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size
            int currentBatchSize = Math.min(batchDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID();
            result.batchIDsCreated.add(batchID);
            
            logger.info("Creating batch: batchID={}, docs={}", batchID, currentBatchSize);
            
            // Process documents in current batch
            int setID = 1;
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                DoubleDoubleDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata
                doc.setSetID(setID);
                doc.setSetDocCount(setDocCount);
                doc.setSetDocIndex(setDocIndex);
                
                // Increment set document index
                setDocIndex++;
                
                // Move to next set if current set is full
                if (setDocIndex > setDocCount) {
                    setID++;
                    setDocIndex = 1;
                }
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
        }
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID() {
        return Constants.JOB_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata for each document
     */
    private void updateDatabaseWithIndexing(List<DoubleDoubleDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
                          "batchDocCount = ?, batchID = ?, jobID = ?, " +
                          "setDocCount = ?, setID = ?, SetDocIndex = ?, isIndexed = 1 " +
                          "WHERE externalID = ?";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            
            conn.setAutoCommit(false); // Use transaction for performance
            
            for (DoubleDoubleDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                        logger.debug("Indexed: externalID={}, batch={}, set={}, index={}", 
                                   doc.getExternalID(), doc.getBatchID(), doc.getSetID(), doc.getSetDocIndex());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                        logger.warn("Failed to index (no record found): externalID={}", doc.getExternalID());
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Database transaction committed successfully");
            
        } catch (SQLException e) {
            logger.error("Database transaction failed", e);
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replabc(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        String selectSQL = "SELECT externalID, claimNumber, claimID, gwDocumentID, " +
                          "amount, author, batchDocCount, batchID, claimant, coverage, customerID, " +
                          "documentDescription, documentSubtype, documentTitle, documentType, " +
                          "doNotCreateActivity, duplicate, exposureID, hidden, inputMethod, " +
                          "insuredName, jobID, mimeType, OrigDateCreated, policyNumber, " +
                          "primaryMembershipNumber, reviewed, sensitive, setDocCount, setID, SetDocIndex, " +
                          "ClbSecurityController, contentFilePath, contentRetrievalName " +
                          "FROM " + dbTableName + " " +
                          "WHERE batchID IN (" + inClause + ") " +
                          "AND isDataMerged = 1 AND isIndexed = 1 AND isProcessed = 0 " +
                          "ORDER BY batchID, setID, SetDocIndex";
        
        List<DoubleDoubleDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    DoubleDoubleDocumentDTO dto = new DoubleDoubleDocumentDTO();
                    
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getString("doNotCreateActivity"));
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getString("sensitive"));
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    dto.setClbSecurityController(rs.getString("ClbSecurityController"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<DoubleDoubleDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (DoubleDoubleDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(DoubleDoubleDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<DoubleDoubleDocumentDTO> documents) throws SQLException {
        
        String updateSQL = "UPDATE " + dbTableName + " SET " +
                          "isProcessed = 1, DateProcessed = ? " +
                          "WHERE externalID = ?";
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateSQL)) {
            
            conn.setAutoCommit(false);
            Date currentDate = new Date();
            
            for (DoubleDoubleDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLabc_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replabc(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
}
