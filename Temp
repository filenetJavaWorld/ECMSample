package com.wawanesa.ace.index;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.utils.GlobalProcessingReport;
import com.wawanesa.ace.index.utils.Utils;

/**
 * CCDataIndexAndPackagingUtility - Main Service Class
 * 
 * Purpose:
 * - Process documents from CC_Extract_Staging table
 * - Apply batching/indexing logic to documents
 * - Update database with batch/set metadata
 * - Generate packaging CSV files for migration
 * 
 * Processing Modes:
 * 
 * MODE 1: ALL PENDING (Default - No Input File Required)
 * - Queries CC_Extract_Staging directly for all documents matching:
 *   claimID IS NOT NULL AND gwDocumentID IS NOT NULL AND 
 *   isDataMerged = 1 AND contentFilePath IS NOT NULL AND isIndexed = 0
 * - Groups by (CC_Extract_file_Name, claimNumber)
 * - Processes all pending documents automatically
 * - Usage: java -jar CCDataIndexAndPackagingUtility.jar
 * 
 * MODE 2: CSV FILE (Legacy - Input Files Required)
 * - Processes CSV files from Completed → Indexing\Data folder
 * - Each CSV contains claim numbers to process
 * - Usage: java -jar CCDataIndexAndPackagingUtility.jar --csv-mode
 * 
 * Processing Flow (Both Modes):
 * Step 1: Validate/Create folder structure
 * Step 2: (CSV mode only) Move CSV files from Completed → Indexing\Data
 * Step 3: Query DB → Apply batching logic
 * Step 4: Update database with indexing metadata
 * Step 5a: Write success/failed tracking files
 * Step 5b: Generate packaging CSV files (pipe-delimited, max 50K rows)
 */
public class CCDataIndexPackagingService {

    private static final Logger logger = LogManager.getLogger(CCDataIndexPackagingService.class);

    public static void main(String[] args) {

        logger.info("===== CCDataIndexAndPackagingUtility STARTED =====");
        
        // Determine processing mode from command line arguments
        boolean csvMode = false;
        for (String arg : args) {
            if ("--csv-mode".equalsIgnoreCase(arg) || "-c".equalsIgnoreCase(arg)) {
                csvMode = true;
                break;
            }
        }
        
        if (csvMode) {
            logger.info("Mode: CSV FILE MODE (Legacy - Processing CSV files from Completed folder)");
        } else {
            logger.info("Mode: ALL PENDING MODE (Default - Processing all pending documents from CC_Extract_Staging)");
        }
        
        ConnectionManager connManager = null;
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        
        // Initialize global processing report
        GlobalProcessingReport globalReport = new GlobalProcessingReport();

        try {
            // Load configuration
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            logger.info("=== CONFIGURATION ===");
            logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            logger.info("Config File: {}", System.getProperty("config.file"));
            logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String basePath = config.getProperty("app.base_path");
            logger.info("Base Path: {}", basePath);
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService;
            
            logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            final ConnectionManager[] connManagerRef = new ConnectionManager[1];
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        globalReport.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            globalReport.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                logger.info("Shutdown hook completed");
            }));
            
            // STEP 1: Validate and create required folder structure
            logger.info("=== STEP 1: Validating Folder Structure ===");
            if (!validateAndCreateFolderStructure(basePath)) {
                logger.error("Failed to validate/create folder structure. Exiting...");
                return;
            }
            logger.info("Folder structure validated successfully");
            
            // Initialize connection manager
            connManager = new ConnectionManager(config);
            connManagerRef[0] = connManager;
            logger.info("Database connection pool initialized");
            
            // Initialize utility class
            Utils utils = new Utils(config, connManager);
            
            if (csvMode) {
                // ==================== CSV FILE MODE (Legacy) ====================
                // STEP 2: Move CSV files from Completed → Indexing\Data
                logger.info("=== STEP 2: Moving CSV Files ===");
                int movedFileCount = moveCSVFilesToIndexingData(basePath);
                logger.info("Moved {} CSV file(s) to Indexing\\Data folder", movedFileCount);
                
                if (movedFileCount == 0) {
                    logger.warn("No CSV files found to process. Exiting...");
                    return;
                }
                
                // STEP 3-5: Process all CSV files in Indexing\Data folder (MULTI-THREADED)
                logger.info("=== STEP 3-5: Processing CSV Files (Multi-Threaded) ===");
                Path indexingDataDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
                int processedFileCount = processCSVFiles(indexingDataDir, utils, executor, globalReport);
                
                logger.info("=== PROCESSING COMPLETE ===");
                logger.info("Total CSV files processed: {}", processedFileCount);
            } else {
                // ==================== ALL PENDING MODE (Default) ====================
                // Process all pending documents directly from CC_Extract_Staging
                logger.info("=== STEP 2: Processing All Pending Documents ===");
                
                Utils.ProcessingResult result = utils.processAllPendingDocuments();
                
                // Record result in global report
                globalReport.recordAllPendingResult(result);
                
                logger.info("=== PROCESSING COMPLETE ===");
                logger.info("Total documents processed: {}", result.totalDocuments);
                logger.info("Total claims processed: {}", result.totalClaimNumbers);
                logger.info("Indexed successfully: {}", result.indexedSuccessCount);
                logger.info("Indexed failed: {}", result.indexedFailureCount);
                logger.info("Packaging files created: {}", result.packagingFilesCreated);
            }
            logger.info("===== CCDataIndexAndPackagingUtility COMPLETED SUCCESSFULLY =====");

        } catch (Exception e) {
            logger.error("Critical error in CCDataIndexPackagingService", e);
            e.printStackTrace();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService gracefully
            if (executor != null && !executor.isShutdown()) {
                try {
                    logger.info("Shutting down executor service...");
                    executor.shutdown();
                    
                    // Calculate dynamic timeout based on number of files queued
                    // Each CSV file can process multiple claims with database operations
                    // Production-optimized: Conservative timeouts for complex processing
                    int filesProcessed = globalReport.getCsvFilesQueued();
                    long shutdownTimeout;
                    
                    if (filesProcessed == 0) {
                        // No files processed - quick shutdown
                        shutdownTimeout = 10; // 10 seconds
                    } else if (filesProcessed <= 10) {
                        // Small batch: 10 minutes per file (indexing + packaging can be lengthy)
                        shutdownTimeout = filesProcessed * 600;
                    } else if (filesProcessed <= 100) {
                        // Medium batch: 5 minutes per file + 30 minute buffer
                        shutdownTimeout = (filesProcessed * 300) + 1800;
                    } else {
                        // Large batch: 2 minutes per file + 1 hour buffer, max 24 hours
                        long calculated = (filesProcessed * 120) + 3600;
                        shutdownTimeout = Math.min(calculated, 86400); // 24 hours max
                    }
                    
                    logger.info("Waiting for {} file(s) to complete (timeout: {} seconds / {}.{} minutes)...", 
                               filesProcessed, shutdownTimeout, shutdownTimeout / 60, (shutdownTimeout % 60));
                    
                    long startTime = System.currentTimeMillis();
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(shutdownTimeout, TimeUnit.SECONDS)) {
                        long waitedTime = (System.currentTimeMillis() - startTime) / 1000;
                        logger.warn("Executor did not terminate after {} seconds, forcing shutdown...", waitedTime);
                        executor.shutdownNow();
                        
                        if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                            logger.error("Executor did not terminate after forced shutdown");
                        }
                    } else {
                        long actualTime = (System.currentTimeMillis() - startTime) / 1000;
                        logger.info("All tasks completed successfully in {} seconds", actualTime);
                    }
                    
                    logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManager != null) {
                try {
                    logger.info("Closing database connection pool...");
                    connManager.close();
                    logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    logger.error("Error closing connection manager", e);
                }
            }
            
            // Write final summary report
            try {
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                
                logger.info("===== FINAL SUMMARY =====");
                logger.info(globalReport.getSummary());
                
                // Validate/Create SummaryReports folder
                File summaryReportsDir = new File(basePath, "SummaryReports");
                if (!summaryReportsDir.exists()) {
                    if (summaryReportsDir.mkdirs()) {
                        logger.info("Created SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                    } else {
                        logger.error("Failed to create SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                        throw new IOException("Cannot create SummaryReports folder");
                    }
                } else {
                    logger.debug("SummaryReports folder already exists: {}", summaryReportsDir.getAbsolutePath());
                }
                
                globalReport.writeReportToFile(basePath);
                logger.info("Summary report written to: {}", summaryReportsDir.getAbsolutePath());
            } catch (Exception e) {
                logger.error("Failed to write summary report", e);
            }
            
            logger.info("Application terminated");
        }
    }

    // ==================== STEP 1: VALIDATE AND CREATE FOLDERS ====================
    
    /**
     * Step 1: Validate and create required folder structure
     * 
     * Required folders:
     * - D:\Rameshwar\ClaimCenterDataMerge\Completed (source)
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Archive
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Completed
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Failed
     * - D:\Rameshwar\ClaimCenterDataMerge\Packaging
     */
    private static boolean validateAndCreateFolderStructure(String basePath) {
        boolean success = true;
        
        // Define required folders
        String[] requiredFolders = {
            Constants.COMPLETED_SOURCE_FOLDER,
            Constants.INDEXING_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.DATA_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.ARCHIVE_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.COMPLETED_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.FAILED_FOLDER,
            Constants.PACKAGING_FOLDER
        };
        
        for (String folderName : requiredFolders) {
            File folder = new File(basePath, folderName);
            
            if (folder.exists() && folder.isDirectory()) {
                logger.info("✓ Folder exists: {}", folder.getAbsolutePath());
            } else {
                // Create folder
                if (folder.mkdirs()) {
                    logger.info("✓ Folder created: {}", folder.getAbsolutePath());
                } else {
                    logger.error("✗ Failed to create folder: {}", folder.getAbsolutePath());
                    success = false;
                }
            }
        }
        
        return success;
    }
    
    // ==================== STEP 2: MOVE CSV FILES ====================
    
    /**
     * Step 2: Move all CSV files from Completed → Indexing\Data
     * 
     * Source: D:\Rameshwar\ClaimCenterDataMerge\Completed
     * Target: D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     */
    private static int moveCSVFilesToIndexingData(String basePath) {
        Path sourceDir = Paths.get(basePath, Constants.COMPLETED_SOURCE_FOLDER);
        Path targetDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
        
        logger.info("Source Directory: {}", sourceDir);
        logger.info("Target Directory: {}", targetDir);
        
        int movedCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    try {
                        String fileName = csvFile.getFileName().toString();
                        Path targetFile = targetDir.resolve(fileName);
                        
                        Files.move(csvFile, targetFile, StandardCopyOption.REPLACE_EXISTING);
                        logger.info("Moved: {} → {}", fileName, targetDir);
                        movedCount++;
                        
                    } catch (IOException e) {
                        logger.error("Failed to move file: {}", csvFile.getFileName(), e);
                    }
                }
            }
        } catch (IOException e) {
            logger.error("Error reading source directory: {}", sourceDir, e);
        }
        
        return movedCount;
    }
    
    // ==================== STEP 3-5: PROCESS CSV FILES (MULTI-THREADED) ====================
    
    /**
     * Step 3-5: Process all CSV files in Indexing\Data folder using multi-threading
     * 
     * For each CSV file (in parallel):
     * - Extract claim numbers
     * - Query database for documents
     * - Apply batching/indexing logic
     * - Update database (BATCH UPDATEs)
     * - Write tracking files
     * - Generate packaging CSV files
     * - Archive processed CSV
     */
    private static int processCSVFiles(Path indexingDataDir, Utils utils, 
                                      ExecutorService executor, GlobalProcessingReport globalReport) {
        logger.info("Scanning Indexing\\Data directory for CSV files: {}", indexingDataDir);
        
        // Collect all CSV files
        List<Path> csvFiles = new ArrayList<>();
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexingDataDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    csvFiles.add(csvFile);
                    globalReport.incrementCSVFilesQueued();
                }
            }
        } catch (IOException e) {
            logger.error("Error reading Indexing\\Data directory: {}", indexingDataDir, e);
            return 0;
        }
        
        if (csvFiles.isEmpty()) {
            logger.warn("No CSV files found in Indexing\\Data directory: {}", indexingDataDir);
            return 0;
        }
        
        logger.info("Found {} CSV file(s) to process with multi-threading", csvFiles.size());
        
        // Process each CSV file in parallel
        for (Path csvFile : csvFiles) {
            final String fileName = csvFile.getFileName().toString();
            
            executor.submit(() -> {
                long taskStartTime = System.currentTimeMillis();
                logger.info("[TASK START] Thread-{}: Processing CSV file: {}", 
                           Thread.currentThread().getId(), fileName);
                
                try {
                    // Process the CSV file (Steps 3-5)
                    Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                    
                    long taskDuration = (System.currentTimeMillis() - taskStartTime) / 1000;
                    logger.info("[TASK COMPLETE] Thread-{}: File: {} processed in {} seconds (Claims: {}, Docs: {}, Indexed: {})", 
                               Thread.currentThread().getId(), fileName, taskDuration,
                               result.totalClaimNumbers, result.totalDocuments, result.indexedSuccessCount);
                    
                    // Record success in global report
                    globalReport.recordCSVFileSuccess(fileName, result);
                    
                } catch (Exception e) {
                    long taskDuration = (System.currentTimeMillis() - taskStartTime) / 1000;
                    logger.error("[TASK FAILED] Thread-{}: File: {} failed after {} seconds - Error: {}", 
                                Thread.currentThread().getId(), fileName, taskDuration, e.getMessage(), e);
                    globalReport.recordCSVFileFailure(fileName, e.getMessage());
                }
            });
        }
        
        // NOTE: Do NOT call executor.shutdown() here - it's handled in finally block
        logger.info("All {} CSV file(s) submitted for processing", csvFiles.size());
        
        return csvFiles.size();
    }
}

package com.wawanesa.ace.index.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // CSV file-level counters
    private final AtomicInteger totalCSVFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesFailed = new AtomicInteger(0);
    
    // Claim-level counters
    private final AtomicInteger totalClaimsProcessed = new AtomicInteger(0);
    
    // Document-level counters
    private final AtomicLong totalDocumentsProcessed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexFailed = new AtomicLong(0);
    private final AtomicLong totalDocumentsPackaged = new AtomicLong(0);
    
    // Batch counters
    private final AtomicInteger totalBatchesCreated = new AtomicInteger(0);
    private final AtomicInteger totalPackagingFilesCreated = new AtomicInteger(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a CSV file has been queued for processing
     */
    public void incrementCSVFilesQueued() {
        totalCSVFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful CSV file processing
     */
    public void recordCSVFileSuccess(String fileName, Utils.ProcessingResult result) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesSuccess.incrementAndGet();
        
        totalClaimsProcessed.addAndGet(result.totalClaimNumbers);
        totalDocumentsProcessed.addAndGet(result.totalDocuments);
        totalDocumentsIndexed.addAndGet(result.indexedSuccessCount);
        totalDocumentsIndexFailed.addAndGet(result.indexedFailureCount);
        totalBatchesCreated.addAndGet(result.batchIDsCreated.size());
        totalPackagingFilesCreated.addAndGet(result.packagingFilesCreated);
        
        logger.info("CSV completed: {} | Claims={}, Docs={}, Indexed={}, Failed={}, Batches={}, PkgFiles={}", 
                   fileName, result.totalClaimNumbers, result.totalDocuments, 
                   result.indexedSuccessCount, result.indexedFailureCount,
                   result.batchIDsCreated.size(), result.packagingFilesCreated);
    }
    
    /**
     * Record failed CSV file processing
     */
    public void recordCSVFileFailure(String fileName, String errorMessage) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesFailed.incrementAndGet();
        
        logger.error("CSV failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Record result from processAllPendingDocuments (no input file mode)
     */
    public void recordAllPendingResult(Utils.ProcessingResult result) {
        // In "all pending" mode, there's no CSV file input
        // We just record the document/claim-level statistics
        totalClaimsProcessed.addAndGet(result.totalClaimNumbers);
        totalDocumentsProcessed.addAndGet(result.totalDocuments);
        totalDocumentsIndexed.addAndGet(result.indexedSuccessCount);
        totalDocumentsIndexFailed.addAndGet(result.indexedFailureCount);
        totalBatchesCreated.addAndGet(result.batchIDsCreated.size());
        totalPackagingFilesCreated.addAndGet(result.packagingFilesCreated);
        
        logger.info("All Pending completed | Claims={}, Docs={}, Indexed={}, Failed={}, Batches={}, PkgFiles={}", 
                   result.totalClaimNumbers, result.totalDocuments, 
                   result.indexedSuccessCount, result.indexedFailureCount,
                   result.batchIDsCreated.size(), result.packagingFilesCreated);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Get total CSV files queued for processing
     */
    public int getCsvFilesQueued() {
        return totalCSVFilesQueued.get();
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("CSV Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalCSVFilesQueued.get(), totalCSVFilesProcessed.get(), totalCSVFilesQueued.get(),
                   totalCSVFilesSuccess.get(), totalCSVFilesFailed.get());
        logger.info("Claims: Processed={}", totalClaimsProcessed.get());
        logger.info("Documents: Processed={}, Indexed={}, Failed={}", 
                   totalDocumentsProcessed.get(), totalDocumentsIndexed.get(), totalDocumentsIndexFailed.get());
        logger.info("Batches: Created={}, Packaging Files={}", 
                   totalBatchesCreated.get(), totalPackagingFilesCreated.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- CSV File Statistics ---\n");
        sb.append(String.format("Total CSV Files Queued: %d\n", totalCSVFilesQueued.get()));
        sb.append(String.format("Total CSV Files Processed: %d\n", totalCSVFilesProcessed.get()));
        sb.append(String.format("CSV Files Success: %d\n", totalCSVFilesSuccess.get()));
        sb.append(String.format("CSV Files Failed: %d\n", totalCSVFilesFailed.get()));
        sb.append("\n--- Claim Statistics ---\n");
        sb.append(String.format("Total Claims Processed: %d\n", totalClaimsProcessed.get()));
        sb.append("\n--- Document Statistics ---\n");
        sb.append(String.format("Total Documents Processed: %d\n", totalDocumentsProcessed.get()));
        sb.append(String.format("Total Documents Indexed: %d\n", totalDocumentsIndexed.get()));
        sb.append(String.format("Total Documents Index Failed: %d\n", totalDocumentsIndexFailed.get()));
        sb.append("\n--- Batch & Packaging Statistics ---\n");
        sb.append(String.format("Total Batches Created: %d\n", totalBatchesCreated.get()));
        sb.append(String.format("Total Packaging Files Created: %d\n", totalPackagingFilesCreated.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
            long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d docs/sec, %d docs/min\n", docsPerSecond, docsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataIndexPackaging_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, "SummaryReports", fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write CSV file statistics
            writer.newLine();
            writer.write("--- CSV File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total CSV Files Queued,%d", totalCSVFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Processed,%d", totalCSVFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Success,%d", totalCSVFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Failed,%d", totalCSVFilesFailed.get()));
            writer.newLine();
            
            // Write claim statistics
            writer.newLine();
            writer.write("--- Claim Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Claims Processed,%d", totalClaimsProcessed.get()));
            writer.newLine();
            
            // Write document statistics
            writer.newLine();
            writer.write("--- Document Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Documents Processed,%d", totalDocumentsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Indexed,%d", totalDocumentsIndexed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Index Failed,%d", totalDocumentsIndexFailed.get()));
            writer.newLine();
            
            // Write batch statistics
            writer.newLine();
            writer.write("--- Batch & Packaging Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Batches Created,%d", totalBatchesCreated.get()));
            writer.newLine();
            writer.write(String.format("Total Packaging Files Created,%d", totalPackagingFilesCreated.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
                long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Documents Per Second,%d", docsPerSecond));
                writer.newLine();
                writer.write(String.format("Documents Per Minute,%d", docsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
}




package com.wawanesa.ace.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.model.ClaimCenterDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * 
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 * 
 * Data Source: CC_Extract_Staging table (changed from Wawa_Doc_Migration_Transit_Data)
 * 
 * Filter Conditions for Document Selection:
 * - claimID IS NOT NULL (synced from ClaimInfo - Step 3)
 * - gwDocumentID IS NOT NULL (synced from guidewireIDMapping - Step 4)
 * - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data - Step 5)
 * - contentFilePath IS NOT NULL
 * - isIndexed = 0 (not yet indexed)
 * - CC_Extract_file_Name matches the input CSV file
 * 
 * Column Mapping:
 * - CC_Extract_Staging.Name → documentTitle (for packaging output)
 * - Missing columns (amount, claimant, coverage, etc.) are set to empty strings
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int setDocCount;
    private boolean forceSingleSet;
    private boolean canSetDocTitleAsContentFileNameIfNull;
    private boolean setGwDocumentIDAsEmpty;
    
    // SQL Queries loaded from properties
    private String selectDocumentsByClaimQuery;
    private String selectAllPendingDocumentsQuery;  // NEW: Query all pending documents
    private String updateIndexingQuery;
    private String selectPackagingByBatchIdsQueryTemplate;
    private String updateProcessedQuery;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load set configuration (with default)
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        // Load force single set configuration (with default)
        String forceSingleSetStr = config.getProperty("app.batch.force.single.set");
        this.forceSingleSet = (forceSingleSetStr != null) ? 
            Boolean.parseBoolean(forceSingleSetStr) : Constants.DEFAULT_FORCE_SINGLE_SET;
        
        // Load configuration for setting documentTitle from contentRetrievalName if null
        String canSetDocTitleStr = config.getProperty("app.canSetDocTitleAsContentFileNameIfNULL");
        this.canSetDocTitleAsContentFileNameIfNull = (canSetDocTitleStr != null) ? 
            Boolean.parseBoolean(canSetDocTitleStr) : false;
        
        // Load configuration for setting gwDocumentID as empty during packaging
        String setGwDocIdEmptyStr = config.getProperty("app.setgwDocumentIDAsEmpty");
        this.setGwDocumentIDAsEmpty = (setGwDocIdEmptyStr != null) ? 
            Boolean.parseBoolean(setGwDocIdEmptyStr) : false;
        
        // Load SQL queries from properties and replace table name placeholder
        this.selectDocumentsByClaimQuery = config.getProperty("db.query.select.documents.by.claim")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectAllPendingDocumentsQuery = config.getProperty("db.query.select.all.pending.documents")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateIndexingQuery = config.getProperty("db.query.update.indexing")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectPackagingByBatchIdsQueryTemplate = config.getProperty("db.query.select.packaging.by.batchids")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateProcessedQuery = config.getProperty("db.query.update.processed")
            .replace("%TABLE_NAME%", this.dbTableName);
        
        logger.info("Utils initialized with setDocCount={}, forceSingleSet={}, canSetDocTitleAsContentFileNameIfNull={}, setGwDocumentIDAsEmpty={}", 
                   setDocCount, forceSingleSet, canSetDocTitleAsContentFileNameIfNull, setGwDocumentIDAsEmpty);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
        
        /**
         * Merge another result into this one (for multi-threaded aggregation)
         */
        public void merge(ProcessingResult other) {
            this.totalClaimNumbers += other.totalClaimNumbers;
            this.totalDocuments += other.totalDocuments;
            this.indexedSuccessCount += other.indexedSuccessCount;
            this.indexedFailureCount += other.indexedFailureCount;
            this.packagingFilesCreated += other.packagingFilesCreated;
            this.batchIDsCreated.addAll(other.batchIDsCreated);
        }
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Process each claim individually:
     *   * Query DB for documents of one claim
     *   * Apply batching/indexing logic
     *   * Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} =====", csvFileName);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return result;
            }
            
            // STEP 3: Process each claim individually
            // Use the CSV file name as the CC_Extract_file_Name filter
            int claimIndex = 0;
            for (String claimNumber : claimNumbers) {
                claimIndex++;
                logger.info("Processing claim {}/{}: {}", claimIndex, claimNumbers.size(), claimNumber);
                
                // Query DB for documents of this specific claim from the specific extract file
                // Filter: claimID NOT NULL, gwDocumentID NOT NULL, isDataMerged=1, contentFilePath NOT NULL
                List<ClaimCenterDocumentDTO> documents = fetchDocumentsForOneClaim(claimNumber, csvFileName);
                
                if (documents.isEmpty()) {
                    logger.warn("No documents found for claim: {} in file: {} (filter: claimID/gwDocumentID NOT NULL, isDataMerged=1, contentFilePath NOT NULL, isIndexed=0)", 
                               claimNumber, csvFileName);
                    continue;
                }
                
                logger.info("Retrieved {} documents for claim: {}", documents.size(), claimNumber);
                result.totalDocuments += documents.size();
                
                // Apply batching and indexing logic (one batch per claim or multiple based on config)
                applyBatchingLogicForClaim(documents, result, claimNumber);
                
                // Update database with indexing metadata
                updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                
                logger.info("Completed claim: {} - Batches created: {}", claimNumber, 
                           result.batchIDsCreated.size() - (claimIndex - 1));
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLACE_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    // ==================== NEW: PROCESS ALL PENDING DOCUMENTS (NO INPUT FILE) ====================
    
    /**
     * Process ALL pending documents from CC_Extract_Staging table.
     * No input CSV file required - queries directly from database.
     * 
     * Filter Conditions:
     *   - claimID IS NOT NULL (synced from ClaimInfo)
     *   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
     *   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
     *   - contentFilePath IS NOT NULL (content file must exist)
     *   - isIndexed = 0 (not yet indexed)
     * 
     * Processing Steps:
     *   1. Query all pending documents from CC_Extract_Staging
     *   2. Group by (CC_Extract_file_Name, claimNumber)
     *   3. Process each group: apply batching logic, update DB
     *   4. Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processAllPendingDocuments() {
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        logger.info("===== PROCESSING ALL PENDING DOCUMENTS (No Input File Mode) =====");
        
        try {
            // STEP 1: Fetch all pending documents from CC_Extract_Staging
            logger.info("STEP 1: Fetching all pending documents from CC_Extract_Staging...");
            List<ClaimCenterDocumentDTO> allPendingDocs = fetchAllPendingDocuments();
            
            if (allPendingDocs.isEmpty()) {
                logger.info("No pending documents found. Nothing to process.");
                return result;
            }
            
            logger.info("Found {} pending documents to process", allPendingDocs.size());
            result.totalDocuments = allPendingDocs.size();
            
            // STEP 2: Group documents by (CC_Extract_file_Name, claimNumber)
            logger.info("STEP 2: Grouping documents by (CC_Extract_file_Name, claimNumber)...");
            Map<String, Map<String, List<ClaimCenterDocumentDTO>>> groupedDocs = groupDocumentsByFileAndClaim(allPendingDocs);
            
            int totalFiles = groupedDocs.size();
            int totalClaims = groupedDocs.values().stream().mapToInt(Map::size).sum();
            logger.info("Grouped into {} extract files, {} claims", totalFiles, totalClaims);
            result.totalClaimNumbers = totalClaims;
            
            // STEP 3: Process each group
            logger.info("STEP 3: Processing each claim group...");
            int fileIndex = 0;
            for (Map.Entry<String, Map<String, List<ClaimCenterDocumentDTO>>> fileEntry : groupedDocs.entrySet()) {
                fileIndex++;
                String extractFileName = fileEntry.getKey();
                Map<String, List<ClaimCenterDocumentDTO>> claimGroups = fileEntry.getValue();
                
                logger.info("[File {}/{}] Processing {} claims from: {}", 
                           fileIndex, totalFiles, claimGroups.size(), extractFileName);
                
                int claimIndex = 0;
                for (Map.Entry<String, List<ClaimCenterDocumentDTO>> claimEntry : claimGroups.entrySet()) {
                    claimIndex++;
                    String claimNumber = claimEntry.getKey();
                    List<ClaimCenterDocumentDTO> documents = claimEntry.getValue();
                    
                    logger.debug("[File {}/{}][Claim {}/{}] Processing claim: {} ({} docs)", 
                               fileIndex, totalFiles, claimIndex, claimGroups.size(), 
                               claimNumber, documents.size());
                    
                    // Apply batching logic to this claim's documents
                    applyBatchingLogicForClaim(documents, result, claimNumber);
                    
                    // Update database with indexing metadata
                    updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                }
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 4: Generate packaging CSV files
            logger.info("STEP 4: Generating packaging CSV files...");
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Write summary tracking file
            writeAllPendingTrackingFile(successLines, failedLines);
            
            logger.info("===== PROCESSING COMPLETE =====");
            logger.info("Summary: Files={}, Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       totalFiles, result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing all pending documents", e);
            return result;
        }
    }
    
    /**
     * Fetch all pending documents from CC_Extract_Staging
     */
    private List<ClaimCenterDocumentDTO> fetchAllPendingDocuments() throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectAllPendingDocumentsQuery);
             ResultSet rs = pstmt.executeQuery()) {
            
            while (rs.next()) {
                ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                
                // Core fields from CC_Extract_Staging
                dto.setExternalID(rs.getString("externalID"));
                dto.setClaimNumber(rs.getString("claimNumber"));
                dto.setClaimID(rs.getString("claimID"));
                dto.setGwDocumentID(rs.getString("gwDocumentID"));
                dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                
                // Document metadata
                dto.setDocumentTitle(rs.getString("documentTitle"));
                dto.setDocumentType(rs.getString("documentType"));
                dto.setDocumentSubtype(rs.getString("documentSubtype"));
                dto.setAuthor(rs.getString("author"));
                dto.setDocumentDescription(rs.getString("documentDescription"));
                dto.setPolicyNumber(rs.getString("policyNumber"));
                dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                dto.setInputMethod(rs.getString("inputMethod"));
                dto.setMimeType(rs.getString("mimeType"));
                dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                dto.setContentFilePath(rs.getString("contentFilePath"));
                dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                dto.setContentType(rs.getString("contentType"));
                
                // Columns NOT in CC_Extract_Staging - set to empty
                dto.setAmount("");
                dto.setClaimant("");
                dto.setCoverage("");
                dto.setCustomerID("");
                dto.setDuplicate("");
                dto.setExposureID("");
                dto.setHidden("");
                dto.setInsuredName("");
                dto.setPrimaryMembershipNumber("");
                dto.setReviewed("");
                
                documents.add(dto);
            }
        }
        
        return documents;
    }
    
    /**
     * Group documents by (CC_Extract_file_Name, claimNumber)
     * Returns: Map<CC_Extract_file_Name, Map<claimNumber, List<Documents>>>
     */
    private Map<String, Map<String, List<ClaimCenterDocumentDTO>>> groupDocumentsByFileAndClaim(
            List<ClaimCenterDocumentDTO> documents) {
        
        Map<String, Map<String, List<ClaimCenterDocumentDTO>>> grouped = new LinkedHashMap<>();
        
        for (ClaimCenterDocumentDTO doc : documents) {
            String fileName = doc.getCcExtractFileName();
            String claimNumber = doc.getClaimNumber();
            
            // Get or create file map
            Map<String, List<ClaimCenterDocumentDTO>> fileMap = 
                grouped.computeIfAbsent(fileName, k -> new LinkedHashMap<>());
            
            // Get or create claim list
            List<ClaimCenterDocumentDTO> claimList = 
                fileMap.computeIfAbsent(claimNumber, k -> new ArrayList<>());
            
            claimList.add(doc);
        }
        
        return grouped;
    }
    
    /**
     * Write tracking file for all pending documents processing
     */
    private void writeAllPendingTrackingFile(List<String> successLines, List<String> failedLines) {
        try {
            String timestamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
            Path trackingDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.COMPLETED_FOLDER);
            
            // Write success file
            if (!successLines.isEmpty()) {
                String successFileName = "AllPending_Success_" + timestamp + ".txt";
                Path successFile = trackingDir.resolve(successFileName);
                try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                    writer.write("# All Pending Documents Processing - Success Report");
                    writer.newLine();
                    writer.write("# Generated: " + new Date());
                    writer.newLine();
                    writer.write("# Total: " + successLines.size() + " documents");
                    writer.newLine();
                    writer.newLine();
                    for (String line : successLines) {
                        writer.write(line);
                        writer.newLine();
                    }
                }
                logger.info("Written success tracking file: {}", successFileName);
            }
            
            // Write failed file
            if (!failedLines.isEmpty()) {
                String failedFileName = "AllPending_Failed_" + timestamp + ".txt";
                Path failedFile = trackingDir.resolve(failedFileName);
                try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                    writer.write("# All Pending Documents Processing - Failed Report");
                    writer.newLine();
                    writer.write("# Generated: " + new Date());
                    writer.newLine();
                    writer.write("# Total: " + failedLines.size() + " documents");
                    writer.newLine();
                    writer.newLine();
                    for (String line : failedLines) {
                        writer.write(line);
                        writer.newLine();
                    }
                }
                logger.info("Written failed tracking file: {}", failedFileName);
            }
            
        } catch (IOException e) {
            logger.error("Error writing tracking files", e);
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive, supports both camelCase and UNDERSCORE formats)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replace("\"", "");
                    String normalizedHeader = header.toUpperCase().replace("_", "");
                    
                    // Support both "claimNumber" and "CLAIM_NUMBER" formats
                    if (normalizedHeader.equals("CLAIMNUMBER")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' or 'CLAIM_NUMBER' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    logger.error("Supported formats: claimNumber, ClaimNumber, CLAIM_NUMBER, claim_number");
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents of a single claim from CC_Extract_Staging table.
     * 
     * Filter conditions:
     *   - claimNumber matches
     *   - CC_Extract_file_Name matches (to filter by specific extract file)
     *   - claimID IS NOT NULL (synced from ClaimInfo)
     *   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
     *   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
     *   - contentFilePath IS NOT NULL (content file must exist)
     *   - isIndexed = 0 (not yet indexed)
     * 
     * Column mapping from CC_Extract_Staging:
     *   - Name -> documentTitle
     *   - Missing columns (amount, claimant, etc.) are set to empty strings
     * 
     * @param claimNumber The claim number to fetch documents for
     * @param ccExtractFileName The CC_Extract_file_Name to filter by
     */
    private List<ClaimCenterDocumentDTO> fetchDocumentsForOneClaim(String claimNumber, String ccExtractFileName) throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectDocumentsByClaimQuery)) {
            
            pstmt.setString(1, claimNumber);
            pstmt.setString(2, ccExtractFileName);
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields from CC_Extract_Staging
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                    dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                    
                    // Document metadata from CC_Extract_Staging
                    // Note: Name column is aliased as documentTitle in SQL query
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    dto.setContentType(rs.getString("contentType"));
                    
                    // Columns NOT in CC_Extract_Staging - set to empty for packaging output
                    dto.setAmount("");
                    dto.setClaimant("");
                    dto.setCoverage("");
                    dto.setCustomerID("");
                    dto.setDuplicate("");
                    dto.setExposureID("");
                    dto.setHidden("");
                    dto.setInsuredName("");
                    dto.setPrimaryMembershipNumber("");
                    dto.setReviewed("");
                    
                    documents.add(dto);
                }
            }
        }
        
        logger.debug("Fetched {} documents for claimNumber: {} from file: {}", 
                    documents.size(), claimNumber, ccExtractFileName);
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents for a single claim
     * 
     * Case 1: forceSingleSet = true
     *   - Create multiple batches, each with 1 set (max setDocCount docs per batch)
     *   - Example: 27 docs → Batch1(25 docs, 1 set) + Batch2(2 docs, 1 set)
     * 
     * Case 2: forceSingleSet = false
     *   - Create one batch per claim with multiple sets (max setDocCount docs per set)
     *   - Example: 27 docs → Batch1(27 docs, Set1: 25 docs + Set2: 2 docs)
     * 
     * @param documents List of documents for this claim
     * @param result ProcessingResult to track batch IDs
     * @param claimNumber Current claim number being processed
     */
    private void applyBatchingLogicForClaim(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        logger.debug("Applying batching logic for claim: {} with {} documents (forceSingleSet={})", 
                    claimNumber, totalDocs, forceSingleSet);
        
        if (forceSingleSet) {
            // CASE 1: Create multiple batches, each with 1 set (max setDocCount docs per batch)
            applyForceSingleSetLogic(documents, result, claimNumber);
        } else {
            // CASE 2: Create one batch per claim with multiple sets
            applyMultiSetPerBatchLogic(documents, result, claimNumber);
        }
    }
    
    /**
     * Case 1: Force single set per batch
     * - Create multiple batches for the claim
     * - Each batch contains exactly 1 set (max setDocCount docs)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 25 docs (setID=1, setDocCount=25, batchDocCount=25)
     *   Batch 2: 2 docs (setID=1, setDocCount=2, batchDocCount=2)
     */
    private void applyForceSingleSetLogic(List<ClaimCenterDocumentDTO> documents, 
                                          ProcessingResult result, 
                                          String claimNumber) {
        
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        int batchNumber = 1;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size (max setDocCount)
            int currentBatchSize = Math.min(setDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID();
            result.batchIDsCreated.add(batchID);
            
            logger.info("Claim: {} - Batch {}: batchID={}, docs={}, sets=1", 
                       claimNumber, batchNumber, batchID, currentBatchSize);
            
            // Process documents in current batch (single set)
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata (always setID = 1 for this case)
                doc.setSetID(1);
                doc.setSetDocCount(currentBatchSize);
                doc.setSetDocIndex(setDocIndex);
                
                setDocIndex++;
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
            batchNumber++;
        }
        
        logger.info("Claim: {} - Created {} batches (forceSingleSet=true)", claimNumber, batchNumber - 1);
    }
    
    /**
     * Case 2: Multiple sets per batch
     * - Create one batch for the claim
     * - Batch contains multiple sets (max setDocCount docs per set)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 27 docs (batchDocCount=27)
     *     - Set 1: setID=1, setDocCount=25, setDocIndex=1-25
     *     - Set 2: setID=2, setDocCount=2, setDocIndex=1-2
     */
    private void applyMultiSetPerBatchLogic(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        
        // Generate unique batch ID and job ID (one per claim)
        String batchID = generateBatchID();
        String jobID = generateJobID();
        result.batchIDsCreated.add(batchID);
        
        // Calculate number of sets needed
        int numberOfSets = (int) Math.ceil((double) totalDocs / setDocCount);
        
        logger.info("Claim: {} - Batch: batchID={}, docs={}, sets={}", 
                   claimNumber, batchID, totalDocs, numberOfSets);
        
        // Process all documents in this single batch
        int setID = 1;
        int setDocIndex = 1;
        int docsInCurrentSet = 0;
        
        for (ClaimCenterDocumentDTO doc : documents) {
            docsInCurrentSet++;
            
            // Calculate set doc count for current set
            int remainingDocs = totalDocs - ((setID - 1) * setDocCount);
            int currentSetDocCount = Math.min(setDocCount, remainingDocs);
            
            // Set batch-level metadata
            doc.setBatchID(batchID);
            doc.setJobID(jobID);
            doc.setBatchDocCount(totalDocs);
            
            // Set set-level metadata
            doc.setSetID(setID);
            doc.setSetDocCount(currentSetDocCount);
            doc.setSetDocIndex(setDocIndex);
            
            // Increment set document index
            setDocIndex++;
            
            // Move to next set if current set is full
            if (setDocIndex > setDocCount) {
                logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
                setID++;
                setDocIndex = 1;
                docsInCurrentSet = 0;
            }
        }
        
        // Log last set if it has documents
        if (docsInCurrentSet > 0) {
            logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
        }
        
        logger.info("Claim: {} - Created 1 batch with {} sets (forceSingleSet=false)", claimNumber, numberOfSets);
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID() {
        return Constants.JOB_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata using JDBC batch processing
     * Falls back to individual UPDATEs if batch fails
     */
    private void updateDatabaseWithIndexing(List<ClaimCenterDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        // Get batch size from config (default 1000)
        String batchSizeStr = config.getProperty("db.indexing.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        try {
            // Try batch UPDATE first (fast path)
            updateDatabaseWithIndexingBatch(documents, successLines, failedLines, result, batchSize);
            
        } catch (SQLException e) {
            logger.warn("Batch UPDATE failed, falling back to individual UPDATEs: {}", e.getMessage());
            // Fallback to individual UPDATEs
            updateDatabaseWithIndexingIndividual(documents, successLines, failedLines, result);
        }
    }
    
    /**
     * Batch UPDATE implementation (100x faster than individual UPDATEs)
     */
    private void updateDatabaseWithIndexingBatch(List<ClaimCenterDocumentDTO> documents,
                                                 List<String> successLines,
                                                 List<String> failedLines,
                                                 ProcessingResult result,
                                                 int batchSize) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            int count = 0;
            int batchStartIndex = 0;
            
            for (int i = 0; i < documents.size(); i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                pstmt.setInt(1, doc.getBatchDocCount());
                pstmt.setString(2, doc.getBatchID());
                pstmt.setString(3, doc.getJobID());
                pstmt.setInt(4, doc.getSetDocCount());
                pstmt.setInt(5, doc.getSetID());
                pstmt.setInt(6, doc.getSetDocIndex());
                pstmt.setString(7, doc.getExternalID());
                pstmt.addBatch();
                count++;
                
                // Execute batch when size reached
                if (count % batchSize == 0) {
                    int[] results = pstmt.executeBatch();
                    conn.commit();
                    
                    // Track results
                    for (int j = 0; j < results.length; j++) {
                        ClaimCenterDocumentDTO batchDoc = documents.get(batchStartIndex + j);
                        if (results[j] > 0) {
                            result.indexedSuccessCount++;
                            successLines.add(batchDoc.getExternalID());
                        } else {
                            result.indexedFailureCount++;
                            failedLines.add(batchDoc.getExternalID() + "|No record found");
                        }
                    }
                    
                    logger.debug("Executed batch of {} indexing UPDATEs", count);
                    batchStartIndex = i + 1;
                }
            }
            
            // Execute remaining batch
            if (count % batchSize != 0) {
                int[] results = pstmt.executeBatch();
                conn.commit();
                
                // Track results for remaining batch
                int remainingStart = (count / batchSize) * batchSize;
                for (int j = 0; j < results.length; j++) {
                    ClaimCenterDocumentDTO batchDoc = documents.get(remainingStart + j);
                    if (results[j] > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(batchDoc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(batchDoc.getExternalID() + "|No record found");
                    }
                }
                
                logger.debug("Executed final batch of {} indexing UPDATEs", count % batchSize);
            }
            
            logger.info("Batch indexing UPDATE completed: {} documents indexed, {} failed", 
                       result.indexedSuccessCount, result.indexedFailureCount);
                       
        } catch (SQLException e) {
            logger.error("Batch indexing UPDATE failed", e);
            throw e; // Re-throw to trigger fallback
        }
    }
    
    /**
     * Individual UPDATE fallback (for when batch fails)
     */
    private void updateDatabaseWithIndexingIndividual(List<ClaimCenterDocumentDTO> documents,
                                                      List<String> successLines,
                                                      List<String> failedLines,
                                                      ProcessingResult result) {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (fallback mode)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            
            for (ClaimCenterDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Individual indexing UPDATE completed (fallback)");
            
        } catch (SQLException e) {
            logger.error("Individual UPDATE fallback also failed", e);
            // Connection auto-closes and rolls back due to auto-commit=false
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replace(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        // Replace the %IN_CLAUSE% placeholder with the generated IN clause
        String selectSQL = selectPackagingByBatchIdsQueryTemplate.replace("%IN_CLAUSE%", inClause.toString());
        
        List<ClaimCenterDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields from CC_Extract_Staging
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                    dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                    
                    // Batch/Set metadata
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    
                    // Document metadata from CC_Extract_Staging
                    // Note: Name column is aliased as documentTitle in SQL query
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    dto.setContentType(rs.getString("contentType"));
                    
                    // Columns NOT in CC_Extract_Staging - set to empty for packaging output
                    dto.setAmount("");
                    dto.setClaimant("");
                    dto.setCoverage("");
                    dto.setCustomerID("");
                    dto.setDuplicate("");
                    dto.setExposureID("");
                    dto.setHidden("");
                    dto.setInsuredName("");
                    dto.setPrimaryMembershipNumber("");
                    dto.setReviewed("");
                    
                    // If config is set to true, clear gwDocumentID (set to empty)
                    if (setGwDocumentIDAsEmpty) {
                        dto.setGwDocumentID("");
                        logger.debug("Set gwDocumentID to empty for externalID={} (config: setgwDocumentIDAsEmpty=true)", 
                                    dto.getExternalID());
                    }
                    
                    // If documentTitle is null/empty and config allows, set it from contentRetrievalName (without extension)
                    if (canSetDocTitleAsContentFileNameIfNull && 
                        (dto.getDocumentTitle() == null || dto.getDocumentTitle().trim().isEmpty())) {
                        String contentFileName = dto.getContentRetrievalName();
                        if (contentFileName != null && !contentFileName.trim().isEmpty()) {
                            // Remove file extension to get the filename without extension
                            String fileNameWithoutExtension = removeFileExtension(contentFileName);
                            dto.setDocumentTitle(fileNameWithoutExtension);
                            logger.debug("Set documentTitle from contentRetrievalName for externalID={}: '{}'", 
                                        dto.getExternalID(), fileNameWithoutExtension);
                        }
                    }
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<ClaimCenterDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (ClaimCenterDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(ClaimCenterDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<ClaimCenterDocumentDTO> documents) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateProcessedQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (updateProcessedFlag)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            Date currentDate = new Date();
            
            for (ClaimCenterDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLACE_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replace(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
    
    /**
     * Remove file extension from filename
     * Example: "document.pdf" -> "document"
     *          "file.name.txt" -> "file.name"
     *          "noextension" -> "noextension"
     */
    private String removeFileExtension(String fileName) {
        if (fileName == null || fileName.isEmpty()) {
            return fileName;
        }
        
        int lastDotIndex = fileName.lastIndexOf('.');
        if (lastDotIndex > 0) {
            return fileName.substring(0, lastDotIndex);
        }
        
        // No extension found, return as-is
        return fileName;
    }
}



# ===========================================================
# CCDataIndexAndPackagingUtility - Production Configuration
# ===========================================================
# Last Updated: 2025-12-19
# Updated to use CC_Extract_Staging table instead of Wawa_Doc_Migration_Transit_Data
# Optimized for high-volume production processing (2M+ documents/day)
# ===========================================================

# ===== APPLICATION SETTINGS =====
app.base_path=D:/Rameshwar/ClaimCenterDataMerge/
app.skipHeaderRow=true
app.archive.after.processing=true

# ===== BATCHING CONFIGURATION =====
# Set document count: Maximum documents per set
app.set.doc.count=25

# Force single set per batch
# true = Multiple batches per claim (1 set per batch)
# false = One batch per claim (multiple sets per batch)
app.batch.force.single.set=false

# ===== PERFORMANCE TUNING =====
# Thread Pool Size: Number of parallel CSV file processors
# Recommended: 5-10 for production (multiple CSV files)
# Note: Each thread processes one complete CSV file
app.thread.pool.size=5

# Progress Monitoring: Log progress every N minutes
# Set to 0 to disable progress monitoring
# Recommended: 5 minutes for production visibility
app.progress.log.interval.minutes=5

# ===== DOCUMENT TITLE CONFIGURATION =====
# If documentTitle (Name) is null/empty, set it from contentRetrievalName (without extension)
app.canSetDocTitleAsContentFileNameIfNULL=false

# Set gwDocumentID as empty during packaging (if needed)
app.setgwDocumentIDAsEmpty=false

# ===== DATABASE SETTINGS =====
db.serverName=RAMESHWAR\\SQLEXPRESS
db.port=1433
db.dbName=Wawa_DMS_Conversion_UAT

# Database Connection Pool Size
# IMPORTANT: Should be >= app.thread.pool.size + 5 (buffer for overhead)
# Recommended: 15-20 for production with 5 threads
db.pool.size=15

# Target table for indexing and packaging operations
# CHANGED: Now using CC_Extract_Staging instead of Wawa_Doc_Migration_Transit_Data
db.staging.claimcenterdbtable=Wawa_DMS_Conversion_UAT.[dbo].[CC_Extract_Staging]

# Batch UPDATE Size for Indexing: Number of rows per JDBC batch
# CRITICAL FOR PERFORMANCE: Batch UPDATEs are 100x faster than individual UPDATEs
# Recommended: 1000-2000 rows per batch
# Default: 1000
db.indexing.batch.update.size=1000

# ===== MEMORY MANAGEMENT =====
# Memory warning threshold (percentage of max heap)
# Warning logged when memory usage exceeds this threshold
app.memory.warning.threshold=80

# ===== NOTES =====
# Performance Estimates for 80K claims, 2M documents:
# - With batch UPDATEs (1000/batch): ~40-50 minutes total
# - Thread pool of 5: Processes 5 CSV files in parallel
# - Connection pool of 15: Adequate for 5 threads + overhead
#
# JVM Recommended Settings:
# -Xms2G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200
#
# Global Processing Report:
# - Written to: {app.base_path}/CCDataIndexPackaging_Summary_Report_{timestamp}.csv
# - Contains: Total CSV files, claims, documents, batches, packaging files
# - Includes: Success/failure counts, processing time, throughput
#
# Packaging Files:
# - Written to: {app.base_path}/Packaging/wawa_migration_docs_{timestamp}.csv
# - Max rows per file: 50,000
# - Format: Pipe-delimited, 32 columns
#
# Table Change:
# - OLD: Wawa_Doc_Migration_Transit_Data
# - NEW: CC_Extract_Staging
# - Column Mapping: documentTitle -> Name
# - Missing columns in packaging output: amount, claimant, coverage, customerID, 
#   duplicate, exposureID, hidden, insuredName, primaryMembershipNumber, reviewed
#   (These will be output as empty values)

# ===========================================================================
# DATABASE QUERIES - Updated for CC_Extract_Staging table
# ===========================================================================

# ===========================================================================
# NEW: Query for fetching ALL pending documents (no input file required)
# Groups by CC_Extract_file_Name, claimNumber for processing
# Filter conditions:
#   - claimID IS NOT NULL (synced from ClaimInfo)
#   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
#   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
#   - contentFilePath IS NOT NULL (content file must exist)
#   - isIndexed = 0 (not yet indexed)
# ===========================================================================
db.query.select.all.pending.documents=SELECT \
    externalID, \
    claimNumber, \
    CAST(claimID AS VARCHAR(50)) AS claimID, \
    gwDocumentID, \
    gwDocExternalID, \
    Name AS documentTitle, \
    documentType, \
    documentSubtype, \
    author, \
    documentDescription, \
    policyNumber, \
    doNotCreateActivity, \
    inputMethod, \
    mimeType, \
    OrigDateCreated, \
    sensitive, \
    contentFilePath, \
    contentRetrievalName, \
    contentType, \
    CC_Extract_file_Name \
FROM %TABLE_NAME% \
WHERE claimID IS NOT NULL \
    AND gwDocumentID IS NOT NULL \
    AND isDataMerged = 1 \
    AND contentFilePath IS NOT NULL \
    AND isIndexed = 0 \
ORDER BY CC_Extract_file_Name, claimNumber, externalID

# SELECT query for fetching documents for one claim
# Filter conditions:
#   - claimNumber matches
#   - CC_Extract_file_Name matches (to filter by specific extract file)
#   - claimID IS NOT NULL (synced from ClaimInfo)
#   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
#   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
#   - contentFilePath IS NOT NULL (content file must exist)
#
# Column mapping from CC_Extract_Staging:
#   - Name -> documentTitle (for packaging output)
#   - Missing columns will be empty: amount, claimant, coverage, customerID,
#     duplicate, exposureID, hidden, insuredName, primaryMembershipNumber, reviewed
db.query.select.documents.by.claim=SELECT \
    externalID, \
    claimNumber, \
    CAST(claimID AS VARCHAR(50)) AS claimID, \
    gwDocumentID, \
    gwDocExternalID, \
    Name AS documentTitle, \
    documentType, \
    documentSubtype, \
    author, \
    documentDescription, \
    policyNumber, \
    doNotCreateActivity, \
    inputMethod, \
    mimeType, \
    OrigDateCreated, \
    sensitive, \
    contentFilePath, \
    contentRetrievalName, \
    contentType, \
    CC_Extract_file_Name \
FROM %TABLE_NAME% \
WHERE claimNumber = ? \
    AND CC_Extract_file_Name = ? \
    AND claimID IS NOT NULL \
    AND gwDocumentID IS NOT NULL \
    AND isDataMerged = 1 \
    AND contentFilePath IS NOT NULL \
    AND isIndexed = 0 \
ORDER BY externalID

# UPDATE query for batch indexing (setting batch/set metadata)
# Sets: batchDocCount, batchID, jobID, setDocCount, setID, SetDocIndex, isIndexed=1
db.query.update.indexing=UPDATE %TABLE_NAME% SET \
    batchDocCount = ?, \
    batchID = ?, \
    jobID = ?, \
    setDocCount = ?, \
    setID = ?, \
    SetDocIndex = ?, \
    isIndexed = 1 \
WHERE externalID = ?

# SELECT query for fetching packaging documents by batchID list
# Returns all columns needed for packaging CSV output
# Missing columns from CC_Extract_Staging will be handled in Java as empty values
db.query.select.packaging.by.batchids=SELECT \
    externalID, \
    claimNumber, \
    CAST(claimID AS VARCHAR(50)) AS claimID, \
    gwDocumentID, \
    gwDocExternalID, \
    Name AS documentTitle, \
    documentType, \
    documentSubtype, \
    author, \
    documentDescription, \
    policyNumber, \
    doNotCreateActivity, \
    inputMethod, \
    mimeType, \
    OrigDateCreated, \
    sensitive, \
    contentFilePath, \
    contentRetrievalName, \
    contentType, \
    batchDocCount, \
    batchID, \
    jobID, \
    setDocCount, \
    setID, \
    SetDocIndex, \
    CC_Extract_file_Name \
FROM %TABLE_NAME% \
WHERE batchID IN (%IN_CLAUSE%) \
    AND isDataMerged = 1 \
    AND isIndexed = 1 \
ORDER BY batchID, setID, SetDocIndex

# UPDATE query for marking documents as processed
# Sets: isProcessed=1, DateProcessed=current timestamp
db.query.update.processed=UPDATE %TABLE_NAME% SET \
    isProcessed = 1, \
    DateProcessed = ? \
WHERE externalID = ?

# ===========================================================================
# END OF CONFIGURATION
# ===========================================================================





-- ============================================================================
-- CCDataMergeUtility - guidewireIDMapping Table Creation Script
-- Purpose: Mapping table for syncing gwDocumentID to CC_Extract_Staging
-- 
-- Data Flow:
--   Step 2: CCDataMergeUtility loads CC Extract CSV into CC_Extract_Staging
--   Step 3: SP syncs claimID from ClaimInfo (join on claimNumber)
--   Step 4: THIS TABLE - SP syncs gwDocumentID (join on externalID = gwDocExternalID)
--   Step 5: SP syncs content columns from Wawa_Doc_Migration_Transit_Data
--   Step 6: CCDataIndexAndPackagingUtility creates batch packages
--   Step 7: WawaDocumentMigrationUtility injects documents
-- 
-- Table Columns:
--   - ID: The gwDocumentID value (varchar) - will be synced to CC_Extract_Staging.gwDocumentID
--   - externalID: The join key - matches CC_Extract_Staging.gwDocExternalID
-- 
-- Join Logic:
--   CC_Extract_Staging.gwDocExternalID = guidewireIDMapping.externalID
--   → CC_Extract_Staging.gwDocumentID = guidewireIDMapping.ID
-- ============================================================================

USE [Wawa_DMS_Conversion_UAT];
GO

SET ANSI_NULLS ON;
GO

SET QUOTED_IDENTIFIER ON;
GO

-- ============================================================================
-- Create guidewireIDMapping Table (if not exists)
-- ============================================================================
IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[guidewireIDMapping]') AND type in (N'U'))
BEGIN
    CREATE TABLE [dbo].[guidewireIDMapping](
        [ID] [varchar](50) NOT NULL,           -- gwDocumentID value to sync
        [externalID] [varchar](200) NOT NULL,  -- Join key (matches gwDocExternalID)
    CONSTRAINT [PK_guidewireIDMapping] PRIMARY KEY CLUSTERED 
    (
        [externalID] ASC  -- Primary key on externalID for unique constraint
    )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]
    ) ON [PRIMARY];
    
    PRINT 'Table [dbo].[guidewireIDMapping] created successfully.';
END
ELSE
BEGIN
    PRINT 'Table [dbo].[guidewireIDMapping] already exists. Skipping creation.';
END
GO

-- ============================================================================
-- Create Index on ID column for better query performance
-- ============================================================================
IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name = 'IX_guidewireIDMapping_ID' AND object_id = OBJECT_ID('[dbo].[guidewireIDMapping]'))
BEGIN
    CREATE NONCLUSTERED INDEX [IX_guidewireIDMapping_ID]
    ON [dbo].[guidewireIDMapping] ([ID]);
    
    PRINT 'Index [IX_guidewireIDMapping_ID] created successfully.';
END
ELSE
BEGIN
    PRINT 'Index [IX_guidewireIDMapping_ID] already exists. Skipping creation.';
END
GO

-- ============================================================================
-- Summary
-- ============================================================================
PRINT '';
PRINT '============================================================';
PRINT 'guidewireIDMapping Table Setup Complete';
PRINT '';
PRINT 'Table Structure:';
PRINT '  - ID (VARCHAR(50)) - gwDocumentID value to sync';
PRINT '  - externalID (VARCHAR(200)) - Join key (PK, matches gwDocExternalID)';
PRINT '';
PRINT 'Usage:';
PRINT '  CC_Extract_Staging.gwDocExternalID = guidewireIDMapping.externalID';
PRINT '  → CC_Extract_Staging.gwDocumentID = guidewireIDMapping.ID';
PRINT '============================================================';
GO
















-- ============================================================================
-- CCDataMergeUtility - Step 4: Sync gwDocumentID Stored Procedure
-- Purpose: Sync gwDocumentID from guidewireIDMapping table to CC_Extract_Staging 
--          using gwDocExternalID as join key
-- 
-- Data Flow:
--   Step 2: CCDataMergeUtility loads CC Extract CSV into CC_Extract_Staging
--   Step 3: SP syncs claimID from ClaimInfo (join on claimNumber)
--   Step 4: THIS SP - syncs gwDocumentID from guidewireIDMapping (join on externalID)
--   Step 5: SP syncs content columns from Wawa_Doc_Migration_Transit_Data
--   Step 6: CCDataIndexAndPackagingUtility creates batch packages
--   Step 7: WawaDocumentMigrationUtility injects documents
-- 
-- Source Table: guidewireIDMapping
--   - ID (varchar 50) = gwDocumentID value
--   - externalID (varchar 200) = join key
-- 
-- Target Table: CC_Extract_Staging
--   - gwDocumentID (varchar 50) = populated from guidewireIDMapping.ID
--   - gwDocExternalID (varchar 200) = join key
-- 
-- Join Logic:
--   CC_Extract_Staging.gwDocExternalID = guidewireIDMapping.externalID
-- 
-- Note: This SP runs through ALL records each time it's executed
-- ============================================================================

USE [Wawa_DMS_Conversion_UAT];
GO

-- Drop existing procedure if exists
IF OBJECT_ID('dbo.sp_SyncGwDocumentIDToStaging', 'P') IS NOT NULL
    DROP PROCEDURE [dbo].[sp_SyncGwDocumentIDToStaging];
GO

CREATE PROCEDURE [dbo].[sp_SyncGwDocumentIDToStaging]
    @CC_Extract_file_Name VARCHAR(200) = NULL,  -- Optional: filter by specific file (NULL = all records)
    @RowsProcessed INT OUTPUT,
    @RowsSynced INT OUTPUT,
    @RowsNotMatched INT OUTPUT
AS
BEGIN
    SET NOCOUNT ON;
    
    DECLARE @StartTime DATETIME2 = SYSDATETIME();
    DECLARE @EndTime DATETIME2;
    DECLARE @ErrorMessage NVARCHAR(MAX);
    
    -- Initialize output parameters
    SET @RowsProcessed = 0;
    SET @RowsSynced = 0;
    SET @RowsNotMatched = 0;
    
    BEGIN TRY
        PRINT '============================================================';
        PRINT 'STEP 4: SYNC GWDOCUMENTID - STARTED';
        PRINT 'Start Time: ' + CONVERT(VARCHAR, @StartTime, 121);
        IF @CC_Extract_file_Name IS NOT NULL
            PRINT 'Filter: CC_Extract_file_Name = ' + @CC_Extract_file_Name;
        ELSE
            PRINT 'Filter: ALL records in CC_Extract_Staging';
        PRINT '============================================================';
        
        -- Count total rows to process (where gwDocumentID is NULL)
        SELECT @RowsProcessed = COUNT(*)
        FROM [dbo].[CC_Extract_Staging] stg
        WHERE stg.[gwDocumentID] IS NULL
          AND (@CC_Extract_file_Name IS NULL OR stg.[CC_Extract_file_Name] = @CC_Extract_file_Name);
        
        PRINT '[INFO] Total rows to process (gwDocumentID IS NULL): ' + CAST(@RowsProcessed AS VARCHAR);
        
        IF @RowsProcessed = 0
        BEGIN
            PRINT '[INFO] No rows with NULL gwDocumentID found. Exiting.';
            RETURN;
        END
        
        BEGIN TRANSACTION;
        
        -- ================================================================
        -- STEP 1: Update CC_Extract_Staging.gwDocumentID from guidewireIDMapping.ID
        --         where gwDocExternalID matches externalID
        -- ================================================================
        UPDATE stg
        SET 
            stg.[gwDocumentID] = gw.[ID]
        FROM [dbo].[CC_Extract_Staging] stg
        INNER JOIN [dbo].[guidewireIDMapping] gw
            ON stg.[gwDocExternalID] = gw.[externalID]
        WHERE stg.[gwDocumentID] IS NULL  -- Only update if not already set
          AND (@CC_Extract_file_Name IS NULL OR stg.[CC_Extract_file_Name] = @CC_Extract_file_Name);
        
        SET @RowsSynced = @@ROWCOUNT;
        
        PRINT '[STEP 1] Synced gwDocumentID for ' + CAST(@RowsSynced AS VARCHAR) + ' rows.';
        
        -- Calculate rows not matched
        SET @RowsNotMatched = @RowsProcessed - @RowsSynced;
        
        IF @RowsNotMatched > 0
        BEGIN
            PRINT '[WARNING] ' + CAST(@RowsNotMatched AS VARCHAR) + ' rows could not be matched.';
            PRINT '          These rows have gwDocExternalID values not found in guidewireIDMapping.';
        END
        
        COMMIT TRANSACTION;
        
        SET @EndTime = SYSDATETIME();
        
        -- ================================================================
        -- Summary Report
        -- ================================================================
        PRINT '';
        PRINT '============================================================';
        PRINT 'STEP 4: SYNC GWDOCUMENTID - COMPLETED';
        PRINT 'End Time: ' + CONVERT(VARCHAR, @EndTime, 121);
        PRINT 'Duration: ' + CAST(DATEDIFF(SECOND, @StartTime, @EndTime) AS VARCHAR) + ' seconds';
        PRINT '';
        PRINT 'Results:';
        PRINT '  Rows Processed (NULL gwDocumentID): ' + CAST(@RowsProcessed AS VARCHAR);
        PRINT '  Rows Synced (matched):              ' + CAST(@RowsSynced AS VARCHAR);
        PRINT '  Rows Not Matched:                   ' + CAST(@RowsNotMatched AS VARCHAR);
        PRINT '============================================================';
        
    END TRY
    BEGIN CATCH
        IF @@TRANCOUNT > 0
            ROLLBACK TRANSACTION;
        
        SET @ErrorMessage = ERROR_MESSAGE();
        PRINT '[ERROR] ' + @ErrorMessage;
        PRINT 'Error Number: ' + CAST(ERROR_NUMBER() AS VARCHAR);
        PRINT 'Error Line: ' + CAST(ERROR_LINE() AS VARCHAR);
        
        -- Re-throw the error
        THROW;
    END CATCH
END;
GO

-- ============================================================================
-- Grant Execute Permission (adjust as needed)
-- ============================================================================
-- GRANT EXECUTE ON [dbo].[sp_SyncGwDocumentIDToStaging] TO [YourAppUser];
-- GO

-- ============================================================================
-- Example Usage
-- ============================================================================
PRINT '';
PRINT '============================================================';
PRINT 'STORED PROCEDURE CREATED: sp_SyncGwDocumentIDToStaging';
PRINT '';
PRINT 'Usage Examples:';
PRINT '';
PRINT '-- Sync ALL records:';
PRINT 'DECLARE @Processed INT, @Synced INT, @NotMatched INT;';
PRINT 'EXEC sp_SyncGwDocumentIDToStaging';
PRINT '    @CC_Extract_file_Name = NULL,';
PRINT '    @RowsProcessed = @Processed OUTPUT,';
PRINT '    @RowsSynced = @Synced OUTPUT,';
PRINT '    @RowsNotMatched = @NotMatched OUTPUT;';
PRINT 'SELECT @Processed AS Processed, @Synced AS Synced, @NotMatched AS NotMatched;';
PRINT '';
PRINT '-- Sync specific file only:';
PRINT 'DECLARE @Processed INT, @Synced INT, @NotMatched INT;';
PRINT 'EXEC sp_SyncGwDocumentIDToStaging';
PRINT '    @CC_Extract_file_Name = ''CC_Extract_2025_01_15.csv'',';
PRINT '    @RowsProcessed = @Processed OUTPUT,';
PRINT '    @RowsSynced = @Synced OUTPUT,';
PRINT '    @RowsNotMatched = @NotMatched OUTPUT;';
PRINT 'SELECT @Processed AS Processed, @Synced AS Synced, @NotMatched AS NotMatched;';
PRINT '============================================================';
GO



