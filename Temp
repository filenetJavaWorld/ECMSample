package com.wawanesa.ace.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.model.ClaimCenterDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * 
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 * 
 * Data Source: CC_Extract_Staging table (changed from Wawa_Doc_Migration_Transit_Data)
 * 
 * Filter Conditions for Document Selection:
 * - claimID IS NOT NULL (synced from ClaimInfo - Step 3)
 * - gwDocumentID IS NOT NULL (synced from guidewireIDMapping - Step 4)
 * - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data - Step 5)
 * - contentFilePath IS NOT NULL
 * - isIndexed = 0 (not yet indexed)
 * - CC_Extract_file_Name matches the input CSV file
 * 
 * Column Mapping:
 * - CC_Extract_Staging.Name → documentTitle (for packaging output)
 * - Missing columns (amount, claimant, coverage, etc.) are set to empty strings
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int setDocCount;
    private boolean forceSingleSet;
    private boolean canSetDocTitleAsContentFileNameIfNull;
    private boolean setGwDocumentIDAsEmpty;
    private int claimThreadPoolSize;
    
    // SQL Queries loaded from properties
    private String selectDocumentsByClaimQuery;
    private String selectAllPendingDocumentsQuery;  // NEW: Query all pending documents
    private String updateIndexingQuery;
    private String selectPackagingByBatchIdsQueryTemplate;
    private String updateProcessedQuery;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load set configuration (with default)
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        // Load force single set configuration (with default)
        String forceSingleSetStr = config.getProperty("app.batch.force.single.set");
        this.forceSingleSet = (forceSingleSetStr != null) ? 
            Boolean.parseBoolean(forceSingleSetStr) : Constants.DEFAULT_FORCE_SINGLE_SET;
        
        // Load configuration for setting documentTitle from contentRetrievalName if null
        String canSetDocTitleStr = config.getProperty("app.canSetDocTitleAsContentFileNameIfNULL");
        this.canSetDocTitleAsContentFileNameIfNull = (canSetDocTitleStr != null) ? 
            Boolean.parseBoolean(canSetDocTitleStr) : false;
        
        // Load configuration for setting gwDocumentID as empty during packaging
        String setGwDocIdEmptyStr = config.getProperty("app.setgwDocumentIDAsEmpty");
        this.setGwDocumentIDAsEmpty = (setGwDocIdEmptyStr != null) ? 
            Boolean.parseBoolean(setGwDocIdEmptyStr) : false;
        
        // Load SQL queries from properties and replace table name placeholder
        this.selectDocumentsByClaimQuery = config.getProperty("db.query.select.documents.by.claim")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectAllPendingDocumentsQuery = config.getProperty("db.query.select.all.pending.documents")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateIndexingQuery = config.getProperty("db.query.update.indexing")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectPackagingByBatchIdsQueryTemplate = config.getProperty("db.query.select.packaging.by.batchids")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateProcessedQuery = config.getProperty("db.query.update.processed")
            .replace("%TABLE_NAME%", this.dbTableName);
        
        // Load claim processing thread pool size (default to 5, same as main thread pool)
        String claimThreadPoolSizeStr = config.getProperty("app.claim.thread.pool.size");
        this.claimThreadPoolSize = (claimThreadPoolSizeStr != null) ? 
            Integer.parseInt(claimThreadPoolSizeStr) : 5;
        
        logger.info("Utils initialized with setDocCount={}, forceSingleSet={}, canSetDocTitleAsContentFileNameIfNull={}, setGwDocumentIDAsEmpty={}, claimThreadPoolSize={}", 
                   setDocCount, forceSingleSet, canSetDocTitleAsContentFileNameIfNull, setGwDocumentIDAsEmpty, claimThreadPoolSize);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
        
        /**
         * Merge another result into this one (for multi-threaded aggregation)
         */
        public void merge(ProcessingResult other) {
            this.totalClaimNumbers += other.totalClaimNumbers;
            this.totalDocuments += other.totalDocuments;
            this.indexedSuccessCount += other.indexedSuccessCount;
            this.indexedFailureCount += other.indexedFailureCount;
            this.packagingFilesCreated += other.packagingFilesCreated;
            this.batchIDsCreated.addAll(other.batchIDsCreated);
        }
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Process each claim individually:
     *   * Query DB for documents of one claim
     *   * Apply batching/indexing logic
     *   * Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        return processCSVFile(csvFilePath, null);
    }
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder (with parallel processing support)
     * 
     * @param csvFilePath Path to CSV file
     * @param executorService ExecutorService for parallel claim processing (null for sequential processing)
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath, ExecutorService executorService) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} (Parallel: {}) =====", csvFileName, executorService != null);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = Collections.synchronizedList(new ArrayList<>());
        List<String> failedLines = Collections.synchronizedList(new ArrayList<>());
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return result;
            }
            
            // STEP 3: Process claims in parallel
            // Create a dedicated executor for claim processing within this CSV file
            // This avoids thread pool exhaustion when multiple CSV files are processed in parallel
            ExecutorService claimExecutor = Executors.newFixedThreadPool(claimThreadPoolSize);
            try {
                processClaimsInParallel(claimNumbers, csvFileName, result, successLines, failedLines, claimExecutor);
            } finally {
                // Shutdown claim executor gracefully
                // Timeout calculated based on number of claims: 2 minutes per claim, minimum 30 seconds, maximum 30 minutes
                claimExecutor.shutdown();
                try {
                    int totalClaims = claimNumbers.size();
                    long shutdownTimeoutSeconds = Math.min(Math.max(totalClaims * 120L, 30L), 1800L); // 30 sec to 30 min
                    
                    if (!claimExecutor.awaitTermination(shutdownTimeoutSeconds, TimeUnit.SECONDS)) {
                        logger.warn("Claim executor did not terminate within {} seconds ({} claims), forcing shutdown", 
                                   shutdownTimeoutSeconds, totalClaims);
                        List<Runnable> pendingTasks = claimExecutor.shutdownNow();
                        if (!pendingTasks.isEmpty()) {
                            logger.warn("Cancelled {} pending claim processing tasks", pendingTasks.size());
                        }
                        // Wait a bit more for cancellation to take effect
                        if (!claimExecutor.awaitTermination(30, TimeUnit.SECONDS)) {
                            logger.error("Claim executor did not terminate after forced shutdown");
                        }
                    }
                } catch (InterruptedException e) {
                    logger.warn("Interrupted while shutting down claim executor, forcing immediate shutdown", e);
                    claimExecutor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLACE_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    /**
     * Process claims sequentially (backward compatible)
     */
    private void processClaimsSequentially(Set<String> claimNumbers, String csvFileName,
                                          ProcessingResult result, List<String> successLines, List<String> failedLines) {
        int claimIndex = 0;
        for (String claimNumber : claimNumbers) {
            claimIndex++;
            logger.info("Processing claim {}/{}: {}", claimIndex, claimNumbers.size(), claimNumber);
            
            processSingleClaim(claimNumber, csvFileName, result, successLines, failedLines);
        }
    }
    
    /**
     * Process claims in parallel using ExecutorService
     */
    private void processClaimsInParallel(Set<String> claimNumbers, String csvFileName,
                                        ProcessingResult result, List<String> successLines, List<String> failedLines,
                                        ExecutorService executorService) {
        AtomicInteger completedClaims = new AtomicInteger(0);
        int totalClaims = claimNumbers.size();
        List<CompletableFuture<ProcessingResult>> futures = new ArrayList<>();
        
        for (String claimNumber : claimNumbers) {
            CompletableFuture<ProcessingResult> future = CompletableFuture.supplyAsync(() -> {
                ProcessingResult claimResult = new ProcessingResult();
                try {
                    processSingleClaim(claimNumber, csvFileName, claimResult, successLines, failedLines);
                    
                    int completed = completedClaims.incrementAndGet();
                    logger.debug("Completed claim {}/{}: {} (Thread: {})", 
                               completed, totalClaims, claimNumber, Thread.currentThread().getId());
                    
                    return claimResult;
                } catch (Exception e) {
                    logger.error("Error processing claim: {}", claimNumber, e);
                    completedClaims.incrementAndGet();
                    return claimResult; // Return result (may have partial data)
                }
            }, executorService);
            
            futures.add(future);
        }
        
        // Wait for all claims to complete with timeout (prevents hanging forever)
        // Timeout: 5 minutes per claim (conservative for database operations)
        // Minimum: 5 minutes, Maximum: 2 hours
        long timeoutMinutes = Math.min(Math.max(totalClaims * 5, 5), 120);
        CompletableFuture<Void> allFutures = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
        
        try {
            allFutures.get(timeoutMinutes, TimeUnit.MINUTES);
        } catch (java.util.concurrent.TimeoutException e) {
            logger.error("Timeout waiting for claims to complete after {} minutes. Total claims: {}", timeoutMinutes, totalClaims);
            // Cancel remaining futures
            for (CompletableFuture<ProcessingResult> future : futures) {
                if (!future.isDone()) {
                    future.cancel(true);
                }
            }
            throw new RuntimeException("Claim processing timeout after " + timeoutMinutes + " minutes", e);
        } catch (Exception e) {
            logger.error("Error waiting for claims to complete", e);
            throw new RuntimeException("Failed to wait for claim processing", e);
        }
        
        // Aggregate results from all claims (all futures are now done)
        for (CompletableFuture<ProcessingResult> future : futures) {
            try {
                // Since all futures are done (either completed or cancelled), get() will return immediately
                if (!future.isCancelled()) {
                    ProcessingResult claimResult = future.get(); // Safe: future is already done
                    result.merge(claimResult);
                }
            } catch (Exception e) {
                logger.warn("Error aggregating claim result (future may be cancelled)", e);
            }
        }
        
        logger.info("All {} claims processed in parallel", totalClaims);
    }
    
    /**
     * Process a single claim (thread-safe - each call uses its own ProcessingResult)
     */
    private void processSingleClaim(String claimNumber, String csvFileName,
                                   ProcessingResult claimResult, List<String> successLines, List<String> failedLines) {
        try {
            // Query DB for documents of this specific claim from the specific extract file
            List<ClaimCenterDocumentDTO> documents = fetchDocumentsForOneClaim(claimNumber, csvFileName);
            
            if (documents.isEmpty()) {
                logger.warn("No documents found for claim: {} in file: {} (filter: claimID/gwDocumentID NOT NULL, isDataMerged=1, contentFilePath NOT NULL, isIndexed=0)", 
                           claimNumber, csvFileName);
                return;
            }
            
            logger.debug("Retrieved {} documents for claim: {}", documents.size(), claimNumber);
            claimResult.totalDocuments = documents.size();
            
            // Apply batching and indexing logic (one batch per claim or multiple based on config)
            applyBatchingLogicForClaim(documents, claimResult, claimNumber);
            
            // Update database with indexing metadata
            updateDatabaseWithIndexing(documents, successLines, failedLines, claimResult);
            
        } catch (Exception e) {
            logger.error("Error processing claim: {}", claimNumber, e);
            throw new RuntimeException("Failed to process claim: " + claimNumber, e);
        }
    }
    
    // ==================== NEW: PROCESS ALL PENDING DOCUMENTS (NO INPUT FILE) ====================
    
    /**
     * Process ALL pending documents from CC_Extract_Staging table.
     * No input CSV file required - queries directly from database.
     * 
     * Filter Conditions:
     *   - claimID IS NOT NULL (synced from ClaimInfo)
     *   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
     *   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
     *   - contentFilePath IS NOT NULL (content file must exist)
     *   - isIndexed = 0 (not yet indexed)
     * 
     * Processing Steps:
     *   1. Query all pending documents from CC_Extract_Staging
     *   2. Group by (CC_Extract_file_Name, claimNumber)
     *   3. Process each group: apply batching logic, update DB
     *   4. Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processAllPendingDocuments() {
        return processAllPendingDocuments(null);
    }
    
    /**
     * Process ALL pending documents from CC_Extract_Staging table (with parallel processing support)
     * 
     * @param executorService ExecutorService for parallel claim processing (null for sequential processing)
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processAllPendingDocuments(ExecutorService executorService) {
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = Collections.synchronizedList(new ArrayList<>());
        List<String> failedLines = Collections.synchronizedList(new ArrayList<>());
        
        logger.info("===== PROCESSING ALL PENDING DOCUMENTS (No Input File Mode, Parallel: {}) =====", executorService != null);
        
        try {
            // STEP 1: Fetch all pending documents from CC_Extract_Staging
            logger.info("STEP 1: Fetching all pending documents from CC_Extract_Staging...");
            List<ClaimCenterDocumentDTO> allPendingDocs = fetchAllPendingDocuments();
            
            if (allPendingDocs.isEmpty()) {
                logger.info("No pending documents found. Nothing to process.");
                return result;
            }
            
            logger.info("Found {} pending documents to process", allPendingDocs.size());
            result.totalDocuments = allPendingDocs.size();
            
            // STEP 2: Group documents by (CC_Extract_file_Name, claimNumber)
            logger.info("STEP 2: Grouping documents by (CC_Extract_file_Name, claimNumber)...");
            Map<String, Map<String, List<ClaimCenterDocumentDTO>>> groupedDocs = groupDocumentsByFileAndClaim(allPendingDocs);
            
            int totalFiles = groupedDocs.size();
            int totalClaims = groupedDocs.values().stream().mapToInt(Map::size).sum();
            logger.info("Grouped into {} extract files, {} claims", totalFiles, totalClaims);
            result.totalClaimNumbers = totalClaims;
            
            // STEP 3: Process each group (parallel if executorService provided, sequential otherwise)
            logger.info("STEP 3: Processing each claim group...");
            if (executorService != null) {
                // Parallel processing - collect all claims across all files
                List<ClaimGroup> allClaimGroups = new ArrayList<>();
                for (Map.Entry<String, Map<String, List<ClaimCenterDocumentDTO>>> fileEntry : groupedDocs.entrySet()) {
                    String extractFileName = fileEntry.getKey();
                    Map<String, List<ClaimCenterDocumentDTO>> claimGroups = fileEntry.getValue();
                    for (Map.Entry<String, List<ClaimCenterDocumentDTO>> claimEntry : claimGroups.entrySet()) {
                        allClaimGroups.add(new ClaimGroup(extractFileName, claimEntry.getKey(), claimEntry.getValue()));
                    }
                }
                
                processClaimGroupsInParallel(allClaimGroups, result, successLines, failedLines, executorService);
            } else {
                // Sequential processing (backward compatible)
                int fileIndex = 0;
                for (Map.Entry<String, Map<String, List<ClaimCenterDocumentDTO>>> fileEntry : groupedDocs.entrySet()) {
                    fileIndex++;
                    String extractFileName = fileEntry.getKey();
                    Map<String, List<ClaimCenterDocumentDTO>> claimGroups = fileEntry.getValue();
                    
                    logger.info("[File {}/{}] Processing {} claims from: {}", 
                               fileIndex, totalFiles, claimGroups.size(), extractFileName);
                    
                    int claimIndex = 0;
                    for (Map.Entry<String, List<ClaimCenterDocumentDTO>> claimEntry : claimGroups.entrySet()) {
                        claimIndex++;
                        String claimNumber = claimEntry.getKey();
                        List<ClaimCenterDocumentDTO> documents = claimEntry.getValue();
                        
                        logger.debug("[File {}/{}][Claim {}/{}] Processing claim: {} ({} docs)", 
                                   fileIndex, totalFiles, claimIndex, claimGroups.size(), 
                                   claimNumber, documents.size());
                        
                        // Apply batching logic to this claim's documents
                        applyBatchingLogicForClaim(documents, result, claimNumber);
                        
                        // Update database with indexing metadata
                        updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                    }
                }
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 4: Generate packaging CSV files
            logger.info("STEP 4: Generating packaging CSV files...");
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Write summary tracking file
            writeAllPendingTrackingFile(successLines, failedLines);
            
            logger.info("===== PROCESSING COMPLETE =====");
            logger.info("Summary: Files={}, Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       totalFiles, result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing all pending documents", e);
            return result;
        }
    }
    
    /**
     * Helper class to hold claim group information
     */
    private static class ClaimGroup {
        String extractFileName;
        String claimNumber;
        List<ClaimCenterDocumentDTO> documents;
        
        ClaimGroup(String extractFileName, String claimNumber, List<ClaimCenterDocumentDTO> documents) {
            this.extractFileName = extractFileName;
            this.claimNumber = claimNumber;
            this.documents = documents;
        }
    }
    
    /**
     * Process claim groups in parallel using ExecutorService
     */
    private void processClaimGroupsInParallel(List<ClaimGroup> claimGroups,
                                             ProcessingResult result, List<String> successLines, List<String> failedLines,
                                             ExecutorService executorService) {
        AtomicInteger completedClaims = new AtomicInteger(0);
        int totalClaims = claimGroups.size();
        List<CompletableFuture<ProcessingResult>> futures = new ArrayList<>();
        
        for (ClaimGroup claimGroup : claimGroups) {
            CompletableFuture<ProcessingResult> future = CompletableFuture.supplyAsync(() -> {
                ProcessingResult claimResult = new ProcessingResult();
                try {
                    logger.debug("Processing claim: {} from file: {} ({} docs)", 
                               claimGroup.claimNumber, claimGroup.extractFileName, claimGroup.documents.size());
                    
                    // Apply batching logic to this claim's documents
                    applyBatchingLogicForClaim(claimGroup.documents, claimResult, claimGroup.claimNumber);
                    
                    // Update database with indexing metadata
                    updateDatabaseWithIndexing(claimGroup.documents, successLines, failedLines, claimResult);
                    
                    int completed = completedClaims.incrementAndGet();
                    logger.debug("Completed claim {}/{}: {} (Thread: {})", 
                               completed, totalClaims, claimGroup.claimNumber, Thread.currentThread().getId());
                    
                    return claimResult;
                } catch (Exception e) {
                    logger.error("Error processing claim: {} from file: {}", claimGroup.claimNumber, claimGroup.extractFileName, e);
                    completedClaims.incrementAndGet();
                    return claimResult; // Return result (may have partial data)
                }
            }, executorService);
            
            futures.add(future);
        }
        
        // Wait for all claims to complete with timeout (prevents hanging forever)
        // Timeout: 5 minutes per claim (conservative for database operations)
        // Minimum: 5 minutes, Maximum: 2 hours
        long timeoutMinutes = Math.min(Math.max(totalClaims * 5, 5), 120);
        CompletableFuture<Void> allFutures = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
        
        try {
            allFutures.get(timeoutMinutes, TimeUnit.MINUTES);
        } catch (java.util.concurrent.TimeoutException e) {
            logger.error("Timeout waiting for claims to complete after {} minutes. Total claims: {}", timeoutMinutes, totalClaims);
            // Cancel remaining futures
            for (CompletableFuture<ProcessingResult> future : futures) {
                if (!future.isDone()) {
                    future.cancel(true);
                }
            }
            throw new RuntimeException("Claim processing timeout after " + timeoutMinutes + " minutes", e);
        } catch (Exception e) {
            logger.error("Error waiting for claims to complete", e);
            throw new RuntimeException("Failed to wait for claim processing", e);
        }
        
        // Aggregate results from all claims (all futures are now done)
        for (CompletableFuture<ProcessingResult> future : futures) {
            try {
                // Since all futures are done (either completed or cancelled), get() will return immediately
                if (!future.isCancelled()) {
                    ProcessingResult claimResult = future.get(); // Safe: future is already done
                    result.merge(claimResult);
                }
            } catch (Exception e) {
                logger.warn("Error aggregating claim result (future may be cancelled)", e);
            }
        }
        
        logger.info("All {} claims processed in parallel", totalClaims);
    }
    
    /**
     * Fetch all pending documents from CC_Extract_Staging
     */
    private List<ClaimCenterDocumentDTO> fetchAllPendingDocuments() throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectAllPendingDocumentsQuery);
             ResultSet rs = pstmt.executeQuery()) {
            
            while (rs.next()) {
                ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                
                // Core fields from CC_Extract_Staging
                dto.setExternalID(rs.getString("externalID"));
                dto.setClaimNumber(rs.getString("claimNumber"));
                dto.setClaimID(rs.getString("claimID"));
                dto.setGwDocumentID(rs.getString("gwDocumentID"));
                dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                
                // Document metadata
                dto.setDocumentTitle(rs.getString("documentTitle"));
                dto.setDocumentType(rs.getString("documentType"));
                dto.setDocumentSubtype(rs.getString("documentSubtype"));
                dto.setAuthor(rs.getString("author"));
                dto.setDocumentDescription(rs.getString("documentDescription"));
                dto.setPolicyNumber(rs.getString("policyNumber"));
                dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                dto.setInputMethod(rs.getString("inputMethod"));
                dto.setMimeType(rs.getString("mimeType"));
                dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                dto.setContentFilePath(rs.getString("contentFilePath"));
                dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                dto.setContentType(rs.getString("contentType"));
                
                // Columns NOT in CC_Extract_Staging - set to empty
                dto.setAmount("");
                dto.setClaimant("");
                dto.setCoverage("");
                dto.setCustomerID("");
                dto.setDuplicate("");
                dto.setExposureID("");
                dto.setHidden("");
                dto.setInsuredName("");
                dto.setPrimaryMembershipNumber("");
                dto.setReviewed("");
                
                documents.add(dto);
            }
        }
        
        return documents;
    }
    
    /**
     * Group documents by (CC_Extract_file_Name, claimNumber)
     * Returns: Map<CC_Extract_file_Name, Map<claimNumber, List<Documents>>>
     */
    private Map<String, Map<String, List<ClaimCenterDocumentDTO>>> groupDocumentsByFileAndClaim(
            List<ClaimCenterDocumentDTO> documents) {
        
        Map<String, Map<String, List<ClaimCenterDocumentDTO>>> grouped = new LinkedHashMap<>();
        
        for (ClaimCenterDocumentDTO doc : documents) {
            String fileName = doc.getCcExtractFileName();
            String claimNumber = doc.getClaimNumber();
            
            // Get or create file map
            Map<String, List<ClaimCenterDocumentDTO>> fileMap = 
                grouped.computeIfAbsent(fileName, k -> new LinkedHashMap<>());
            
            // Get or create claim list
            List<ClaimCenterDocumentDTO> claimList = 
                fileMap.computeIfAbsent(claimNumber, k -> new ArrayList<>());
            
            claimList.add(doc);
        }
        
        return grouped;
    }
    
    /**
     * Write tracking file for all pending documents processing
     */
    private void writeAllPendingTrackingFile(List<String> successLines, List<String> failedLines) {
        try {
            String timestamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
            Path trackingDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.COMPLETED_FOLDER);
            
            // Write success file
            if (!successLines.isEmpty()) {
                String successFileName = "AllPending_Success_" + timestamp + ".txt";
                Path successFile = trackingDir.resolve(successFileName);
                try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                    writer.write("# All Pending Documents Processing - Success Report");
                    writer.newLine();
                    writer.write("# Generated: " + new Date());
                    writer.newLine();
                    writer.write("# Total: " + successLines.size() + " documents");
                    writer.newLine();
                    writer.newLine();
                    for (String line : successLines) {
                        writer.write(line);
                        writer.newLine();
                    }
                }
                logger.info("Written success tracking file: {}", successFileName);
            }
            
            // Write failed file
            if (!failedLines.isEmpty()) {
                String failedFileName = "AllPending_Failed_" + timestamp + ".txt";
                Path failedFile = trackingDir.resolve(failedFileName);
                try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                    writer.write("# All Pending Documents Processing - Failed Report");
                    writer.newLine();
                    writer.write("# Generated: " + new Date());
                    writer.newLine();
                    writer.write("# Total: " + failedLines.size() + " documents");
                    writer.newLine();
                    writer.newLine();
                    for (String line : failedLines) {
                        writer.write(line);
                        writer.newLine();
                    }
                }
                logger.info("Written failed tracking file: {}", failedFileName);
            }
            
        } catch (IOException e) {
            logger.error("Error writing tracking files", e);
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive, supports both camelCase and UNDERSCORE formats)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replace("\"", "");
                    String normalizedHeader = header.toUpperCase().replace("_", "");
                    
                    // Support both "claimNumber" and "CLAIM_NUMBER" formats
                    if (normalizedHeader.equals("CLAIMNUMBER")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' or 'CLAIM_NUMBER' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    logger.error("Supported formats: claimNumber, ClaimNumber, CLAIM_NUMBER, claim_number");
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents of a single claim from CC_Extract_Staging table.
     * 
     * Filter conditions:
     *   - claimNumber matches
     *   - CC_Extract_file_Name matches (to filter by specific extract file)
     *   - claimID IS NOT NULL (synced from ClaimInfo)
     *   - gwDocumentID IS NOT NULL (synced from guidewireIDMapping)
     *   - isDataMerged = 1 (content synced from Wawa_Doc_Migration_Transit_Data)
     *   - contentFilePath IS NOT NULL (content file must exist)
     *   - isIndexed = 0 (not yet indexed)
     * 
     * Column mapping from CC_Extract_Staging:
     *   - Name -> documentTitle
     *   - Missing columns (amount, claimant, etc.) are set to empty strings
     * 
     * @param claimNumber The claim number to fetch documents for
     * @param ccExtractFileName The CC_Extract_file_Name to filter by
     */
    private List<ClaimCenterDocumentDTO> fetchDocumentsForOneClaim(String claimNumber, String ccExtractFileName) throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectDocumentsByClaimQuery)) {
            
            pstmt.setString(1, claimNumber);
            pstmt.setString(2, ccExtractFileName);
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields from CC_Extract_Staging
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                    dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                    
                    // Document metadata from CC_Extract_Staging
                    // Note: Name column is aliased as documentTitle in SQL query
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    dto.setContentType(rs.getString("contentType"));
                    
                    // Columns NOT in CC_Extract_Staging - set to empty for packaging output
                    dto.setAmount("");
                    dto.setClaimant("");
                    dto.setCoverage("");
                    dto.setCustomerID("");
                    dto.setDuplicate("");
                    dto.setExposureID("");
                    dto.setHidden("");
                    dto.setInsuredName("");
                    dto.setPrimaryMembershipNumber("");
                    dto.setReviewed("");
                    
                    documents.add(dto);
                }
            }
        }
        
        logger.debug("Fetched {} documents for claimNumber: {} from file: {}", 
                    documents.size(), claimNumber, ccExtractFileName);
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents for a single claim
     * 
     * Case 1: forceSingleSet = true
     *   - Create multiple batches, each with 1 set (max setDocCount docs per batch)
     *   - Example: 27 docs → Batch1(25 docs, 1 set) + Batch2(2 docs, 1 set)
     * 
     * Case 2: forceSingleSet = false
     *   - Create one batch per claim with multiple sets (max setDocCount docs per set)
     *   - Example: 27 docs → Batch1(27 docs, Set1: 25 docs + Set2: 2 docs)
     * 
     * @param documents List of documents for this claim
     * @param result ProcessingResult to track batch IDs
     * @param claimNumber Current claim number being processed
     */
    private void applyBatchingLogicForClaim(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        logger.debug("Applying batching logic for claim: {} with {} documents (forceSingleSet={})", 
                    claimNumber, totalDocs, forceSingleSet);
        
        if (forceSingleSet) {
            // CASE 1: Create multiple batches, each with 1 set (max setDocCount docs per batch)
            applyForceSingleSetLogic(documents, result, claimNumber);
        } else {
            // CASE 2: Create one batch per claim with multiple sets
            applyMultiSetPerBatchLogic(documents, result, claimNumber);
        }
    }
    
    /**
     * Case 1: Force single set per batch
     * - Create multiple batches for the claim
     * - Each batch contains exactly 1 set (max setDocCount docs)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 25 docs (setID=1, setDocCount=25, batchDocCount=25)
     *   Batch 2: 2 docs (setID=1, setDocCount=2, batchDocCount=2)
     */
    private void applyForceSingleSetLogic(List<ClaimCenterDocumentDTO> documents, 
                                          ProcessingResult result, 
                                          String claimNumber) {
        
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        int batchNumber = 1;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size (max setDocCount)
            int currentBatchSize = Math.min(setDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID(batchID);
            result.batchIDsCreated.add(batchID);
            
            logger.info("Claim: {} - Batch {}: batchID={}, docs={}, sets=1", 
                       claimNumber, batchNumber, batchID, currentBatchSize);
            
            // Process documents in current batch (single set)
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata (always setID = 1 for this case)
                doc.setSetID(1);
                doc.setSetDocCount(currentBatchSize);
                doc.setSetDocIndex(setDocIndex);
                
                setDocIndex++;
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
            batchNumber++;
        }
        
        logger.info("Claim: {} - Created {} batches (forceSingleSet=true)", claimNumber, batchNumber - 1);
    }
    
    /**
     * Case 2: Multiple sets per batch
     * - Create one batch for the claim
     * - Batch contains multiple sets (max setDocCount docs per set)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 27 docs (batchDocCount=27)
     *     - Set 1: setID=1, setDocCount=25, setDocIndex=1-25
     *     - Set 2: setID=2, setDocCount=2, setDocIndex=1-2
     */
    private void applyMultiSetPerBatchLogic(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        
        // Generate unique batch ID and job ID (one per claim)
        String batchID = generateBatchID();
        String jobID = generateJobID(batchID);
        result.batchIDsCreated.add(batchID);
        
        // Calculate number of sets needed
        int numberOfSets = (int) Math.ceil((double) totalDocs / setDocCount);
        
        logger.info("Claim: {} - Batch: batchID={}, docs={}, sets={}", 
                   claimNumber, batchID, totalDocs, numberOfSets);
        
        // Process all documents in this single batch
        int setID = 1;
        int setDocIndex = 1;
        int docsInCurrentSet = 0;
        
        for (ClaimCenterDocumentDTO doc : documents) {
            docsInCurrentSet++;
            
            // Calculate set doc count for current set
            int remainingDocs = totalDocs - ((setID - 1) * setDocCount);
            int currentSetDocCount = Math.min(setDocCount, remainingDocs);
            
            // Set batch-level metadata
            doc.setBatchID(batchID);
            doc.setJobID(jobID);
            doc.setBatchDocCount(totalDocs);
            
            // Set set-level metadata
            doc.setSetID(setID);
            doc.setSetDocCount(currentSetDocCount);
            doc.setSetDocIndex(setDocIndex);
            
            // Increment set document index
            setDocIndex++;
            
            // Move to next set if current set is full
            if (setDocIndex > setDocCount) {
                logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
                setID++;
                setDocIndex = 1;
                docsInCurrentSet = 0;
            }
        }
        
        // Log last set if it has documents
        if (docsInCurrentSet > 0) {
            logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
        }
        
        logger.info("Claim: {} - Created 1 batch with {} sets (forceSingleSet=false)", claimNumber, numberOfSets);
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID(String batchID) {
        return Constants.JOB_ID_PREFIX + batchID;
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata using JDBC batch processing
     * Falls back to individual UPDATEs if batch fails
     */
    private void updateDatabaseWithIndexing(List<ClaimCenterDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        // Get batch size from config (default 1000)
        String batchSizeStr = config.getProperty("db.indexing.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        try {
            // Try batch UPDATE first (fast path)
            updateDatabaseWithIndexingBatch(documents, successLines, failedLines, result, batchSize);
            
        } catch (SQLException e) {
            logger.warn("Batch UPDATE failed, falling back to individual UPDATEs: {}", e.getMessage());
            // Fallback to individual UPDATEs
            updateDatabaseWithIndexingIndividual(documents, successLines, failedLines, result);
        }
    }
    
    /**
     * Batch UPDATE implementation (100x faster than individual UPDATEs)
     */
    private void updateDatabaseWithIndexingBatch(List<ClaimCenterDocumentDTO> documents,
                                                 List<String> successLines,
                                                 List<String> failedLines,
                                                 ProcessingResult result,
                                                 int batchSize) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            int count = 0;
            int batchStartIndex = 0;
            
            for (int i = 0; i < documents.size(); i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                pstmt.setInt(1, doc.getBatchDocCount());
                pstmt.setString(2, doc.getBatchID());
                pstmt.setString(3, doc.getJobID());
                pstmt.setInt(4, doc.getSetDocCount());
                pstmt.setInt(5, doc.getSetID());
                pstmt.setInt(6, doc.getSetDocIndex());
                pstmt.setString(7, doc.getExternalID());
                pstmt.addBatch();
                count++;
                
                // Execute batch when size reached
                if (count % batchSize == 0) {
                    int[] results = pstmt.executeBatch();
                    conn.commit();
                    
                    // Track results
                    for (int j = 0; j < results.length; j++) {
                        ClaimCenterDocumentDTO batchDoc = documents.get(batchStartIndex + j);
                        if (results[j] > 0) {
                            result.indexedSuccessCount++;
                            successLines.add(batchDoc.getExternalID());
                        } else {
                            result.indexedFailureCount++;
                            failedLines.add(batchDoc.getExternalID() + "|No record found");
                        }
                    }
                    
                    logger.debug("Executed batch of {} indexing UPDATEs", count);
                    batchStartIndex = i + 1;
                }
            }
            
            // Execute remaining batch
            if (count % batchSize != 0) {
                int[] results = pstmt.executeBatch();
                conn.commit();
                
                // Track results for remaining batch
                int remainingStart = (count / batchSize) * batchSize;
                for (int j = 0; j < results.length; j++) {
                    ClaimCenterDocumentDTO batchDoc = documents.get(remainingStart + j);
                    if (results[j] > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(batchDoc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(batchDoc.getExternalID() + "|No record found");
                    }
                }
                
                logger.debug("Executed final batch of {} indexing UPDATEs", count % batchSize);
            }
            
            logger.info("Batch indexing UPDATE completed: {} documents indexed, {} failed", 
                       result.indexedSuccessCount, result.indexedFailureCount);
                       
        } catch (SQLException e) {
            logger.error("Batch indexing UPDATE failed", e);
            throw e; // Re-throw to trigger fallback
        }
    }
    
    /**
     * Individual UPDATE fallback (for when batch fails)
     */
    private void updateDatabaseWithIndexingIndividual(List<ClaimCenterDocumentDTO> documents,
                                                      List<String> successLines,
                                                      List<String> failedLines,
                                                      ProcessingResult result) {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (fallback mode)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            
            for (ClaimCenterDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Individual indexing UPDATE completed (fallback)");
            
        } catch (SQLException e) {
            logger.error("Individual UPDATE fallback also failed", e);
            // Connection auto-closes and rolls back due to auto-commit=false
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replace(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        // Replace the %IN_CLAUSE% placeholder with the generated IN clause
        String selectSQL = selectPackagingByBatchIdsQueryTemplate.replace("%IN_CLAUSE%", inClause.toString());
        
        List<ClaimCenterDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields from CC_Extract_Staging
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setGwDocExternalID(rs.getString("gwDocExternalID"));
                    dto.setCcExtractFileName(rs.getString("CC_Extract_file_Name"));
                    
                    // Batch/Set metadata
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    
                    // Document metadata from CC_Extract_Staging
                    // Note: Name column is aliased as documentTitle in SQL query
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    dto.setContentType(rs.getString("contentType"));
                    
                    // Columns NOT in CC_Extract_Staging - set to empty for packaging output
                    dto.setAmount("");
                    dto.setClaimant("");
                    dto.setCoverage("");
                    dto.setCustomerID("");
                    dto.setDuplicate("");
                    dto.setExposureID("");
                    dto.setHidden("");
                    dto.setInsuredName("");
                    dto.setPrimaryMembershipNumber("");
                    dto.setReviewed("");
                    
                    // If config is set to true, clear gwDocumentID (set to empty)
                    if (setGwDocumentIDAsEmpty) {
                        dto.setGwDocumentID("");
                        logger.debug("Set gwDocumentID to empty for externalID={} (config: setgwDocumentIDAsEmpty=true)", 
                                    dto.getExternalID());
                    }
                    
                    // If documentTitle is null/empty and config allows, set it from contentRetrievalName (without extension)
                    if (canSetDocTitleAsContentFileNameIfNull && 
                        (dto.getDocumentTitle() == null || dto.getDocumentTitle().trim().isEmpty())) {
                        String contentFileName = dto.getContentRetrievalName();
                        if (contentFileName != null && !contentFileName.trim().isEmpty()) {
                            // Remove file extension to get the filename without extension
                            String fileNameWithoutExtension = removeFileExtension(contentFileName);
                            dto.setDocumentTitle(fileNameWithoutExtension);
                            logger.debug("Set documentTitle from contentRetrievalName for externalID={}: '{}'", 
                                        dto.getExternalID(), fileNameWithoutExtension);
                        }
                    }
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<ClaimCenterDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (ClaimCenterDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(ClaimCenterDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<ClaimCenterDocumentDTO> documents) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateProcessedQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (updateProcessedFlag)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            Date currentDate = new Date();
            
            for (ClaimCenterDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLACE_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replace(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
    
    /**
     * Remove file extension from filename
     * Example: "document.pdf" -> "document"
     *          "file.name.txt" -> "file.name"
     *          "noextension" -> "noextension"
     */
    private String removeFileExtension(String fileName) {
        if (fileName == null || fileName.isEmpty()) {
            return fileName;
        }
        
        int lastDotIndex = fileName.lastIndexOf('.');
        if (lastDotIndex > 0) {
            return fileName.substring(0, lastDotIndex);
        }
        
        // No extension found, return as-is
        return fileName;
    }
}
