package com.wawanesa.ace.index;

import java.io.File;
import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.utils.GlobalProcessingReport;
import com.wawanesa.ace.index.utils.Utils;

/**
 * CCDataIndexAndPackagingUtility - Main Service Class
 * 
 * Purpose:
 * - Process CSV files from CCDataMergeUtility output
 * - Apply batching/indexing logic to documents
 * - Update database with batch/set metadata
 * - Generate packaging CSV files for migration
 * 
 * Processing Flow:
 * Step 1: Validate/Create folder structure
 * Step 2: Move CSV files from Completed → Indexing\Data
 * Step 3: Extract claim numbers → Query DB → Apply batching logic
 * Step 4: Update database with indexing metadata
 * Step 5a: Write success/failed tracking files → Archive CSV
 * Step 5b: Generate packaging CSV files (pipe-delimited, max 50K rows)
 */
public class CCDataIndexPackagingService {

    private static final Logger logger = LogManager.getLogger(CCDataIndexPackagingService.class);

    public static void main(String[] args) {

        logger.info("===== CCDataIndexAndPackagingUtility STARTED =====");
        ConnectionManager connManager = null;
        ExecutorService executor = null;
        ScheduledExecutorService progressMonitor = null;
        
        // Initialize global processing report
        GlobalProcessingReport globalReport = new GlobalProcessingReport();

        try {
            // Load configuration
            PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
            
            logger.info("=== CONFIGURATION ===");
            logger.info("JRE Library Path: {}", System.getProperty("java.library.path"));
            logger.info("Config File: {}", System.getProperty("config.file"));
            logger.info("Log4j Config: {}", System.getProperty("log4j.configurationFile"));
            
            String basePath = config.getProperty("app.base_path");
            logger.info("Base Path: {}", basePath);
            
            // Configure thread pool size from properties
            String threadPoolSizeStr = config.getProperty("app.thread.pool.size");
            int threadPoolSize = (threadPoolSizeStr != null) ? Integer.parseInt(threadPoolSizeStr) : 5;
            ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);
            executor = executorService;
            
            logger.info("Thread pool initialized with {} threads", threadPoolSize);
            
            // Configure progress monitoring
            String progressIntervalStr = config.getProperty("app.progress.log.interval.minutes");
            int progressIntervalMinutes = (progressIntervalStr != null) ? Integer.parseInt(progressIntervalStr) : 5;
            
            // Create final references for shutdown hook and monitoring
            final ExecutorService executorRef = executor;
            final ConnectionManager[] connManagerRef = new ConnectionManager[1];
            
            // Setup progress monitoring if enabled
            if (progressIntervalMinutes > 0) {
                ScheduledExecutorService progressMonitorService = Executors.newScheduledThreadPool(1);
                progressMonitor = progressMonitorService;
                
                progressMonitorService.scheduleAtFixedRate(() -> {
                    try {
                        globalReport.logProgress();
                        
                        // Log thread pool statistics
                        if (executorRef instanceof ThreadPoolExecutor) {
                            ThreadPoolExecutor tpe = (ThreadPoolExecutor) executorRef;
                            logger.info("Thread Pool: Active={}/{}, Queue={}", 
                                       tpe.getActiveCount(), tpe.getPoolSize(), tpe.getQueue().size());
                            globalReport.updateThreadPoolStats(tpe.getPoolSize(), tpe.getActiveCount(), tpe.getQueue().size());
                        }
                    } catch (Exception e) {
                        logger.error("Error in progress monitor", e);
                    }
                }, 0, progressIntervalMinutes, TimeUnit.MINUTES);
                
                logger.info("Progress monitoring enabled (interval: {} minutes)", progressIntervalMinutes);
            } else {
                logger.info("Progress monitoring disabled");
            }
            
            // Store progressMonitor reference for shutdown hook
            final ScheduledExecutorService progressMonitorRef = progressMonitor;
            
            // Add shutdown hook for abnormal termination
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Shutdown hook triggered - cleaning up resources...");
                
                // Shutdown progress monitor first
                if (progressMonitorRef != null && !progressMonitorRef.isShutdown()) {
                    try {
                        progressMonitorRef.shutdown();
                        progressMonitorRef.awaitTermination(5, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        progressMonitorRef.shutdownNow();
                    }
                }
                
                if (executorRef != null && !executorRef.isShutdown()) {
                    try {
                        executorRef.shutdown();
                        if (!executorRef.awaitTermination(30, TimeUnit.SECONDS)) {
                            executorRef.shutdownNow();
                        }
                    } catch (InterruptedException e) {
                        executorRef.shutdownNow();
                    }
                }
                
                if (connManagerRef[0] != null) {
                    try {
                        connManagerRef[0].close();
                    } catch (Exception e) {
                        logger.error("Error closing connection in shutdown hook", e);
                    }
                }
                
                logger.info("Shutdown hook completed");
            }));
            
            // STEP 1: Validate and create required folder structure
            logger.info("=== STEP 1: Validating Folder Structure ===");
            if (!validateAndCreateFolderStructure(basePath)) {
                logger.error("Failed to validate/create folder structure. Exiting...");
                return;
            }
            logger.info("Folder structure validated successfully");
            
            // Initialize connection manager
            connManager = new ConnectionManager(config);
            connManagerRef[0] = connManager;
            logger.info("Database connection pool initialized");
            
            // Initialize utility class
            Utils utils = new Utils(config, connManager);
            
            // STEP 2: Move CSV files from Completed → Indexing\Data
            logger.info("=== STEP 2: Moving CSV Files ===");
            int movedFileCount = moveCSVFilesToIndexingData(basePath);
            logger.info("Moved {} CSV file(s) to Indexing\\Data folder", movedFileCount);
            
            if (movedFileCount == 0) {
                logger.warn("No CSV files found to process. Exiting...");
                return;
            }
            
            // STEP 3-5: Process all CSV files in Indexing\Data folder (MULTI-THREADED)
            logger.info("=== STEP 3-5: Processing CSV Files (Multi-Threaded) ===");
            Path indexingDataDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
            int processedFileCount = processCSVFiles(indexingDataDir, utils, executor, globalReport);
            
            logger.info("=== PROCESSING COMPLETE ===");
            logger.info("Total CSV files processed: {}", processedFileCount);
            logger.info("===== CCDataIndexAndPackagingUtility COMPLETED SUCCESSFULLY =====");

        } catch (Exception e) {
            logger.error("Critical error in CCDataIndexPackagingService", e);
            e.printStackTrace();
        } finally {
            // Shutdown progress monitor
            if (progressMonitor != null && !progressMonitor.isShutdown()) {
                try {
                    logger.info("Shutting down progress monitor...");
                    progressMonitor.shutdown();
                    progressMonitor.awaitTermination(5, TimeUnit.SECONDS);
                    logger.info("Progress monitor shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down progress monitor", e);
                    progressMonitor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Shutdown ExecutorService gracefully
            if (executor != null && !executor.isShutdown()) {
                try {
                    logger.info("Shutting down executor service...");
                    executor.shutdown();
                    
                    // Calculate dynamic timeout based on number of files queued
                    // Each CSV file can process multiple claims with database operations
                    // Production-optimized: Conservative timeouts for complex processing
                    int filesProcessed = globalReport.getCsvFilesQueued();
                    long shutdownTimeout;
                    
                    if (filesProcessed == 0) {
                        // No files processed - quick shutdown
                        shutdownTimeout = 10; // 10 seconds
                    } else if (filesProcessed <= 10) {
                        // Small batch: 10 minutes per file (indexing + packaging can be lengthy)
                        shutdownTimeout = filesProcessed * 600;
                    } else if (filesProcessed <= 100) {
                        // Medium batch: 5 minutes per file + 30 minute buffer
                        shutdownTimeout = (filesProcessed * 300) + 1800;
                    } else {
                        // Large batch: 2 minutes per file + 1 hour buffer, max 24 hours
                        long calculated = (filesProcessed * 120) + 3600;
                        shutdownTimeout = Math.min(calculated, 86400); // 24 hours max
                    }
                    
                    logger.info("Waiting for {} file(s) to complete (timeout: {} seconds / {}.{} minutes)...", 
                               filesProcessed, shutdownTimeout, shutdownTimeout / 60, (shutdownTimeout % 60));
                    
                    long startTime = System.currentTimeMillis();
                    
                    // Wait for existing tasks to terminate
                    if (!executor.awaitTermination(shutdownTimeout, TimeUnit.SECONDS)) {
                        long waitedTime = (System.currentTimeMillis() - startTime) / 1000;
                        logger.warn("Executor did not terminate after {} seconds, forcing shutdown...", waitedTime);
                        executor.shutdownNow();
                        
                        if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                            logger.error("Executor did not terminate after forced shutdown");
                        }
                    } else {
                        long actualTime = (System.currentTimeMillis() - startTime) / 1000;
                        logger.info("All tasks completed successfully in {} seconds", actualTime);
                    }
                    
                    logger.info("Executor service shut down successfully");
                } catch (InterruptedException e) {
                    logger.error("Interrupted while shutting down executor", e);
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }
            
            // Close connection pool
            if (connManager != null) {
                try {
                    logger.info("Closing database connection pool...");
                    connManager.close();
                    logger.info("Database connection pool closed successfully");
                } catch (Exception e) {
                    logger.error("Error closing connection manager", e);
                }
            }
            
            // Write final summary report
            try {
                PropertiesConfigLoader config = PropertiesConfigLoader.getInstance();
                String basePath = config.getProperty("app.base_path");
                
                logger.info("===== FINAL SUMMARY =====");
                logger.info(globalReport.getSummary());
                
                // Validate/Create SummaryReports folder
                File summaryReportsDir = new File(basePath, "SummaryReports");
                if (!summaryReportsDir.exists()) {
                    if (summaryReportsDir.mkdirs()) {
                        logger.info("Created SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                    } else {
                        logger.error("Failed to create SummaryReports folder: {}", summaryReportsDir.getAbsolutePath());
                        throw new IOException("Cannot create SummaryReports folder");
                    }
                } else {
                    logger.debug("SummaryReports folder already exists: {}", summaryReportsDir.getAbsolutePath());
                }
                
                globalReport.writeReportToFile(basePath);
                logger.info("Summary report written to: {}", summaryReportsDir.getAbsolutePath());
            } catch (Exception e) {
                logger.error("Failed to write summary report", e);
            }
            
            logger.info("Application terminated");
        }
    }

    // ==================== STEP 1: VALIDATE AND CREATE FOLDERS ====================
    
    /**
     * Step 1: Validate and create required folder structure
     * 
     * Required folders:
     * - D:\Rameshwar\ClaimCenterDataMerge\Completed (source)
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Archive
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Completed
     * - D:\Rameshwar\ClaimCenterDataMerge\Indexing\Failed
     * - D:\Rameshwar\ClaimCenterDataMerge\Packaging
     */
    private static boolean validateAndCreateFolderStructure(String basePath) {
        boolean success = true;
        
        // Define required folders
        String[] requiredFolders = {
            Constants.COMPLETED_SOURCE_FOLDER,
            Constants.INDEXING_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.DATA_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.ARCHIVE_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.COMPLETED_FOLDER,
            Constants.INDEXING_FOLDER + File.separator + Constants.FAILED_FOLDER,
            Constants.PACKAGING_FOLDER
        };
        
        for (String folderName : requiredFolders) {
            File folder = new File(basePath, folderName);
            
            if (folder.exists() && folder.isDirectory()) {
                logger.info("✓ Folder exists: {}", folder.getAbsolutePath());
            } else {
                // Create folder
                if (folder.mkdirs()) {
                    logger.info("✓ Folder created: {}", folder.getAbsolutePath());
                } else {
                    logger.error("✗ Failed to create folder: {}", folder.getAbsolutePath());
                    success = false;
                }
            }
        }
        
        return success;
    }
    
    // ==================== STEP 2: MOVE CSV FILES ====================
    
    /**
     * Step 2: Move all CSV files from Completed → Indexing\Data
     * 
     * Source: D:\Rameshwar\ClaimCenterDataMerge\Completed
     * Target: D:\Rameshwar\ClaimCenterDataMerge\Indexing\Data
     */
    private static int moveCSVFilesToIndexingData(String basePath) {
        Path sourceDir = Paths.get(basePath, Constants.COMPLETED_SOURCE_FOLDER);
        Path targetDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.DATA_FOLDER);
        
        logger.info("Source Directory: {}", sourceDir);
        logger.info("Target Directory: {}", targetDir);
        
        int movedCount = 0;
        
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    try {
                        String fileName = csvFile.getFileName().toString();
                        Path targetFile = targetDir.resolve(fileName);
                        
                        Files.move(csvFile, targetFile, StandardCopyOption.REPLACE_EXISTING);
                        logger.info("Moved: {} → {}", fileName, targetDir);
                        movedCount++;
                        
                    } catch (IOException e) {
                        logger.error("Failed to move file: {}", csvFile.getFileName(), e);
                    }
                }
            }
        } catch (IOException e) {
            logger.error("Error reading source directory: {}", sourceDir, e);
        }
        
        return movedCount;
    }
    
    // ==================== STEP 3-5: PROCESS CSV FILES (MULTI-THREADED) ====================
    
    /**
     * Step 3-5: Process all CSV files in Indexing\Data folder using multi-threading
     * 
     * For each CSV file (in parallel):
     * - Extract claim numbers
     * - Query database for documents
     * - Apply batching/indexing logic
     * - Update database (BATCH UPDATEs)
     * - Write tracking files
     * - Generate packaging CSV files
     * - Archive processed CSV
     */
    private static int processCSVFiles(Path indexingDataDir, Utils utils, 
                                      ExecutorService executor, GlobalProcessingReport globalReport) {
        logger.info("Scanning Indexing\\Data directory for CSV files: {}", indexingDataDir);
        
        // Collect all CSV files
        List<Path> csvFiles = new ArrayList<>();
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexingDataDir, "*.csv")) {
            for (Path csvFile : stream) {
                if (Files.isRegularFile(csvFile)) {
                    csvFiles.add(csvFile);
                    globalReport.incrementCSVFilesQueued();
                }
            }
        } catch (IOException e) {
            logger.error("Error reading Indexing\\Data directory: {}", indexingDataDir, e);
            return 0;
        }
        
        if (csvFiles.isEmpty()) {
            logger.warn("No CSV files found in Indexing\\Data directory: {}", indexingDataDir);
            return 0;
        }
        
        logger.info("Found {} CSV file(s) to process with multi-threading", csvFiles.size());
        
        // Process each CSV file in parallel
        for (Path csvFile : csvFiles) {
            final String fileName = csvFile.getFileName().toString();
            
            executor.submit(() -> {
                long taskStartTime = System.currentTimeMillis();
                logger.info("[TASK START] Thread-{}: Processing CSV file: {}", 
                           Thread.currentThread().getId(), fileName);
                
                try {
                    // Process the CSV file (Steps 3-5)
                    Utils.ProcessingResult result = utils.processCSVFile(csvFile);
                    
                    long taskDuration = (System.currentTimeMillis() - taskStartTime) / 1000;
                    logger.info("[TASK COMPLETE] Thread-{}: File: {} processed in {} seconds (Claims: {}, Docs: {}, Indexed: {})", 
                               Thread.currentThread().getId(), fileName, taskDuration,
                               result.totalClaimNumbers, result.totalDocuments, result.indexedSuccessCount);
                    
                    // Record success in global report
                    globalReport.recordCSVFileSuccess(fileName, result);
                    
                } catch (Exception e) {
                    long taskDuration = (System.currentTimeMillis() - taskStartTime) / 1000;
                    logger.error("[TASK FAILED] Thread-{}: File: {} failed after {} seconds - Error: {}", 
                                Thread.currentThread().getId(), fileName, taskDuration, e.getMessage(), e);
                    globalReport.recordCSVFileFailure(fileName, e.getMessage());
                }
            });
        }
        
        // NOTE: Do NOT call executor.shutdown() here - it's handled in finally block
        logger.info("All {} CSV file(s) submitted for processing", csvFiles.size());
        
        return csvFiles.size();
    }
}


package com.wawanesa.ace.index.connection;

import java.sql.Connection;
import java.sql.SQLException;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.zaxxer.hikari.HikariConfig;
import com.zaxxer.hikari.HikariDataSource;

public class ConnectionManager {

    private HikariDataSource dataSource;

    public ConnectionManager(PropertiesConfigLoader properties) {
        HikariConfig config = new HikariConfig();
        String serverName = properties.getProperty("db.serverName");
        String port = properties.getProperty("db.port");
        String dbName = properties.getProperty("db.dbName");
        int poolsize = Integer.parseInt(properties.getProperty("db.pool.size"));

        System.out.println("serverName : " + serverName);
        System.out.println("port       : " + port);
        System.out.println("dbName     : " + dbName);
        System.out.println("poolsize   : " + poolsize);

        String jdbcUrl = "jdbc:sqlserver://" + serverName + ":" + port
                + ";DatabaseName=" + dbName
                + ";integratedSecurity=true;trustServerCertificate=true;";

        System.out.println("jdbcUrl    : " + jdbcUrl);

        config.setJdbcUrl(jdbcUrl);
        config.setDriverClassName("com.microsoft.sqlserver.jdbc.SQLServerDriver");

        // Performance tuning optimized for batch processing
        config.setMaximumPoolSize(poolsize);
        config.setMinimumIdle(2);
        config.setIdleTimeout(30000);         // 30 seconds idle timeout
        config.setConnectionTimeout(30000);    // 30 seconds connection timeout
        config.setMaxLifetime(1800000);        // 30 minutes max lifetime
        
        // Leak detection: Only warn if connection held > 10 minutes
        // (Batch processing can legitimately hold connections for several minutes)
        config.setLeakDetectionThreshold(600000); // 10 minutes
        
        // Enable fast connection validation
        config.setConnectionTestQuery("SELECT 1");
        config.setValidationTimeout(3000);     // 3 seconds validation timeout

        dataSource = new HikariDataSource(config);
    }

    public void getPoolInfo() {
        System.out.println("Active connections: " + dataSource.getHikariPoolMXBean().getActiveConnections());
        System.out.println("Idle connections  : " + dataSource.getHikariPoolMXBean().getIdleConnections());
        System.out.println("Total connections : " + dataSource.getHikariPoolMXBean().getTotalConnections());
    }

    public Connection getConnection() throws SQLException {
        Connection conn = dataSource.getConnection();
        // Disable auto-commit for batch processing performance
        // Commits will be managed explicitly by the calling code
        conn.setAutoCommit(false);
        return conn;
    }

    public void close() {
        if (dataSource != null && !dataSource.isClosed()) {
            dataSource.close();
        }
    }
}




package com.wawanesa.ace.index.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Set;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.wawanesa.ace.index.configuration.PropertiesConfigLoader;
import com.wawanesa.ace.index.connection.ConnectionManager;
import com.wawanesa.ace.index.constants.Constants;
import com.wawanesa.ace.index.model.ClaimCenterDocumentDTO;

/**
 * Utility class for CCDataIndexAndPackagingUtility
 * Handles:
 * - Indexing: Batching documents and updating DB with batch/set metadata
 * - Packaging: Generating pipe-delimited CSV files for migration
 */
public class Utils {
    
    private static final Logger logger = LogManager.getLogger(Utils.class);
    
    private PropertiesConfigLoader config;
    private ConnectionManager connectionManager;
    private String basePath;
    private String dbTableName;
    private int setDocCount;
    private boolean forceSingleSet;
    
    // SQL Queries loaded from properties
    private String selectDocumentsByClaimQuery;
    private String updateIndexingQuery;
    private String selectPackagingByBatchIdsQueryTemplate;
    private String updateProcessedQuery;
    
    /**
     * Constructor
     */
    public Utils(PropertiesConfigLoader config, ConnectionManager connectionManager) {
        this.config = config;
        this.connectionManager = connectionManager;
        this.basePath = config.getProperty("app.base_path");
        this.dbTableName = config.getProperty("db.staging.claimcenterdbtable");
        
        // Load set configuration (with default)
        String setDocCountStr = config.getProperty("app.set.doc.count");
        this.setDocCount = (setDocCountStr != null) ? 
            Integer.parseInt(setDocCountStr) : Constants.DEFAULT_SET_DOC_COUNT;
        
        // Load force single set configuration (with default)
        String forceSingleSetStr = config.getProperty("app.batch.force.single.set");
        this.forceSingleSet = (forceSingleSetStr != null) ? 
            Boolean.parseBoolean(forceSingleSetStr) : Constants.DEFAULT_FORCE_SINGLE_SET;
        
        // Load SQL queries from properties and replace table name placeholder
        this.selectDocumentsByClaimQuery = config.getProperty("db.query.select.documents.by.claim")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateIndexingQuery = config.getProperty("db.query.update.indexing")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.selectPackagingByBatchIdsQueryTemplate = config.getProperty("db.query.select.packaging.by.batchids")
            .replace("%TABLE_NAME%", this.dbTableName);
        this.updateProcessedQuery = config.getProperty("db.query.update.processed")
            .replace("%TABLE_NAME%", this.dbTableName);
        
        logger.info("Utils initialized with setDocCount={}, forceSingleSet={}", setDocCount, forceSingleSet);
    }
    
    /**
     * Inner class to track processing results
     * Made public for GlobalProcessingReport integration
     */
    public static class ProcessingResult {
        public int totalClaimNumbers = 0;
        public int totalDocuments = 0;
        public int indexedSuccessCount = 0;
        public int indexedFailureCount = 0;
        public int packagingFilesCreated = 0;
        public Set<String> batchIDsCreated = new LinkedHashSet<>();
        
        /**
         * Merge another result into this one (for multi-threaded aggregation)
         */
        public void merge(ProcessingResult other) {
            this.totalClaimNumbers += other.totalClaimNumbers;
            this.totalDocuments += other.totalDocuments;
            this.indexedSuccessCount += other.indexedSuccessCount;
            this.indexedFailureCount += other.indexedFailureCount;
            this.packagingFilesCreated += other.packagingFilesCreated;
            this.batchIDsCreated.addAll(other.batchIDsCreated);
        }
    }
    
    // ==================== STEP 2-5: PROCESS CSV FILE ====================
    
    /**
     * Main method to process a single CSV file from Indexing\Data folder
     * Steps:
     * - Read CSV and extract claim numbers
     * - Process each claim individually:
     *   * Query DB for documents of one claim
     *   * Apply batching/indexing logic
     *   * Update DB
     * - Create success/failed tracking files
     * - Generate packaging CSV files
     * 
     * @return ProcessingResult with statistics
     */
    public ProcessingResult processCSVFile(Path csvFilePath) {
        String csvFileName = csvFilePath.getFileName().toString();
        logger.info("===== PROCESSING CSV FILE: {} =====", csvFileName);
        
        ProcessingResult result = new ProcessingResult();
        List<String> successLines = new ArrayList<>();
        List<String> failedLines = new ArrayList<>();
        
        try {
            // STEP 3: Read CSV and extract unique claim numbers
            Set<String> claimNumbers = extractClaimNumbersFromCSV(csvFilePath);
            result.totalClaimNumbers = claimNumbers.size();
            logger.info("Extracted {} unique claim numbers from CSV", claimNumbers.size());
            
            if (claimNumbers.isEmpty()) {
                logger.warn("No claim numbers found in CSV file. Skipping...");
                archiveCSVFile(csvFilePath);
                return result;
            }
            
            // STEP 3: Process each claim individually
            int claimIndex = 0;
            for (String claimNumber : claimNumbers) {
                claimIndex++;
                logger.info("Processing claim {}/{}: {}", claimIndex, claimNumbers.size(), claimNumber);
                
                // Query DB for documents of this specific claim
                List<ClaimCenterDocumentDTO> documents = fetchDocumentsForOneClaim(claimNumber);
                
                if (documents.isEmpty()) {
                    logger.warn("No documents found for claim: {} (isDataMerged=1)", claimNumber);
                    continue;
                }
                
                logger.info("Retrieved {} documents for claim: {}", documents.size(), claimNumber);
                result.totalDocuments += documents.size();
                
                // Apply batching and indexing logic (one batch per claim or multiple based on config)
                applyBatchingLogicForClaim(documents, result, claimNumber);
                
                // Update database with indexing metadata
                updateDatabaseWithIndexing(documents, successLines, failedLines, result);
                
                logger.info("Completed claim: {} - Batches created: {}", claimNumber, 
                           result.batchIDsCreated.size() - (claimIndex - 1));
            }
            
            logger.info("Created total {} batches with IDs: {}", result.batchIDsCreated.size(), result.batchIDsCreated);
            logger.info("Database update complete. Success: {}, Failed: {}", 
                       result.indexedSuccessCount, result.indexedFailureCount);
            
            // STEP 5a: Write success/failed tracking files
            writeTrackingFiles(csvFileName, successLines, failedLines);
            
            // STEP 5b: Generate packaging CSV files
            generatePackagingFiles(result.batchIDsCreated, result);
            logger.info("Generated {} packaging file(s)", result.packagingFilesCreated);
            
            // Archive processed CSV file
            archiveCSVFile(csvFilePath);
            
            logger.info("===== COMPLETED CSV FILE: {} =====", csvFileName);
            logger.info("Summary: Claims={}, Docs={}, Indexed={}, Failed={}, PackageFiles={}", 
                       result.totalClaimNumbers, result.totalDocuments, 
                       result.indexedSuccessCount, result.indexedFailureCount, result.packagingFilesCreated);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Critical error processing CSV file: {}", csvFileName, e);
            // Move to failed folder
            try {
                Path failedDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.FAILED_FOLDER);
                Path failedFile = failedDir.resolve(csvFileName);
                Files.move(csvFilePath, failedFile, StandardCopyOption.REPLACE_EXISTING);
                createErrorLogFile(failedDir, csvFileName, "Critical error: " + e.getMessage());
            } catch (IOException ioe) {
                logger.error("Failed to move file to Failed folder", ioe);
            }
            
            // Return empty result on critical error
            return new ProcessingResult();
        }
    }
    
    // ==================== STEP 3: EXTRACT CLAIM NUMBERS ====================
    
    /**
     * Read CSV file and extract unique claim numbers
     * Respects app.skipHeaderRow property to handle CSV files with/without headers
     * Expects CSV to have a 'claimNumber' or 'ClaimNumber' column
     */
    private Set<String> extractClaimNumbersFromCSV(Path csvFilePath) throws IOException {
        Set<String> claimNumbers = new LinkedHashSet<>();
        
        // Check if we should skip header row (from properties)
        String skipHeaderProp = config.getProperty("app.skipHeaderRow");
        boolean skipHeaderRow = (skipHeaderProp == null) ? true : Boolean.parseBoolean(skipHeaderProp);
        
        logger.debug("Processing CSV with skipHeaderRow={}", skipHeaderRow);
        
        try (BufferedReader reader = new BufferedReader(new FileReader(csvFilePath.toFile()))) {
            
            String headerLine = null;
            int claimNumberIndex = -1;
            
            if (skipHeaderRow) {
                // Read header to find claimNumber column index
                headerLine = reader.readLine();
                if (headerLine == null || headerLine.trim().isEmpty()) {
                    logger.warn("CSV file has no header line: {}", csvFilePath.getFileName());
                    return claimNumbers;
                }
                
                String[] headers = headerLine.split("\\|", -1);
                
                // Find claimNumber column (case-insensitive, supports both camelCase and UNDERSCORE formats)
                for (int i = 0; i < headers.length; i++) {
                    String header = headers[i].trim().replace("\"", "");
                    String normalizedHeader = header.toUpperCase().replace("_", "");
                    
                    // Support both "claimNumber" and "CLAIM_NUMBER" formats
                    if (normalizedHeader.equals("CLAIMNUMBER")) {
                        claimNumberIndex = i;
                        logger.debug("Found claimNumber column at index: {} (header: '{}')", i, header);
                        break;
                    }
                }
                
                if (claimNumberIndex == -1) {
                    logger.error("CSV file does not have 'claimNumber' or 'CLAIM_NUMBER' column in header: {}", csvFilePath.getFileName());
                    logger.error("Available headers: {}", String.join(", ", headers));
                    logger.error("Supported formats: claimNumber, ClaimNumber, CLAIM_NUMBER, claim_number");
                    return claimNumbers;
                }
            } else {
                // No header row - assume claimNumber is first column (index 0)
                claimNumberIndex = 0;
                logger.warn("No header row - assuming claimNumber is at index 0");
            }
            
            // Read data rows and extract claim numbers
            String line;
            int rowNum = 0;
            while ((line = reader.readLine()) != null) {
                rowNum++;
                
                // Skip empty lines
                if (line.trim().isEmpty()) {
                    logger.debug("Skipping empty line at row {}", rowNum);
                    continue;
                }
                
                String[] cells = line.split("\\|", -1);
                if (cells.length > claimNumberIndex) {
                    String claimNumber = safeTrim(cells[claimNumberIndex]);
                    if (!claimNumber.isEmpty()) {
                        claimNumbers.add(claimNumber);
                        logger.debug("Row {}: Found claimNumber '{}'", rowNum, claimNumber);
                    } else {
                        logger.debug("Row {}: Empty claimNumber, skipping", rowNum);
                    }
                } else {
                    logger.warn("Row {}: Insufficient columns (expected > {}, got {})", 
                               rowNum, claimNumberIndex, cells.length);
                }
            }
        }
        
        logger.info("Extracted {} unique claim number(s) from CSV (total rows processed)", claimNumbers.size());
        return claimNumbers;
    }
    
    // ==================== STEP 3: FETCH DOCUMENTS FROM DB ====================
    
    /**
     * Query database for documents of a single claim with isDataMerged = 1
     * This method is called per claim to support one-batch-per-claim logic
     */
    private List<ClaimCenterDocumentDTO> fetchDocumentsForOneClaim(String claimNumber) throws SQLException {
        List<ClaimCenterDocumentDTO> documents = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectDocumentsByClaimQuery)) {
            
            pstmt.setString(1, claimNumber);
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    // Core fields
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    
                    // Packaging fields
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    documents.add(dto);
                }
            }
        }
        
        logger.debug("Fetched {} documents for claimNumber: {}", documents.size(), claimNumber);
        return documents;
    }
    
    // ==================== STEP 3: APPLY BATCHING LOGIC ====================
    
    /**
     * Apply batching and set logic to documents for a single claim
     * 
     * Case 1: forceSingleSet = true
     *   - Create multiple batches, each with 1 set (max setDocCount docs per batch)
     *   - Example: 27 docs → Batch1(25 docs, 1 set) + Batch2(2 docs, 1 set)
     * 
     * Case 2: forceSingleSet = false
     *   - Create one batch per claim with multiple sets (max setDocCount docs per set)
     *   - Example: 27 docs → Batch1(27 docs, Set1: 25 docs + Set2: 2 docs)
     * 
     * @param documents List of documents for this claim
     * @param result ProcessingResult to track batch IDs
     * @param claimNumber Current claim number being processed
     */
    private void applyBatchingLogicForClaim(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        logger.debug("Applying batching logic for claim: {} with {} documents (forceSingleSet={})", 
                    claimNumber, totalDocs, forceSingleSet);
        
        if (forceSingleSet) {
            // CASE 1: Create multiple batches, each with 1 set (max setDocCount docs per batch)
            applyForceSingleSetLogic(documents, result, claimNumber);
        } else {
            // CASE 2: Create one batch per claim with multiple sets
            applyMultiSetPerBatchLogic(documents, result, claimNumber);
        }
    }
    
    /**
     * Case 1: Force single set per batch
     * - Create multiple batches for the claim
     * - Each batch contains exactly 1 set (max setDocCount docs)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 25 docs (setID=1, setDocCount=25, batchDocCount=25)
     *   Batch 2: 2 docs (setID=1, setDocCount=2, batchDocCount=2)
     */
    private void applyForceSingleSetLogic(List<ClaimCenterDocumentDTO> documents, 
                                          ProcessingResult result, 
                                          String claimNumber) {
        
        int totalDocs = documents.size();
        int currentBatchStartIndex = 0;
        int batchNumber = 1;
        
        while (currentBatchStartIndex < totalDocs) {
            // Determine batch size (max setDocCount)
            int currentBatchSize = Math.min(setDocCount, totalDocs - currentBatchStartIndex);
            int batchEndIndex = currentBatchStartIndex + currentBatchSize;
            
            // Generate unique batch ID and job ID
            String batchID = generateBatchID();
            String jobID = generateJobID();
            result.batchIDsCreated.add(batchID);
            
            logger.info("Claim: {} - Batch {}: batchID={}, docs={}, sets=1", 
                       claimNumber, batchNumber, batchID, currentBatchSize);
            
            // Process documents in current batch (single set)
            int setDocIndex = 1;
            
            for (int i = currentBatchStartIndex; i < batchEndIndex; i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                // Set batch-level metadata
                doc.setBatchID(batchID);
                doc.setJobID(jobID);
                doc.setBatchDocCount(currentBatchSize);
                
                // Set set-level metadata (always setID = 1 for this case)
                doc.setSetID(1);
                doc.setSetDocCount(currentBatchSize);
                doc.setSetDocIndex(setDocIndex);
                
                setDocIndex++;
            }
            
            // Move to next batch
            currentBatchStartIndex = batchEndIndex;
            batchNumber++;
        }
        
        logger.info("Claim: {} - Created {} batches (forceSingleSet=true)", claimNumber, batchNumber - 1);
    }
    
    /**
     * Case 2: Multiple sets per batch
     * - Create one batch for the claim
     * - Batch contains multiple sets (max setDocCount docs per set)
     * 
     * Example for 27 documents with setDocCount=25:
     *   Batch 1: 27 docs (batchDocCount=27)
     *     - Set 1: setID=1, setDocCount=25, setDocIndex=1-25
     *     - Set 2: setID=2, setDocCount=2, setDocIndex=1-2
     */
    private void applyMultiSetPerBatchLogic(List<ClaimCenterDocumentDTO> documents, 
                                            ProcessingResult result, 
                                            String claimNumber) {
        
        int totalDocs = documents.size();
        
        // Generate unique batch ID and job ID (one per claim)
        String batchID = generateBatchID();
        String jobID = generateJobID();
        result.batchIDsCreated.add(batchID);
        
        // Calculate number of sets needed
        int numberOfSets = (int) Math.ceil((double) totalDocs / setDocCount);
        
        logger.info("Claim: {} - Batch: batchID={}, docs={}, sets={}", 
                   claimNumber, batchID, totalDocs, numberOfSets);
        
        // Process all documents in this single batch
        int setID = 1;
        int setDocIndex = 1;
        int docsInCurrentSet = 0;
        
        for (ClaimCenterDocumentDTO doc : documents) {
            docsInCurrentSet++;
            
            // Calculate set doc count for current set
            int remainingDocs = totalDocs - ((setID - 1) * setDocCount);
            int currentSetDocCount = Math.min(setDocCount, remainingDocs);
            
            // Set batch-level metadata
            doc.setBatchID(batchID);
            doc.setJobID(jobID);
            doc.setBatchDocCount(totalDocs);
            
            // Set set-level metadata
            doc.setSetID(setID);
            doc.setSetDocCount(currentSetDocCount);
            doc.setSetDocIndex(setDocIndex);
            
            // Increment set document index
            setDocIndex++;
            
            // Move to next set if current set is full
            if (setDocIndex > setDocCount) {
                logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
                setID++;
                setDocIndex = 1;
                docsInCurrentSet = 0;
            }
        }
        
        // Log last set if it has documents
        if (docsInCurrentSet > 0) {
            logger.debug("Claim: {} - Completed Set {}: {} documents", claimNumber, setID, docsInCurrentSet);
        }
        
        logger.info("Claim: {} - Created 1 batch with {} sets (forceSingleSet=false)", claimNumber, numberOfSets);
    }
    
    /**
     * Generate unique batch ID: wawa_<yyyyMMddHHmmssSSSSS>
     */
    private String generateBatchID() {
        return Constants.BATCH_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate unique job ID: wawa_migrate_<yyyyMMddHHmmssSSSSS>
     */
    private String generateJobID() {
        return Constants.JOB_ID_PREFIX + generateTimestamp();
    }
    
    /**
     * Generate timestamp with milliseconds: yyyyMMddHHmmssSSSSS
     */
    private String generateTimestamp() {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMddHHmmssSSSSS");
        return sdf.format(new Date());
    }
    
    // ==================== STEP 4: UPDATE DATABASE ====================
    
    /**
     * Update database with indexing metadata using JDBC batch processing
     * Falls back to individual UPDATEs if batch fails
     */
    private void updateDatabaseWithIndexing(List<ClaimCenterDocumentDTO> documents, 
                                           List<String> successLines, 
                                           List<String> failedLines,
                                           ProcessingResult result) {
        
        // Get batch size from config (default 1000)
        String batchSizeStr = config.getProperty("db.indexing.batch.update.size");
        int batchSize = (batchSizeStr != null) ? Integer.parseInt(batchSizeStr) : 1000;
        
        try {
            // Try batch UPDATE first (fast path)
            updateDatabaseWithIndexingBatch(documents, successLines, failedLines, result, batchSize);
            
        } catch (SQLException e) {
            logger.warn("Batch UPDATE failed, falling back to individual UPDATEs: {}", e.getMessage());
            // Fallback to individual UPDATEs
            updateDatabaseWithIndexingIndividual(documents, successLines, failedLines, result);
        }
    }
    
    /**
     * Batch UPDATE implementation (100x faster than individual UPDATEs)
     */
    private void updateDatabaseWithIndexingBatch(List<ClaimCenterDocumentDTO> documents,
                                                 List<String> successLines,
                                                 List<String> failedLines,
                                                 ProcessingResult result,
                                                 int batchSize) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            int count = 0;
            int batchStartIndex = 0;
            
            for (int i = 0; i < documents.size(); i++) {
                ClaimCenterDocumentDTO doc = documents.get(i);
                
                pstmt.setInt(1, doc.getBatchDocCount());
                pstmt.setString(2, doc.getBatchID());
                pstmt.setString(3, doc.getJobID());
                pstmt.setInt(4, doc.getSetDocCount());
                pstmt.setInt(5, doc.getSetID());
                pstmt.setInt(6, doc.getSetDocIndex());
                pstmt.setString(7, doc.getExternalID());
                pstmt.addBatch();
                count++;
                
                // Execute batch when size reached
                if (count % batchSize == 0) {
                    int[] results = pstmt.executeBatch();
                    conn.commit();
                    
                    // Track results
                    for (int j = 0; j < results.length; j++) {
                        ClaimCenterDocumentDTO batchDoc = documents.get(batchStartIndex + j);
                        if (results[j] > 0) {
                            result.indexedSuccessCount++;
                            successLines.add(batchDoc.getExternalID());
                        } else {
                            result.indexedFailureCount++;
                            failedLines.add(batchDoc.getExternalID() + "|No record found");
                        }
                    }
                    
                    logger.debug("Executed batch of {} indexing UPDATEs", count);
                    batchStartIndex = i + 1;
                }
            }
            
            // Execute remaining batch
            if (count % batchSize != 0) {
                int[] results = pstmt.executeBatch();
                conn.commit();
                
                // Track results for remaining batch
                int remainingStart = (count / batchSize) * batchSize;
                for (int j = 0; j < results.length; j++) {
                    ClaimCenterDocumentDTO batchDoc = documents.get(remainingStart + j);
                    if (results[j] > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(batchDoc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(batchDoc.getExternalID() + "|No record found");
                    }
                }
                
                logger.debug("Executed final batch of {} indexing UPDATEs", count % batchSize);
            }
            
            logger.info("Batch indexing UPDATE completed: {} documents indexed, {} failed", 
                       result.indexedSuccessCount, result.indexedFailureCount);
                       
        } catch (SQLException e) {
            logger.error("Batch indexing UPDATE failed", e);
            throw e; // Re-throw to trigger fallback
        }
    }
    
    /**
     * Individual UPDATE fallback (for when batch fails)
     */
    private void updateDatabaseWithIndexingIndividual(List<ClaimCenterDocumentDTO> documents,
                                                      List<String> successLines,
                                                      List<String> failedLines,
                                                      ProcessingResult result) {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateIndexingQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (fallback mode)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            
            for (ClaimCenterDocumentDTO doc : documents) {
                try {
                    pstmt.setInt(1, doc.getBatchDocCount());
                    pstmt.setString(2, doc.getBatchID());
                    pstmt.setString(3, doc.getJobID());
                    pstmt.setInt(4, doc.getSetDocCount());
                    pstmt.setInt(5, doc.getSetID());
                    pstmt.setInt(6, doc.getSetDocIndex());
                    pstmt.setString(7, doc.getExternalID());
                    
                    int rowsUpdated = pstmt.executeUpdate();
                    
                    if (rowsUpdated > 0) {
                        result.indexedSuccessCount++;
                        successLines.add(doc.getExternalID());
                    } else {
                        result.indexedFailureCount++;
                        failedLines.add(doc.getExternalID() + "|No record found");
                    }
                    
                } catch (SQLException e) {
                    result.indexedFailureCount++;
                    failedLines.add(doc.getExternalID() + "|" + e.getMessage());
                    logger.error("Failed to index: externalID={}", doc.getExternalID(), e);
                }
            }
            
            conn.commit();
            logger.info("Individual indexing UPDATE completed (fallback)");
            
        } catch (SQLException e) {
            logger.error("Individual UPDATE fallback also failed", e);
            // Connection auto-closes and rolls back due to auto-commit=false
            throw new RuntimeException("Database update failed", e);
        }
    }
    
    // ==================== STEP 5a: WRITE TRACKING FILES ====================
    
    /**
     * Write success and failed tracking files
     */
    private void writeTrackingFiles(String csvFileName, List<String> successLines, List<String> failedLines) 
            throws IOException {
        
        String baseFileName = csvFileName.replace(".csv", "");
        Path indexingPath = Paths.get(basePath, Constants.INDEXING_FOLDER);
        
        // Write success file
        if (!successLines.isEmpty()) {
            Path successFile = indexingPath.resolve(Constants.COMPLETED_FOLDER)
                                          .resolve(baseFileName + Constants.INDEX_SUCCESS_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(successFile.toFile()))) {
                writer.write("externalID");
                writer.newLine();
                for (String line : successLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created success tracking file: {} ({} records)", successFile.getFileName(), successLines.size());
        }
        
        // Write failed file
        if (!failedLines.isEmpty()) {
            Path failedFile = indexingPath.resolve(Constants.FAILED_FOLDER)
                                         .resolve(baseFileName + Constants.INDEX_FAILED_SUFFIX);
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(failedFile.toFile()))) {
                writer.write("externalID|error_message");
                writer.newLine();
                for (String line : failedLines) {
                    writer.write(line);
                    writer.newLine();
                }
            }
            logger.info("Created failed tracking file: {} ({} records)", failedFile.getFileName(), failedLines.size());
        }
    }
    
    // ==================== STEP 5b: GENERATE PACKAGING FILES ====================
    
    /**
     * Generate packaging CSV files for all batches created
     * - Query DB for documents with created batchIDs
     * - Write pipe-delimited CSV with 33 columns
     * - Max 50,000 rows per file
     * - Update isProcessed = 1, DateProcessed = current date
     */
    private void generatePackagingFiles(Set<String> batchIDs, ProcessingResult result) throws SQLException, IOException {
        
        if (batchIDs.isEmpty()) {
            logger.warn("No batchIDs to process for packaging");
            return;
        }
        
        // Build IN clause for batch IDs
        StringBuilder inClause = new StringBuilder();
        int count = 0;
        for (int i = 0; i < batchIDs.size(); i++) {
            if (count > 0) inClause.append(",");
            inClause.append("?");
            count++;
        }
        
        // Replace the %IN_CLAUSE% placeholder with the generated IN clause
        String selectSQL = selectPackagingByBatchIdsQueryTemplate.replace("%IN_CLAUSE%", inClause.toString());
        
        List<ClaimCenterDocumentDTO> packagingDocs = new ArrayList<>();
        
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(selectSQL)) {
            
            // Set parameters
            int paramIndex = 1;
            for (String batchID : batchIDs) {
                pstmt.setString(paramIndex++, batchID);
            }
            
            try (ResultSet rs = pstmt.executeQuery()) {
                while (rs.next()) {
                    ClaimCenterDocumentDTO dto = new ClaimCenterDocumentDTO();
                    
                    dto.setExternalID(rs.getString("externalID"));
                    dto.setClaimNumber(rs.getString("claimNumber"));
                    dto.setClaimID(rs.getString("claimID"));
                    dto.setGwDocumentID(rs.getString("gwDocumentID"));
                    dto.setAmount(rs.getString("amount"));
                    dto.setAuthor(rs.getString("author"));
                    dto.setBatchDocCount(rs.getInt("batchDocCount"));
                    dto.setBatchID(rs.getString("batchID"));
                    dto.setClaimant(rs.getString("claimant"));
                    dto.setCoverage(rs.getString("coverage"));
                    dto.setCustomerID(rs.getString("customerID"));
                    dto.setDocumentDescription(rs.getString("documentDescription"));
                    dto.setDocumentSubtype(rs.getString("documentSubtype"));
                    dto.setDocumentTitle(rs.getString("documentTitle"));
                    dto.setDocumentType(rs.getString("documentType"));
                    dto.setDoNotCreateActivity(rs.getBoolean("doNotCreateActivity") ? "true" : "false");
                    dto.setDuplicate(rs.getString("duplicate"));
                    dto.setExposureID(rs.getString("exposureID"));
                    dto.setHidden(rs.getString("hidden"));
                    dto.setInputMethod(rs.getString("inputMethod"));
                    dto.setInsuredName(rs.getString("insuredName"));
                    dto.setJobID(rs.getString("jobID"));
                    dto.setMimeType(rs.getString("mimeType"));
                    dto.setOrigDateCreated(rs.getString("OrigDateCreated"));
                    dto.setPolicyNumber(rs.getString("policyNumber"));
                    dto.setPrimaryMembershipNumber(rs.getString("primaryMembershipNumber"));
                    dto.setReviewed(rs.getString("reviewed"));
                    dto.setSensitive(rs.getBoolean("sensitive") ? "true" : "false");
                    dto.setSetDocCount(rs.getInt("setDocCount"));
                    dto.setSetID(rs.getInt("setID"));
                    dto.setSetDocIndex(rs.getInt("SetDocIndex"));
                    dto.setContentFilePath(rs.getString("contentFilePath"));
                    dto.setContentRetrievalName(rs.getString("contentRetrievalName"));
                    
                    packagingDocs.add(dto);
                }
            }
        }
        
        logger.info("Retrieved {} documents for packaging", packagingDocs.size());
        
        // Write packaging CSV files (max 50K rows per file)
        writePackagingCSVFiles(packagingDocs, result);
        
        // Update isProcessed flag in database
        updateProcessedFlag(packagingDocs);
    }
    
    /**
     * Write packaging CSV files with max 50,000 rows per file
     */
    private void writePackagingCSVFiles(List<ClaimCenterDocumentDTO> documents, ProcessingResult result) 
            throws IOException {
        
        Path packagingDir = Paths.get(basePath, Constants.PACKAGING_FOLDER);
        
        int fileCount = 0;
        int rowCount = 0;
        BufferedWriter writer = null;
        
        try {
            for (ClaimCenterDocumentDTO doc : documents) {
                // Create new file if needed
                if (writer == null || rowCount >= Constants.MAX_PACKAGING_FILE_ROWS) {
                    // Close previous file
                    if (writer != null) {
                        writer.close();
                        logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
                    }
                    
                    // Create new file
                    fileCount++;
                    rowCount = 0;
                    String fileName = Constants.PACKAGING_FILE_PREFIX + generateTimestamp() + 
                                    Constants.PACKAGING_FILE_EXTENSION;
                    Path filePath = packagingDir.resolve(fileName);
                    writer = new BufferedWriter(new FileWriter(filePath.toFile()));
                    
                    // Write header
                    writer.write(ClaimCenterDocumentDTO.getPackagingCSVHeader());
                    writer.newLine();
                    
                    logger.info("Created packaging file: {}", fileName);
                }
                
                // Write document row
                writer.write(doc.toPackagingCSVLine());
                writer.newLine();
                rowCount++;
            }
            
            // Close last file
            if (writer != null) {
                writer.close();
                logger.info("Closed packaging file {} with {} rows", fileCount, rowCount);
            }
            
            result.packagingFilesCreated = fileCount;
            
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    logger.error("Error closing packaging file", e);
                }
            }
        }
    }
    
    /**
     * Update isProcessed = 1 and DateProcessed for packaged documents
     */
    private void updateProcessedFlag(List<ClaimCenterDocumentDTO> documents) throws SQLException {
        
        long connStartTime = System.currentTimeMillis();
        try (Connection conn = connectionManager.getConnection();
             PreparedStatement pstmt = conn.prepareStatement(updateProcessedQuery)) {
            
            long connAcquireTime = System.currentTimeMillis() - connStartTime;
            logger.debug("Connection acquired in {} ms (updateProcessedFlag)", connAcquireTime);
            
            // Note: conn.setAutoCommit(false) already set by ConnectionManager
            Date currentDate = new Date();
            
            for (ClaimCenterDocumentDTO doc : documents) {
                pstmt.setTimestamp(1, new java.sql.Timestamp(currentDate.getTime()));
                pstmt.setString(2, doc.getExternalID());
                pstmt.addBatch();
            }
            
            int[] results = pstmt.executeBatch();
            conn.commit();
            
            int successCount = 0;
            for (int result : results) {
                if (result > 0) successCount++;
            }
            
            logger.info("Updated isProcessed flag for {} documents", successCount);
        }
    }
    
    // ==================== HELPER METHODS ====================
    
    /**
     * Archive CSV file to Archive folder
     */
    private void archiveCSVFile(Path csvFilePath) throws IOException {
        Path archiveDir = Paths.get(basePath, Constants.INDEXING_FOLDER, Constants.ARCHIVE_FOLDER);
        Path archiveFile = archiveDir.resolve(csvFilePath.getFileName());
        Files.move(csvFilePath, archiveFile, StandardCopyOption.REPLACE_EXISTING);
        logger.info("Archived CSV file to: {}", archiveFile);
    }
    
    /**
     * Create error log file
     */
    private void createErrorLogFile(Path directory, String csvFileName, String errorMessage) {
        try {
            String baseFileName = csvFileName.replace(".csv", "");
            Path errorLogFile = directory.resolve(baseFileName + "_error.log");
            
            try (BufferedWriter writer = new BufferedWriter(new FileWriter(errorLogFile.toFile()))) {
                writer.write("Error processing file: " + csvFileName);
                writer.newLine();
                writer.write("Timestamp: " + new Date());
                writer.newLine();
                writer.write("Error: " + errorMessage);
                writer.newLine();
            }
            
            logger.info("Created error log file: {}", errorLogFile);
        } catch (IOException e) {
            logger.error("Failed to create error log file", e);
        }
    }
    
    /**
     * Safe trim utility
     */
    private String safeTrim(String value) {
        if (value == null) return "";
        String trimmed = value.trim();
        if (trimmed.equalsIgnoreCase("null")) return "";
        return trimmed;
    }
}




package com.wawanesa.ace.index.utils;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.Duration;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Global processing report for tracking statistics across all CSV files
 * Thread-safe implementation using atomic counters
 */
public class GlobalProcessingReport {
    
    private static final Logger logger = LogManager.getLogger(GlobalProcessingReport.class);
    
    // CSV file-level counters
    private final AtomicInteger totalCSVFilesQueued = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesProcessed = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesSuccess = new AtomicInteger(0);
    private final AtomicInteger totalCSVFilesFailed = new AtomicInteger(0);
    
    // Claim-level counters
    private final AtomicInteger totalClaimsProcessed = new AtomicInteger(0);
    
    // Document-level counters
    private final AtomicLong totalDocumentsProcessed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexed = new AtomicLong(0);
    private final AtomicLong totalDocumentsIndexFailed = new AtomicLong(0);
    private final AtomicLong totalDocumentsPackaged = new AtomicLong(0);
    
    // Batch counters
    private final AtomicInteger totalBatchesCreated = new AtomicInteger(0);
    private final AtomicInteger totalPackagingFilesCreated = new AtomicInteger(0);
    
    // Thread pool statistics
    private final AtomicInteger currentThreadPoolSize = new AtomicInteger(0);
    private final AtomicInteger currentActiveThreads = new AtomicInteger(0);
    private final AtomicInteger currentQueueSize = new AtomicInteger(0);
    
    // Timing
    private final LocalDateTime startTime;
    private LocalDateTime endTime;
    
    public GlobalProcessingReport() {
        this.startTime = LocalDateTime.now();
    }
    
    /**
     * Record that a CSV file has been queued for processing
     */
    public void incrementCSVFilesQueued() {
        totalCSVFilesQueued.incrementAndGet();
    }
    
    /**
     * Record successful CSV file processing
     */
    public void recordCSVFileSuccess(String fileName, Utils.ProcessingResult result) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesSuccess.incrementAndGet();
        
        totalClaimsProcessed.addAndGet(result.totalClaimNumbers);
        totalDocumentsProcessed.addAndGet(result.totalDocuments);
        totalDocumentsIndexed.addAndGet(result.indexedSuccessCount);
        totalDocumentsIndexFailed.addAndGet(result.indexedFailureCount);
        totalBatchesCreated.addAndGet(result.batchIDsCreated.size());
        totalPackagingFilesCreated.addAndGet(result.packagingFilesCreated);
        
        logger.info("CSV completed: {} | Claims={}, Docs={}, Indexed={}, Failed={}, Batches={}, PkgFiles={}", 
                   fileName, result.totalClaimNumbers, result.totalDocuments, 
                   result.indexedSuccessCount, result.indexedFailureCount,
                   result.batchIDsCreated.size(), result.packagingFilesCreated);
    }
    
    /**
     * Record failed CSV file processing
     */
    public void recordCSVFileFailure(String fileName, String errorMessage) {
        totalCSVFilesProcessed.incrementAndGet();
        totalCSVFilesFailed.incrementAndGet();
        
        logger.error("CSV failed: {} | Error: {}", fileName, errorMessage);
    }
    
    /**
     * Update thread pool statistics
     */
    public void updateThreadPoolStats(int poolSize, int activeThreads, int queueSize) {
        currentThreadPoolSize.set(poolSize);
        currentActiveThreads.set(activeThreads);
        currentQueueSize.set(queueSize);
    }
    
    /**
     * Get total CSV files queued for processing
     */
    public int getCsvFilesQueued() {
        return totalCSVFilesQueued.get();
    }
    
    /**
     * Log current progress
     */
    public void logProgress() {
        Duration elapsed = Duration.between(startTime, LocalDateTime.now());
        long elapsedMinutes = elapsed.toMinutes();
        long elapsedSeconds = elapsed.getSeconds() % 60;
        
        logger.info("========== PROGRESS REPORT ==========");
        logger.info("Elapsed Time: {} min {} sec", elapsedMinutes, elapsedSeconds);
        logger.info("CSV Files: Queued={}, Processed={}/{}, Success={}, Failed={}", 
                   totalCSVFilesQueued.get(), totalCSVFilesProcessed.get(), totalCSVFilesQueued.get(),
                   totalCSVFilesSuccess.get(), totalCSVFilesFailed.get());
        logger.info("Claims: Processed={}", totalClaimsProcessed.get());
        logger.info("Documents: Processed={}, Indexed={}, Failed={}", 
                   totalDocumentsProcessed.get(), totalDocumentsIndexed.get(), totalDocumentsIndexFailed.get());
        logger.info("Batches: Created={}, Packaging Files={}", 
                   totalBatchesCreated.get(), totalPackagingFilesCreated.get());
        
        // Log memory statistics
        MemoryMonitor.logMemoryStats();
        
        logger.info("=====================================");
    }
    
    /**
     * Get summary as string
     */
    public String getSummary() {
        markCompleted();
        
        Duration totalDuration = Duration.between(startTime, endTime);
        long minutes = totalDuration.toMinutes();
        long seconds = totalDuration.getSeconds() % 60;
        
        StringBuilder sb = new StringBuilder();
        sb.append("\n========== FINAL SUMMARY ==========\n");
        sb.append(String.format("Start Time: %s\n", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("End Time: %s\n", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
        sb.append(String.format("Total Duration: %d min %d sec\n", minutes, seconds));
        sb.append("\n--- CSV File Statistics ---\n");
        sb.append(String.format("Total CSV Files Queued: %d\n", totalCSVFilesQueued.get()));
        sb.append(String.format("Total CSV Files Processed: %d\n", totalCSVFilesProcessed.get()));
        sb.append(String.format("CSV Files Success: %d\n", totalCSVFilesSuccess.get()));
        sb.append(String.format("CSV Files Failed: %d\n", totalCSVFilesFailed.get()));
        sb.append("\n--- Claim Statistics ---\n");
        sb.append(String.format("Total Claims Processed: %d\n", totalClaimsProcessed.get()));
        sb.append("\n--- Document Statistics ---\n");
        sb.append(String.format("Total Documents Processed: %d\n", totalDocumentsProcessed.get()));
        sb.append(String.format("Total Documents Indexed: %d\n", totalDocumentsIndexed.get()));
        sb.append(String.format("Total Documents Index Failed: %d\n", totalDocumentsIndexFailed.get()));
        sb.append("\n--- Batch & Packaging Statistics ---\n");
        sb.append(String.format("Total Batches Created: %d\n", totalBatchesCreated.get()));
        sb.append(String.format("Total Packaging Files Created: %d\n", totalPackagingFilesCreated.get()));
        
        // Calculate throughput
        if (totalDuration.getSeconds() > 0) {
            long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
            long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
            sb.append("\n--- Performance ---\n");
            sb.append(String.format("Throughput: %d docs/sec, %d docs/min\n", docsPerSecond, docsPerMinute));
        }
        
        sb.append("===================================\n");
        return sb.toString();
    }
    
    /**
     * Mark processing as completed
     */
    public void markCompleted() {
        if (this.endTime == null) {
            this.endTime = LocalDateTime.now();
        }
    }
    
    /**
     * Write final report to CSV file
     */
    public void writeReportToFile(String basePath) throws IOException {
        markCompleted();
        
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String fileName = String.format("CCDataIndexPackaging_Summary_Report_%s.csv", timestamp);
        Path reportPath = Paths.get(basePath, "SummaryReports", fileName);
        
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(reportPath.toFile()))) {
            // Write header
            writer.write("Metric,Value");
            writer.newLine();
            
            // Write timing information
            writer.write(String.format("Start Time,%s", startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            writer.write(String.format("End Time,%s", endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))));
            writer.newLine();
            
            Duration totalDuration = Duration.between(startTime, endTime);
            writer.write(String.format("Total Duration (minutes),%d", totalDuration.toMinutes()));
            writer.newLine();
            writer.write(String.format("Total Duration (seconds),%d", totalDuration.getSeconds()));
            writer.newLine();
            
            // Write CSV file statistics
            writer.newLine();
            writer.write("--- CSV File Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total CSV Files Queued,%d", totalCSVFilesQueued.get()));
            writer.newLine();
            writer.write(String.format("Total CSV Files Processed,%d", totalCSVFilesProcessed.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Success,%d", totalCSVFilesSuccess.get()));
            writer.newLine();
            writer.write(String.format("CSV Files Failed,%d", totalCSVFilesFailed.get()));
            writer.newLine();
            
            // Write claim statistics
            writer.newLine();
            writer.write("--- Claim Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Claims Processed,%d", totalClaimsProcessed.get()));
            writer.newLine();
            
            // Write document statistics
            writer.newLine();
            writer.write("--- Document Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Documents Processed,%d", totalDocumentsProcessed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Indexed,%d", totalDocumentsIndexed.get()));
            writer.newLine();
            writer.write(String.format("Total Documents Index Failed,%d", totalDocumentsIndexFailed.get()));
            writer.newLine();
            
            // Write batch statistics
            writer.newLine();
            writer.write("--- Batch & Packaging Statistics ---,");
            writer.newLine();
            writer.write(String.format("Total Batches Created,%d", totalBatchesCreated.get()));
            writer.newLine();
            writer.write(String.format("Total Packaging Files Created,%d", totalPackagingFilesCreated.get()));
            writer.newLine();
            
            // Write performance metrics
            if (totalDuration.getSeconds() > 0) {
                long docsPerSecond = totalDocumentsProcessed.get() / totalDuration.getSeconds();
                long docsPerMinute = totalDocumentsProcessed.get() / Math.max(1, totalDuration.toMinutes());
                
                writer.newLine();
                writer.write("--- Performance Metrics ---,");
                writer.newLine();
                writer.write(String.format("Documents Per Second,%d", docsPerSecond));
                writer.newLine();
                writer.write(String.format("Documents Per Minute,%d", docsPerMinute));
                writer.newLine();
            }
            
            logger.info("Summary report written to: {}", reportPath);
        }
    }
    
}



